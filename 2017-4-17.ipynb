{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import theano.tensor.nnet as nnet\n",
    "from sklearn import datasets\n",
    "import pylab\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n"
     ]
    }
   ],
   "source": [
    "train_num = 150000\n",
    "\n",
    "#load training labels\n",
    "train_label_f = open('coordinate_data_150/train/labels.txt')\n",
    "train_label_input = train_label_f.readlines()\n",
    "n = 0\n",
    "train_labels = [([-1]*3) for i in range(train_num)]\n",
    "while n < (train_num):\n",
    "    t = train_label_input[n].split()\n",
    "    for i in range(0,3):\n",
    "        train_labels[n][i] = int(t[i])\n",
    "    n = n + 1\n",
    "\n",
    "word_df1 = pd.read_csv('train_words_dis1.csv')\n",
    "word_df2 = pd.read_csv('train_words_dis2.csv')\n",
    "word_df2.index = word_df2.index+50000\n",
    "word_df3 = pd.read_csv('train_words_dis3.csv')\n",
    "word_df3.index = word_df3.index+100000\n",
    "train_word_df = pd.concat([word_df1,word_df2,word_df3])\n",
    "del train_word_df['Unnamed: 0']\n",
    "\n",
    "type_df1 = pd.read_csv('train_type_dis1.csv')\n",
    "type_df2 = pd.read_csv('train_type_dis2.csv')\n",
    "type_df2.index = type_df2.index+50000\n",
    "type_df3 = pd.read_csv('train_type_dis3.csv')\n",
    "type_df3.index = type_df3.index+100000\n",
    "train_type_df = pd.concat([type_df1,type_df2,type_df3])\n",
    "del train_type_df['Unnamed: 0']\n",
    "\n",
    "train_Y = [([int]) for i in range(train_num)]\n",
    "for i in range(train_num):\n",
    "    train_Y[i] = train_labels[i][2]\n",
    "    \n",
    "test_num = 10000\n",
    "\n",
    "#load test labels\n",
    "test_label_f = open('coordinate_data_150/test/labels.txt')\n",
    "test_label_input = test_label_f.readlines()\n",
    "n = 0\n",
    "test_labels = [([-1]*3) for i in range(test_num)]\n",
    "while n < test_num:\n",
    "    t = test_label_input[n].split()\n",
    "    for i in range(0,3):\n",
    "        test_labels[n][i] = int(t[i])\n",
    "    n = n + 1\n",
    "    \n",
    "\n",
    "test_word_df = pd.read_csv('test_words_dis1.csv')\n",
    "del test_word_df['Unnamed: 0']\n",
    "test_type_df = pd.read_csv('test_type_dis1.csv')\n",
    "del test_type_df['Unnamed: 0']\n",
    "'''\n",
    "test_X = np.ndarray(shape=(test_num,8))\n",
    "for i in range(test_num):\n",
    "    test_X[i][0] = test_word_df.ix[i][6]\n",
    "    test_X[i][1] = test_word_df.ix[i][6]\n",
    "    test_X[i][2] = test_word_df.ix[i][0] - test_word_df.ix[i][2]\n",
    "    test_X[i][3] = test_word_df.ix[i][11] - test_word_df.ix[i][14]\n",
    "    test_X[i][4] = test_type_df.ix[i][6]\n",
    "    test_X[i][5] = test_type_df.ix[i][6]\n",
    "    test_X[i][6] = test_type_df.ix[i][0] - test_type_df.ix[i][2]\n",
    "    test_X[i][7] = test_type_df.ix[i][11] - test_type_df.ix[i][14]\n",
    "    if i %1000 ==0:\n",
    "        print(i)\n",
    "        \n",
    "#replace nan with 0\n",
    "for i in range(test_num):\n",
    "    for j in range(8):\n",
    "        if np.isnan(test_X[i][j]):\n",
    "            test_X[i][j] = 0\n",
    "'''            \n",
    "test_Y = [([int]) for i in range(test_num)]\n",
    "for i in range(test_num):\n",
    "    test_Y[i] = test_labels[i][2]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n",
      "50000\n",
      "51000\n",
      "52000\n",
      "53000\n",
      "54000\n",
      "55000\n",
      "56000\n",
      "57000\n",
      "58000\n",
      "59000\n",
      "60000\n",
      "61000\n",
      "62000\n",
      "63000\n",
      "64000\n",
      "65000\n",
      "66000\n",
      "67000\n",
      "68000\n",
      "69000\n",
      "70000\n",
      "71000\n",
      "72000\n",
      "73000\n",
      "74000\n",
      "75000\n",
      "76000\n",
      "77000\n",
      "78000\n",
      "79000\n",
      "80000\n",
      "81000\n",
      "82000\n",
      "83000\n",
      "84000\n",
      "85000\n",
      "86000\n",
      "87000\n",
      "88000\n",
      "89000\n",
      "90000\n",
      "91000\n",
      "92000\n",
      "93000\n",
      "94000\n",
      "95000\n",
      "96000\n",
      "97000\n",
      "98000\n",
      "99000\n",
      "100000\n",
      "101000\n",
      "102000\n",
      "103000\n",
      "104000\n",
      "105000\n",
      "106000\n",
      "107000\n",
      "108000\n",
      "109000\n",
      "110000\n",
      "111000\n",
      "112000\n",
      "113000\n",
      "114000\n",
      "115000\n",
      "116000\n",
      "117000\n",
      "118000\n",
      "119000\n",
      "120000\n",
      "121000\n",
      "122000\n",
      "123000\n",
      "124000\n",
      "125000\n",
      "126000\n",
      "127000\n",
      "128000\n",
      "129000\n",
      "130000\n",
      "131000\n",
      "132000\n",
      "133000\n",
      "134000\n",
      "135000\n",
      "136000\n",
      "137000\n",
      "138000\n",
      "139000\n",
      "140000\n",
      "141000\n",
      "142000\n",
      "143000\n",
      "144000\n",
      "145000\n",
      "146000\n",
      "147000\n",
      "148000\n",
      "149000\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# 1 2 3 4 5 6\n",
    "#指标1：2 and 4\n",
    "#指标2：3 and 5\n",
    "#指标3：1-2 and 1-4\n",
    "#指标4：3-6 and 5-6\n",
    "\n",
    "#word和type总共8个指标\n",
    "\n",
    "#train_word_df.ix[:,0]-train_word_df.ix[:,0]\n",
    "train_X = np.ndarray(shape=(train_num,8))\n",
    "for i in range(train_num):\n",
    "    train_X[i][0] = train_word_df.ix[i][6]\n",
    "    train_X[i][1] = train_word_df.ix[i][6]\n",
    "    train_X[i][2] = train_word_df.ix[i][0] - train_word_df.ix[i][2]\n",
    "    train_X[i][3] = train_word_df.ix[i][11] - train_word_df.ix[i][14]\n",
    "    train_X[i][4] = train_type_df.ix[i][6]\n",
    "    train_X[i][5] = train_type_df.ix[i][6]\n",
    "    train_X[i][6] = train_type_df.ix[i][0] - train_type_df.ix[i][2]\n",
    "    train_X[i][7] = train_type_df.ix[i][11] - train_type_df.ix[i][14]\n",
    "    if i %1000 ==0:\n",
    "        print(i)\n",
    "        \n",
    "#replace nan with 0\n",
    "for i in range(train_num):\n",
    "    for j in range(8):\n",
    "        if np.isnan(train_X[i][j]):\n",
    "            train_X[i][j] = 0\n",
    "            \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n"
     ]
    }
   ],
   "source": [
    "temp1_X = pd.concat([train_word_df,train_type_df],axis=1)\n",
    "train_X = np.array(temp1_X)\n",
    "temp2_X = pd.concat([test_word_df,test_type_df],axis=1)\n",
    "test_X = np.array(temp2_X)\n",
    "for i in range(train_num):\n",
    "    for j in range(30):\n",
    "        if np.isnan(train_X[i][j]):\n",
    "            train_X[i][j] = 0\n",
    "\n",
    "for i in range(test_num):\n",
    "    for j in range(30):\n",
    "        if np.isnan(test_X[i][j]):\n",
    "            test_X[i][j] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train after iteration 0, Cost: 0.47605367567\n",
      "Train after iteration 1, Cost: 0.267963100322\n",
      "Train after iteration 2, Cost: 0.0967108586651\n",
      "Train after iteration 3, Cost: 0.0943237816605\n",
      "Train after iteration 4, Cost: 0.0910980239438\n",
      "Train after iteration 5, Cost: 0.0873378797394\n",
      "Train after iteration 6, Cost: 0.0849328388372\n",
      "Train after iteration 7, Cost: 0.0824750057904\n",
      "Train after iteration 8, Cost: 0.0797589224049\n",
      "Train after iteration 9, Cost: 0.0768383984588\n",
      "Train after iteration 10, Cost: 0.0738520857712\n",
      "Train after iteration 11, Cost: 0.0668237683343\n",
      "Train after iteration 12, Cost: 0.0519209002005\n",
      "Train after iteration 13, Cost: 0.0434155994859\n",
      "Train after iteration 14, Cost: 0.0398759942554\n",
      "Train after iteration 15, Cost: 0.0382785000688\n",
      "Train after iteration 16, Cost: 0.0374528590386\n",
      "Train after iteration 17, Cost: 0.0369828959727\n",
      "Train after iteration 18, Cost: 0.0365786955734\n",
      "Train after iteration 19, Cost: 0.0365975875381\n",
      "Train after iteration 20, Cost: 0.0366915392634\n",
      "Train after iteration 21, Cost: 0.0352538407867\n",
      "Train after iteration 22, Cost: 0.0337181079297\n",
      "Train after iteration 23, Cost: 0.0315375502061\n",
      "Train after iteration 24, Cost: 0.0301132128389\n",
      "Train after iteration 25, Cost: 0.0286803746807\n",
      "Train after iteration 26, Cost: 0.0278341034346\n",
      "Train after iteration 27, Cost: 0.027040072698\n",
      "Train after iteration 28, Cost: 0.0263590403585\n",
      "Train after iteration 29, Cost: 0.0257064304018\n",
      "Train after iteration 30, Cost: 0.0250878986476\n",
      "Train after iteration 31, Cost: 0.0246677809461\n",
      "Train after iteration 32, Cost: 0.0243935682557\n",
      "Train after iteration 33, Cost: 0.0242142740854\n",
      "Train after iteration 34, Cost: 0.0237170596918\n",
      "Train after iteration 35, Cost: 0.0227899786293\n",
      "Train after iteration 36, Cost: 0.0224229004476\n",
      "Train after iteration 37, Cost: 0.0219718400692\n",
      "Train after iteration 38, Cost: 0.0216306754933\n",
      "Train after iteration 39, Cost: 0.021309007826\n",
      "Train after iteration 40, Cost: 0.0210228215065\n",
      "Train after iteration 41, Cost: 0.0207474375093\n",
      "Train after iteration 42, Cost: 0.0204738784856\n",
      "Train after iteration 43, Cost: 0.0202142106762\n",
      "Train after iteration 44, Cost: 0.0199809108045\n",
      "Train after iteration 45, Cost: 0.0197743188795\n",
      "Train after iteration 46, Cost: 0.0195780959362\n",
      "Train after iteration 47, Cost: 0.0193849409238\n",
      "Train after iteration 48, Cost: 0.0191795286724\n",
      "Train after iteration 49, Cost: 0.0189992466916\n",
      "Train after iteration 50, Cost: 0.0188233631861\n",
      "Train after iteration 51, Cost: 0.018649595736\n",
      "Train after iteration 52, Cost: 0.0184798143291\n",
      "Train after iteration 53, Cost: 0.01831761172\n",
      "Train after iteration 54, Cost: 0.0181616378941\n",
      "Train after iteration 55, Cost: 0.0179949819101\n",
      "Train after iteration 56, Cost: 0.0178213539764\n",
      "Train after iteration 57, Cost: 0.0176626095886\n",
      "Train after iteration 58, Cost: 0.017511291438\n",
      "Train after iteration 59, Cost: 0.0174022134198\n",
      "Train after iteration 60, Cost: 0.0172788662779\n",
      "Train after iteration 61, Cost: 0.0171390915875\n",
      "Train after iteration 62, Cost: 0.0169924781091\n",
      "Train after iteration 63, Cost: 0.0168546037415\n",
      "Train after iteration 64, Cost: 0.0167257834066\n",
      "Train after iteration 65, Cost: 0.0166026252722\n",
      "Train after iteration 66, Cost: 0.0164645289552\n",
      "Train after iteration 67, Cost: 0.0163318819893\n",
      "Train after iteration 68, Cost: 0.0162165307999\n",
      "Train after iteration 69, Cost: 0.0161104843326\n",
      "Train after iteration 70, Cost: 0.0160053835284\n",
      "Train after iteration 71, Cost: 0.0158944781078\n",
      "Train after iteration 72, Cost: 0.0157921950799\n",
      "Train after iteration 73, Cost: 0.015701996987\n",
      "Train after iteration 74, Cost: 0.0156176269373\n",
      "Train after iteration 75, Cost: 0.0155344610379\n",
      "Train after iteration 76, Cost: 0.0154505031589\n",
      "Train after iteration 77, Cost: 0.0153794644061\n",
      "Train after iteration 78, Cost: 0.0153076027151\n",
      "Train after iteration 79, Cost: 0.0152166753002\n",
      "Train after iteration 80, Cost: 0.0151072251172\n",
      "Train after iteration 81, Cost: 0.0149559371203\n",
      "Train after iteration 82, Cost: 0.014848431075\n",
      "Train after iteration 83, Cost: 0.0147480258312\n",
      "Train after iteration 84, Cost: 0.0146414841638\n",
      "Train after iteration 85, Cost: 0.0145247561161\n",
      "Train after iteration 86, Cost: 0.0144141377472\n",
      "Train after iteration 87, Cost: 0.0142811804105\n",
      "Train after iteration 88, Cost: 0.0141428287405\n",
      "Train after iteration 89, Cost: 0.0140174912492\n",
      "Train after iteration 90, Cost: 0.0138927623562\n",
      "Train after iteration 91, Cost: 0.0138160608393\n",
      "Train after iteration 92, Cost: 0.0137107770861\n",
      "Train after iteration 93, Cost: 0.0136504355979\n",
      "Train after iteration 94, Cost: 0.0135398768622\n",
      "Train after iteration 95, Cost: 0.0135042642864\n",
      "Train after iteration 96, Cost: 0.0134051898438\n",
      "Train after iteration 97, Cost: 0.0133693545756\n",
      "Train after iteration 98, Cost: 0.0132776632529\n"
     ]
    }
   ],
   "source": [
    "#神经网络模型\n",
    "'''\n",
    "\n",
    "#设置参数  \n",
    "nn_input_dim=8 #输入神经元个数  \n",
    "nn_output_dim=2 #输出神经元个数  \n",
    "nn_hdim=50  #隐层节点数\n",
    "epsilon=0.2 #learning rate  \n",
    "reg_lambda=0.01 #正则化长度  \n",
    "\n",
    "#参数初始化，设置为shared加速计算\n",
    "w1=theano.shared(np.random.randn(nn_input_dim,nn_hdim),name=\"W1\")  \n",
    "b1=theano.shared(np.zeros(nn_hdim),name=\"b1\")  \n",
    "w2=theano.shared(np.random.randn(nn_hdim,nn_output_dim),name=\"W2\")  \n",
    "b2=theano.shared(np.zeros(nn_output_dim),name=\"b2\")  \n",
    "\n",
    "#前馈算法  \n",
    "X=T.matrix('X')  #double类型的矩阵 \n",
    "y=T.lvector('y') #int64类型的向量  \n",
    "z1=X.dot(w1)+b1   #1 输入和w1的加权和\n",
    "a1=T.tanh(z1)     #2 激活函数\n",
    "z2=a1.dot(w2)+b2  #3 隐层输出和w2的加权和  \n",
    "y_hat=T.nnet.softmax(z2) #4 激活函数  \n",
    "loss_reg=1./train_num * reg_lambda/2 * (T.sum(T.square(w1))+T.sum(T.square(w2))) #5 正则化项    \n",
    "loss=T.nnet.categorical_crossentropy(y_hat,y).mean() + loss_reg  #6 损失函数    \n",
    "prediction=T.argmax(y_hat,axis=1) #7 预测结果  \n",
    "\n",
    "forword_prop=theano.function([X],y_hat)  \n",
    "calculate_loss=theano.function([X,y],loss)  \n",
    "predict=theano.function([X],prediction)  \n",
    "\n",
    "#求导  \n",
    "dw2=T.grad(loss,w2)  \n",
    "db2=T.grad(loss,b2)  \n",
    "dw1=T.grad(loss,w1)  \n",
    "db1=T.grad(loss,b1)  \n",
    "#更新值  \n",
    "gradient_step=theano.function(  \n",
    "    [X,y],  \n",
    "    updates=(  \n",
    "        (w2,w2-epsilon*dw2),  \n",
    "        (b2,b2-epsilon*db2),  \n",
    "        (w1,w1-epsilon*dw1),  \n",
    "        (b1,b1-epsilon*db1)  \n",
    "  \n",
    "    )  \n",
    ")  \n",
    "\n",
    "def build_model(num_passes=5000,print_loss=False):  \n",
    "    w1.set_value(np.random.randn(nn_input_dim, nn_hdim) / np.sqrt(nn_input_dim))  \n",
    "    b1.set_value(np.zeros(nn_hdim))  \n",
    "    w2.set_value(np.random.randn(nn_hdim, nn_output_dim) / np.sqrt(nn_hdim))  \n",
    "    b2.set_value(np.zeros(nn_output_dim))  \n",
    "    for i in range(0,num_passes):  \n",
    "        gradient_step(train_X,train_Y)   #TODO change here\n",
    "        if print_loss and i%1==0:  \n",
    "            print(\"Loss after iteration %i: %f\"%(i,calculate_loss(train_X,train_Y)))  \n",
    "'''\n",
    "\n",
    "x = T.dvector()\n",
    "y = T.dscalar()\n",
    "\n",
    "#设置参数  \n",
    "nn_input_dim=30 #输入神经元个数  \n",
    "nn_output_dim=1 #输出神经元个数  \n",
    "nn_hdim1=50  #隐层1节点数\n",
    "nn_hdim2=30  #隐层2节点数\n",
    "  \n",
    "def layer1(x, w):\n",
    "    b = np.array([1], dtype=theano.config.floatX)\n",
    "    new_x = T.concatenate([x, b])\n",
    "    m = T.dot(w.T, new_x) \n",
    "    h = T.tanh(m)\n",
    "    return h\n",
    "\n",
    "def layer2(x,w):\n",
    "    b = np.array([1], dtype=theano.config.floatX)\n",
    "    new_x = T.concatenate([x, b])\n",
    "    m = T.dot(w.T, new_x) \n",
    "    h = nnet.sigmoid(m)\n",
    "    return h\n",
    "\n",
    "def layer3(x, w):\n",
    "    b = np.array([1], dtype=theano.config.floatX)\n",
    "    new_x = T.concatenate([x, b])\n",
    "    m = T.dot(w.T, new_x) \n",
    "    h = T.tanh(m)\n",
    "    return h\n",
    "\n",
    "def grad_desc(cost, theta):\n",
    "    alpha = 0.01 #learning rate\n",
    "    return theta - (alpha * T.grad(cost, wrt=theta))\n",
    "\n",
    "theta1 = theano.shared(np.array(np.random.rand(nn_input_dim+1,nn_hdim1), dtype=theano.config.floatX)) \n",
    "theta2 = theano.shared(np.array(np.random.rand(nn_hdim1+1,nn_hdim2), dtype=theano.config.floatX))\n",
    "theta3 = theano.shared(np.array(np.random.rand(nn_hdim2+1,nn_output_dim), dtype=theano.config.floatX))\n",
    "\n",
    "hid1 = layer1(x, theta1) #hidden layer\n",
    "hid2 = layer2(hid1,theta2)\n",
    "out1 = T.sum(layer3(hid2, theta3)) #output layer\n",
    "fc = (out1 - y)**2 #cost expression\n",
    "cost = theano.function(inputs=[x, y], outputs=fc, updates=[\n",
    "        (theta1, grad_desc(fc, theta1)),\n",
    "        (theta2, grad_desc(fc, theta2)),\n",
    "        (theta3, grad_desc(fc, theta3))])\n",
    "run_forward = theano.function(inputs=[x], outputs=out1)\n",
    "\n",
    "cur_cost = 0\n",
    "all_cost = 0\n",
    "for i in range(200):\n",
    "    for k in range(len(train_X)):\n",
    "        cur_cost = cost(train_X[k], train_Y[k]) #call our Theano-compiled cost function, it will auto update weights\n",
    "        all_cost = all_cost+cur_cost\n",
    "    all_cost = all_cost/len(train_X)\n",
    "    if i % 1 == 0: \n",
    "        print('Train after iteration %i, Cost: %s' % (i,all_cost,))\n",
    "    all_cost = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "430 324 969 8277\n",
      "1293\n",
      "accuracy is :  0.8707\n",
      "Precision is :  0.3073624017155111\n",
      "Recall is :  0.5702917771883289\n",
      "F1 is :  0.3994426381792847\n"
     ]
    }
   ],
   "source": [
    "\n",
    "error_num = 0\n",
    "\n",
    "threshold = 0.1\n",
    "\n",
    "TP=0\n",
    "FN=0\n",
    "FP=0\n",
    "TN=0\n",
    "\n",
    "for i in range(0,test_num):\n",
    "    if(test_Y[i]==1):\n",
    "        if(run_forward(test_X[i])>threshold):\n",
    "            TP=TP+1\n",
    "        else:\n",
    "            FN=FN+1\n",
    "            error_num=error_num+1\n",
    "    else:\n",
    "        if(run_forward(test_X[i])>threshold):\n",
    "            FP=FP+1\n",
    "            error_num=error_num+1\n",
    "        else:\n",
    "            TN=TN+1\n",
    "\n",
    "print(TP,FN,FP,TN)\n",
    "print(error_num)\n",
    "accuracy = (test_num-error_num)/test_num\n",
    "P=TP/(TP+FP)\n",
    "R=TP/(TP+FN)\n",
    "F1=2*P*R/(P+R)\n",
    "print('accuracy is : ',accuracy)\n",
    "print(\"Precision is : \",P)\n",
    "print(\"Recall is : \",R)\n",
    "print(\"F1 is : \",F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
