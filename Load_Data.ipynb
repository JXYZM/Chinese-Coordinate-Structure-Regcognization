{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#从对应的txt文件中读取word向量表和训练数据集\n",
    "#embedding:words对应的150维向量(float)\n",
    "#train_words:训练并列短语(string)\n",
    "#train_types:训练并列短语每个词的词性(string)\n",
    "#train_labels:训练并列短语对应的标记(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import theano\n",
    "from sklearn import datasets\n",
    "import pylab\n",
    "import theano.tensor as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#load embedding\n",
    "embedding = pd.read_csv('embedding.csv')\n",
    "embedding.index = embedding['Unnamed: 0'].tolist()\n",
    "del embedding['Unnamed: 0']\n",
    "\n",
    "\n",
    "#load training context\n",
    "train_context_f = open('coordinate_data_150/train/context.txt')\n",
    "train_context_input = train_context_f.readlines()\n",
    "n = 0\n",
    "train_words = [([\"unknown\"]*6) for i in range(int(len(train_context_input)/3))]\n",
    "train_types = [([\"unknown\"]*6) for i in range(int(len(train_context_input)/3))]\n",
    "words_index = 0\n",
    "types_index = 0\n",
    "while n < len(train_context_input):\n",
    "    t = train_context_input[n].split()\n",
    "    if n%3==0:\n",
    "        for i in range(0,6):\n",
    "            train_words[words_index][i] = t[i]\n",
    "        words_index = words_index + 1\n",
    "    elif n%3==1:\n",
    "        for i in range(0,6):                                \n",
    "            train_types[types_index][i] = t[i]\n",
    "        types_index = types_index + 1\n",
    "    n = n + 1\n",
    "    \n",
    "#load training labels\n",
    "train_label_f = open('coordinate_data_150/train/labels.txt')\n",
    "train_label_input = train_label_f.readlines()\n",
    "n = 0\n",
    "train_labels = [([-1]*3) for i in range(len(train_label_input))]\n",
    "while n < len(train_label_input):\n",
    "    t = train_label_input[n].split()\n",
    "    for i in range(0,3):\n",
    "        train_labels[n][i] = int(t[i])\n",
    "    n = n + 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#embedding.ix[train_types[0][0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#计算余弦距离的函数\n",
    "def cosine_dis(vector1,vector2):\n",
    "    '''\n",
    "    @vector1 and @vector2 are two input vectors\n",
    "    @Return the cosine distance between two vectors\n",
    "    '''\n",
    "    \n",
    "    mm = 0\n",
    "    for i in range(0,len(vector1)):\n",
    "        mm = mm + vector1[i]*vector2[i]\n",
    "    dd1 = 0\n",
    "    dd2 = 0\n",
    "    for i in range(0,len(vector1)):\n",
    "        dd1 = dd1 + vector1[i]*vector1[i]\n",
    "        dd2 = dd2 + vector2[i]*vector2[i]\n",
    "    dd1 = math.sqrt(dd1)\n",
    "    dd2 = math.sqrt(dd2)\n",
    "    result = mm/(dd1*dd2)\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#TODO: 根据词性来训练\n",
    "#余弦距离的定义\n",
    "#距离度量标准？？？\n",
    "\n",
    "    \n",
    "#第一步：单纯以context中对应位置的词性作为训练标准\n",
    "#第二步：在词性的基础上加入词汇\n",
    "#第三步：选取更多的度量方法\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#神经网络模型\n",
    "\n",
    "train_num = 10000; #训练量\n",
    "\n",
    "#设置参数  \n",
    "nn_input_dim=2 #输入神经元个数  \n",
    "nn_output_dim=2 #输出神经元个数  \n",
    "nn_hdim=100  #隐层节点数\n",
    "epsilon=0.01 #learning rate  \n",
    "reg_lambda=0.01 #正则化长度  \n",
    "\n",
    "#参数初始化，设置为shared加速计算\n",
    "w1=theano.shared(np.random.randn(nn_input_dim,nn_hdim),name=\"W1\")  \n",
    "b1=theano.shared(np.zeros(nn_hdim),name=\"b1\")  \n",
    "w2=theano.shared(np.random.randn(nn_hdim,nn_output_dim),name=\"W2\")  \n",
    "b2=theano.shared(np.zeros(nn_output_dim),name=\"b2\")  \n",
    "\n",
    "#前馈算法  \n",
    "X=T.matrix('X')  #double类型的矩阵 \n",
    "y=T.lvector('y') #int64类型的向量  \n",
    "z1=X.dot(w1)+b1   #1 输入和w1的加权和\n",
    "a1=T.tanh(z1)     #2 激活函数\n",
    "z2=a1.dot(w2)+b2  #3 隐层输出和w2的加权和  \n",
    "y_hat=T.nnet.softmax(z2) #4 激活函数  \n",
    "loss_reg=1./train_num * reg_lambda/2 * (T.sum(T.square(w1))+T.sum(T.square(w2))) #5 正则化项    \n",
    "loss=T.nnet.categorical_crossentropy(y_hat,y).mean()+loss_reg  #6 损失函数    \n",
    "prediction=T.argmax(y_hat,axis=1) #7 预测结果  \n",
    "\n",
    "forword_prop=theano.function([X],y_hat)  \n",
    "calculate_loss=theano.function([X,y],loss)  \n",
    "predict=theano.function([X],prediction)  \n",
    "\n",
    "#求导  \n",
    "dw2=T.grad(loss,w2)  \n",
    "db2=T.grad(loss,b2)  \n",
    "dw1=T.grad(loss,w1)  \n",
    "db1=T.grad(loss,b1)  \n",
    "#更新值  \n",
    "gradient_step=theano.function(  \n",
    "    [X,y],  \n",
    "    updates=(  \n",
    "        (w2,w2-epsilon*dw2),  \n",
    "        (b2,b2-epsilon*db2),  \n",
    "        (w1,w1-epsilon*dw1),  \n",
    "        (b1,b1-epsilon*db1)  \n",
    "  \n",
    "    )  \n",
    ")  \n",
    "\n",
    "def build_model(num_passes=2000,print_loss=False):  \n",
    "    w1.set_value(np.random.randn(nn_input_dim, nn_hdim) / np.sqrt(nn_input_dim))  \n",
    "    b1.set_value(np.zeros(nn_hdim))  \n",
    "    w2.set_value(np.random.randn(nn_hdim, nn_output_dim) / np.sqrt(nn_hdim))  \n",
    "    b2.set_value(np.zeros(nn_output_dim))  \n",
    "    for i in range(0,num_passes):  \n",
    "        gradient_step(train_words_dis,train_Y)   #TODO change here\n",
    "        if print_loss and i%10==0:  \n",
    "            print(\"Loss after iteration %i: %f\"%(i,calculate_loss(train_words_dis,train_Y)))  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#第一步：单纯以context中对应位置的词性作为训练标准\n",
    "#计算context对应位置的余弦距离（2-4，3-5） TODO:改写这部分\n",
    "\n",
    "train_words_dis = [([float]*nn_input_dim) for i in range(train_num)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "8100\n",
      "8200\n",
      "8300\n",
      "8400\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "8800\n",
      "8900\n",
      "9000\n",
      "9100\n",
      "9200\n",
      "9300\n",
      "9400\n",
      "9500\n",
      "9600\n",
      "9700\n",
      "9800\n",
      "9900\n"
     ]
    }
   ],
   "source": [
    "#embedding.ix[train_types[0][1]]\n",
    "\n",
    "for i in range(0,train_num):\n",
    "    train_words_dis[i][0] = cosine_dis(embedding.ix[train_types[i][1]],embedding.ix[train_types[i][3]])\n",
    "    train_words_dis[i][1] = cosine_dis(embedding.ix[train_types[i][2]],embedding.ix[train_types[i][4]])\n",
    "    if(i%100==0):\n",
    "        print(i)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_Y = [([int]) for i in range(train_num)]\n",
    "for i in range(train_num):\n",
    "    train_Y[i] = train_labels[i][2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 0: 0.620067\n",
      "Loss after iteration 10: 0.612536\n",
      "Loss after iteration 20: 0.609048\n",
      "Loss after iteration 30: 0.606344\n",
      "Loss after iteration 40: 0.604018\n",
      "Loss after iteration 50: 0.601949\n",
      "Loss after iteration 60: 0.600075\n",
      "Loss after iteration 70: 0.598360\n",
      "Loss after iteration 80: 0.596778\n",
      "Loss after iteration 90: 0.595315\n",
      "Loss after iteration 100: 0.593956\n",
      "Loss after iteration 110: 0.592692\n",
      "Loss after iteration 120: 0.591513\n",
      "Loss after iteration 130: 0.590413\n",
      "Loss after iteration 140: 0.589385\n",
      "Loss after iteration 150: 0.588423\n",
      "Loss after iteration 160: 0.587522\n",
      "Loss after iteration 170: 0.586677\n",
      "Loss after iteration 180: 0.585884\n",
      "Loss after iteration 190: 0.585138\n",
      "Loss after iteration 200: 0.584436\n",
      "Loss after iteration 210: 0.583776\n",
      "Loss after iteration 220: 0.583153\n",
      "Loss after iteration 230: 0.582565\n",
      "Loss after iteration 240: 0.582010\n",
      "Loss after iteration 250: 0.581485\n",
      "Loss after iteration 260: 0.580988\n",
      "Loss after iteration 270: 0.580517\n",
      "Loss after iteration 280: 0.580070\n",
      "Loss after iteration 290: 0.579646\n",
      "Loss after iteration 300: 0.579243\n",
      "Loss after iteration 310: 0.578859\n",
      "Loss after iteration 320: 0.578494\n",
      "Loss after iteration 330: 0.578146\n",
      "Loss after iteration 340: 0.577814\n",
      "Loss after iteration 350: 0.577497\n",
      "Loss after iteration 360: 0.577193\n",
      "Loss after iteration 370: 0.576903\n",
      "Loss after iteration 380: 0.576625\n",
      "Loss after iteration 390: 0.576358\n",
      "Loss after iteration 400: 0.576102\n",
      "Loss after iteration 410: 0.575856\n",
      "Loss after iteration 420: 0.575620\n",
      "Loss after iteration 430: 0.575392\n",
      "Loss after iteration 440: 0.575173\n",
      "Loss after iteration 450: 0.574961\n",
      "Loss after iteration 460: 0.574757\n",
      "Loss after iteration 470: 0.574560\n",
      "Loss after iteration 480: 0.574369\n",
      "Loss after iteration 490: 0.574185\n",
      "Loss after iteration 500: 0.574006\n",
      "Loss after iteration 510: 0.573832\n",
      "Loss after iteration 520: 0.573664\n",
      "Loss after iteration 530: 0.573500\n",
      "Loss after iteration 540: 0.573341\n",
      "Loss after iteration 550: 0.573187\n",
      "Loss after iteration 560: 0.573036\n",
      "Loss after iteration 570: 0.572889\n",
      "Loss after iteration 580: 0.572746\n",
      "Loss after iteration 590: 0.572606\n",
      "Loss after iteration 600: 0.572470\n",
      "Loss after iteration 610: 0.572336\n",
      "Loss after iteration 620: 0.572206\n",
      "Loss after iteration 630: 0.572078\n",
      "Loss after iteration 640: 0.571953\n",
      "Loss after iteration 650: 0.571830\n",
      "Loss after iteration 660: 0.571710\n",
      "Loss after iteration 670: 0.571591\n",
      "Loss after iteration 680: 0.571475\n",
      "Loss after iteration 690: 0.571361\n",
      "Loss after iteration 700: 0.571249\n",
      "Loss after iteration 710: 0.571139\n",
      "Loss after iteration 720: 0.571030\n",
      "Loss after iteration 730: 0.570923\n",
      "Loss after iteration 740: 0.570817\n",
      "Loss after iteration 750: 0.570713\n",
      "Loss after iteration 760: 0.570611\n",
      "Loss after iteration 770: 0.570509\n",
      "Loss after iteration 780: 0.570409\n",
      "Loss after iteration 790: 0.570311\n",
      "Loss after iteration 800: 0.570213\n",
      "Loss after iteration 810: 0.570116\n",
      "Loss after iteration 820: 0.570021\n",
      "Loss after iteration 830: 0.569927\n",
      "Loss after iteration 840: 0.569833\n",
      "Loss after iteration 850: 0.569741\n",
      "Loss after iteration 860: 0.569649\n",
      "Loss after iteration 870: 0.569558\n",
      "Loss after iteration 880: 0.569468\n",
      "Loss after iteration 890: 0.569379\n",
      "Loss after iteration 900: 0.569290\n",
      "Loss after iteration 910: 0.569202\n",
      "Loss after iteration 920: 0.569115\n",
      "Loss after iteration 930: 0.569029\n",
      "Loss after iteration 940: 0.568943\n",
      "Loss after iteration 950: 0.568858\n",
      "Loss after iteration 960: 0.568773\n",
      "Loss after iteration 970: 0.568689\n",
      "Loss after iteration 980: 0.568606\n",
      "Loss after iteration 990: 0.568523\n",
      "Loss after iteration 1000: 0.568440\n",
      "Loss after iteration 1010: 0.568358\n",
      "Loss after iteration 1020: 0.568277\n",
      "Loss after iteration 1030: 0.568196\n",
      "Loss after iteration 1040: 0.568115\n",
      "Loss after iteration 1050: 0.568035\n",
      "Loss after iteration 1060: 0.567955\n",
      "Loss after iteration 1070: 0.567876\n",
      "Loss after iteration 1080: 0.567797\n",
      "Loss after iteration 1090: 0.567718\n",
      "Loss after iteration 1100: 0.567640\n",
      "Loss after iteration 1110: 0.567562\n",
      "Loss after iteration 1120: 0.567484\n",
      "Loss after iteration 1130: 0.567407\n",
      "Loss after iteration 1140: 0.567330\n",
      "Loss after iteration 1150: 0.567253\n",
      "Loss after iteration 1160: 0.567177\n",
      "Loss after iteration 1170: 0.567101\n",
      "Loss after iteration 1180: 0.567025\n",
      "Loss after iteration 1190: 0.566950\n",
      "Loss after iteration 1200: 0.566874\n",
      "Loss after iteration 1210: 0.566799\n",
      "Loss after iteration 1220: 0.566725\n",
      "Loss after iteration 1230: 0.566650\n",
      "Loss after iteration 1240: 0.566576\n",
      "Loss after iteration 1250: 0.566502\n",
      "Loss after iteration 1260: 0.566428\n",
      "Loss after iteration 1270: 0.566355\n",
      "Loss after iteration 1280: 0.566282\n",
      "Loss after iteration 1290: 0.566209\n",
      "Loss after iteration 1300: 0.566136\n",
      "Loss after iteration 1310: 0.566063\n",
      "Loss after iteration 1320: 0.565991\n",
      "Loss after iteration 1330: 0.565919\n",
      "Loss after iteration 1340: 0.565847\n",
      "Loss after iteration 1350: 0.565775\n",
      "Loss after iteration 1360: 0.565703\n",
      "Loss after iteration 1370: 0.565632\n",
      "Loss after iteration 1380: 0.565561\n",
      "Loss after iteration 1390: 0.565490\n",
      "Loss after iteration 1400: 0.565419\n",
      "Loss after iteration 1410: 0.565348\n",
      "Loss after iteration 1420: 0.565278\n",
      "Loss after iteration 1430: 0.565207\n",
      "Loss after iteration 1440: 0.565137\n",
      "Loss after iteration 1450: 0.565067\n",
      "Loss after iteration 1460: 0.564997\n",
      "Loss after iteration 1470: 0.564928\n",
      "Loss after iteration 1480: 0.564858\n",
      "Loss after iteration 1490: 0.564789\n",
      "Loss after iteration 1500: 0.564720\n",
      "Loss after iteration 1510: 0.564651\n",
      "Loss after iteration 1520: 0.564582\n",
      "Loss after iteration 1530: 0.564513\n",
      "Loss after iteration 1540: 0.564445\n",
      "Loss after iteration 1550: 0.564376\n",
      "Loss after iteration 1560: 0.564308\n",
      "Loss after iteration 1570: 0.564240\n",
      "Loss after iteration 1580: 0.564172\n",
      "Loss after iteration 1590: 0.564104\n",
      "Loss after iteration 1600: 0.564036\n",
      "Loss after iteration 1610: 0.563969\n",
      "Loss after iteration 1620: 0.563901\n",
      "Loss after iteration 1630: 0.563834\n",
      "Loss after iteration 1640: 0.563767\n",
      "Loss after iteration 1650: 0.563700\n",
      "Loss after iteration 1660: 0.563633\n",
      "Loss after iteration 1670: 0.563566\n",
      "Loss after iteration 1680: 0.563500\n",
      "Loss after iteration 1690: 0.563433\n",
      "Loss after iteration 1700: 0.563367\n",
      "Loss after iteration 1710: 0.563301\n",
      "Loss after iteration 1720: 0.563235\n",
      "Loss after iteration 1730: 0.563169\n",
      "Loss after iteration 1740: 0.563103\n",
      "Loss after iteration 1750: 0.563037\n",
      "Loss after iteration 1760: 0.562971\n",
      "Loss after iteration 1770: 0.562906\n",
      "Loss after iteration 1780: 0.562841\n",
      "Loss after iteration 1790: 0.562775\n",
      "Loss after iteration 1800: 0.562710\n",
      "Loss after iteration 1810: 0.562645\n",
      "Loss after iteration 1820: 0.562580\n",
      "Loss after iteration 1830: 0.562516\n",
      "Loss after iteration 1840: 0.562451\n",
      "Loss after iteration 1850: 0.562386\n",
      "Loss after iteration 1860: 0.562322\n",
      "Loss after iteration 1870: 0.562258\n",
      "Loss after iteration 1880: 0.562193\n",
      "Loss after iteration 1890: 0.562129\n",
      "Loss after iteration 1900: 0.562065\n",
      "Loss after iteration 1910: 0.562002\n",
      "Loss after iteration 1920: 0.561938\n",
      "Loss after iteration 1930: 0.561874\n",
      "Loss after iteration 1940: 0.561811\n",
      "Loss after iteration 1950: 0.561747\n",
      "Loss after iteration 1960: 0.561684\n",
      "Loss after iteration 1970: 0.561621\n",
      "Loss after iteration 1980: 0.561558\n",
      "Loss after iteration 1990: 0.561495\n"
     ]
    }
   ],
   "source": [
    "build_model(print_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = predict([[1,1],[0,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
