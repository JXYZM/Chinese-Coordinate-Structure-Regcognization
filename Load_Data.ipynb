{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#从对应的txt文件中读取word向量表和训练数据集\n",
    "#embedding:words对应的150维向量(float)\n",
    "#train_words:训练并列短语(string)\n",
    "#train_types:训练并列短语每个词的词性(string)\n",
    "#train_labels:训练并列短语对应的标记(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import theano\n",
    "from sklearn import datasets\n",
    "import pylab\n",
    "import theano.tensor as T\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load data\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#load embedding\n",
    "embedding = pd.read_csv('embedding.csv')\n",
    "embedding.index = embedding['Unnamed: 0'].tolist()\n",
    "del embedding['Unnamed: 0']\n",
    "\n",
    "\n",
    "#load training context\n",
    "train_context_f = open('coordinate_data_150/train/context.txt')\n",
    "train_context_input = train_context_f.readlines()\n",
    "n = 0\n",
    "train_words = [([\"unknown\"]*6) for i in range(int(len(train_context_input)/3))]\n",
    "train_types = [([\"unknown\"]*6) for i in range(int(len(train_context_input)/3))]\n",
    "words_index = 0\n",
    "types_index = 0\n",
    "while n < len(train_context_input):\n",
    "    t = train_context_input[n].split()\n",
    "    if n%3==0:\n",
    "        for i in range(0,6):\n",
    "            train_words[words_index][i] = t[i]\n",
    "        words_index = words_index + 1\n",
    "    elif n%3==1:\n",
    "        for i in range(0,6):                                \n",
    "            train_types[types_index][i] = t[i]\n",
    "        types_index = types_index + 1\n",
    "    n = n + 1\n",
    "    \n",
    "#load training labels\n",
    "train_label_f = open('coordinate_data_150/train/labels.txt')\n",
    "train_label_input = train_label_f.readlines()\n",
    "n = 0\n",
    "train_labels = [([-1]*3) for i in range(len(train_label_input))]\n",
    "while n < len(train_label_input):\n",
    "    t = train_label_input[n].split()\n",
    "    for i in range(0,3):\n",
    "        train_labels[n][i] = int(t[i])\n",
    "    n = n + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#embedding.ix[train_types[0][0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cos\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "#计算余弦距离的函数\n",
    "def cosine_dis(vector1,vector2):\n",
    "    \n",
    "    #@vector1 and @vector2 are two input vectors\n",
    "    #@Return the cosine distance between two vectors\n",
    "    \n",
    "   \n",
    "    mm = 0\n",
    "    for i in range(0,len(vector1)):\n",
    "        mm = mm + vector1[i]*vector2[i]\n",
    "    dd1 = 0\n",
    "    dd2 = 0\n",
    "    for i in range(0,len(vector1)):\n",
    "        dd1 = dd1 + vector1[i]*vector1[i]\n",
    "        dd2 = dd2 + vector2[i]*vector2[i]\n",
    "    dd1 = math.sqrt(dd1)\n",
    "    dd2 = math.sqrt(dd2)\n",
    "    result = mm/(dd1*dd2)\n",
    "    return result\n",
    "'''\n",
    "print('cos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#TODO: 根据词性来训练\n",
    "#余弦距离的定义\n",
    "#距离度量标准？？？\n",
    "\n",
    "    \n",
    "#第一步：单纯以context中对应位置的词性作为训练标准\n",
    "#第二步：在词性的基础上加入词汇\n",
    "#第三步：选取更多的度量方法\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_num = 150000 #len(train_words); #训练量\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#train_type_dis = [([float]*15) for i in range(train_num)]\n",
    "#train_words_dis = [([float]*15) for i in range(train_num)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#type和words的150维向量\n",
    "#train_type_vector = pd.DataFrame(train_types)\n",
    "#train_words_vector = pd.DataFrame(train_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150000\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "for i in range(0,(train_num)):\n",
    "    for j in range(0,6):\n",
    "        if train_types[i][j] in embedding.index:\n",
    "            train_type_vector.ix[i][j]=embedding.ix[train_types[i][j]]\n",
    "    if i%1000==0:\n",
    "        print(i)\n",
    "                                                \n",
    "#train_type_vector.to_csv('train_type_vector.csv')\n",
    "                \n",
    "#TODO 用0取代缺失值\n",
    "\n",
    "for i in range(0,(train_num)):\n",
    "    for j in range(0,6):\n",
    "        if train_words[i][j] in embedding.index:\n",
    "            train_words_vector.ix[i][j]=embedding.ix[train_words[i][j]]\n",
    "        else:\n",
    "            train_words_vector.ix[i][j] = np.zeros(150)\n",
    "    if i%1000==0:\n",
    "        print(i)\n",
    "\n",
    "#train_words_vector.to_csv('train_words_vector.csv')\n",
    "\n",
    "\n",
    "for i in range(0,train_num):\n",
    "    \n",
    "    train_words_dis[i][0] = cosine_dis(train_words_vector.ix[i][0],train_words_vector.ix[i][1])\n",
    "    train_words_dis[i][1] = cosine_dis(train_words_vector.ix[i][0],train_words_vector.ix[i][2])\n",
    "    train_words_dis[i][2] = cosine_dis(train_words_vector.ix[i][0],train_words_vector.ix[i][3])\n",
    "    train_words_dis[i][3] = cosine_dis(train_words_vector.ix[i][0],train_words_vector.ix[i][4])\n",
    "    train_words_dis[i][4] = cosine_dis(train_words_vector.ix[i][0],train_words_vector.ix[i][5])\n",
    "    train_words_dis[i][5] = cosine_dis(train_words_vector.ix[i][1],train_words_vector.ix[i][2])\n",
    "    train_words_dis[i][6] = cosine_dis(train_words_vector.ix[i][1],train_words_vector.ix[i][3])\n",
    "    train_words_dis[i][7] = cosine_dis(train_words_vector.ix[i][1],train_words_vector.ix[i][4])\n",
    "    train_words_dis[i][8] = cosine_dis(train_words_vector.ix[i][1],train_words_vector.ix[i][5])\n",
    "    train_words_dis[i][9] = cosine_dis(train_words_vector.ix[i][2],train_words_vector.ix[i][3])\n",
    "    train_words_dis[i][10] = cosine_dis(train_words_vector.ix[i][2],train_words_vector.ix[i][4])\n",
    "    train_words_dis[i][11] = cosine_dis(train_words_vector.ix[i][2],train_words_vector.ix[i][5])\n",
    "    train_words_dis[i][12] = cosine_dis(train_words_vector.ix[i][3],train_words_vector.ix[i][4])\n",
    "    train_words_dis[i][13] = cosine_dis(train_words_vector.ix[i][3],train_words_vector.ix[i][5])\n",
    "    train_words_dis[i][14] = cosine_dis(train_words_vector.ix[i][4],train_words_vector.ix[i][5])\n",
    "    \n",
    "    \n",
    "    if(i%10==0):\n",
    "        print(i)\n",
    "        \n",
    "for i in range(0,train_num):\n",
    "    \n",
    "    train_type_dis[i][0] = cosine_dis(train_type_vector.ix[i][0],train_type_vector.ix[i][1])\n",
    "    train_type_dis[i][1] = cosine_dis(train_type_vector.ix[i][0],train_type_vector.ix[i][2])\n",
    "    train_type_dis[i][2] = cosine_dis(train_type_vector.ix[i][0],train_type_vector.ix[i][3])\n",
    "    train_type_dis[i][3] = cosine_dis(train_type_vector.ix[i][0],train_type_vector.ix[i][4])\n",
    "    train_type_dis[i][4] = cosine_dis(train_type_vector.ix[i][0],train_type_vector.ix[i][5])\n",
    "    train_type_dis[i][5] = cosine_dis(train_type_vector.ix[i][1],train_type_vector.ix[i][2])\n",
    "    train_type_dis[i][6] = cosine_dis(train_type_vector.ix[i][1],train_type_vector.ix[i][3])\n",
    "    train_type_dis[i][7] = cosine_dis(train_type_vector.ix[i][1],train_type_vector.ix[i][4])\n",
    "    train_type_dis[i][8] = cosine_dis(train_type_vector.ix[i][1],train_type_vector.ix[i][5])\n",
    "    train_type_dis[i][9] = cosine_dis(train_type_vector.ix[i][2],train_type_vector.ix[i][3])\n",
    "    train_type_dis[i][10] = cosine_dis(train_type_vector.ix[i][2],train_type_vector.ix[i][4])\n",
    "    train_type_dis[i][11] = cosine_dis(train_type_vector.ix[i][2],train_type_vector.ix[i][5])\n",
    "    train_type_dis[i][12] = cosine_dis(train_type_vector.ix[i][3],train_type_vector.ix[i][4])\n",
    "    train_type_dis[i][13] = cosine_dis(train_type_vector.ix[i][3],train_type_vector.ix[i][5])\n",
    "    train_type_dis[i][14] = cosine_dis(train_type_vector.ix[i][4],train_type_vector.ix[i][5])\n",
    "    \n",
    "    \n",
    "    if(i%10==0):\n",
    "        print(i)\n",
    "\n",
    "train_type_dis_temp = pd.DataFrame(train_type_dis)\n",
    "train_words_dis_temp = pd.DataFrame(train_words_dis)\n",
    "#train_type_dis_temp.to_csv('train_type_dis.csv')\n",
    "#train_words_dis_temp.to_csv('train_words_dis.csv')\n",
    "#aaaa = pd.read_csv('train_words_dis.csv')\n",
    "'''\n",
    "\n",
    "print(train_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#train_Y = [([int]) for i in range(train_num)]\n",
    "#for i in range(train_num):\n",
    "#    train_Y[i] = train_labels[i][2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "word_df1 = pd.read_csv('train_words_dis1.csv')\n",
    "word_df2 = pd.read_csv('train_words_dis2.csv')\n",
    "word_df2.index = word_df2.index+50000\n",
    "word_df3 = pd.read_csv('train_words_dis3.csv')\n",
    "word_df3.index = word_df3.index+100000\n",
    "train_word_df = pd.concat([word_df1,word_df2,word_df3])\n",
    "del train_word_df['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.078186</td>\n",
       "      <td>0.078186</td>\n",
       "      <td>0.093237</td>\n",
       "      <td>0.093237</td>\n",
       "      <td>0.187492</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.082442</td>\n",
       "      <td>0.082442</td>\n",
       "      <td>0.566941</td>\n",
       "      <td>0.082442</td>\n",
       "      <td>0.082442</td>\n",
       "      <td>0.566941</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.406256</td>\n",
       "      <td>0.406256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.078186</td>\n",
       "      <td>0.078186</td>\n",
       "      <td>0.093237</td>\n",
       "      <td>0.140478</td>\n",
       "      <td>0.053903</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.082442</td>\n",
       "      <td>0.043238</td>\n",
       "      <td>-0.547686</td>\n",
       "      <td>0.082442</td>\n",
       "      <td>0.043238</td>\n",
       "      <td>-0.547686</td>\n",
       "      <td>0.133389</td>\n",
       "      <td>-0.062813</td>\n",
       "      <td>0.161728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.179263</td>\n",
       "      <td>0.043238</td>\n",
       "      <td>0.133389</td>\n",
       "      <td>0.133389</td>\n",
       "      <td>0.120529</td>\n",
       "      <td>0.150082</td>\n",
       "      <td>0.081872</td>\n",
       "      <td>0.081872</td>\n",
       "      <td>0.200082</td>\n",
       "      <td>0.082442</td>\n",
       "      <td>0.082442</td>\n",
       "      <td>0.566941</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.406256</td>\n",
       "      <td>0.406256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.652996</td>\n",
       "      <td>0.150082</td>\n",
       "      <td>0.081872</td>\n",
       "      <td>0.081872</td>\n",
       "      <td>0.200082</td>\n",
       "      <td>0.078186</td>\n",
       "      <td>0.093237</td>\n",
       "      <td>0.093237</td>\n",
       "      <td>0.187492</td>\n",
       "      <td>0.082442</td>\n",
       "      <td>0.082442</td>\n",
       "      <td>0.566941</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.406256</td>\n",
       "      <td>0.406256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.179263</td>\n",
       "      <td>0.043238</td>\n",
       "      <td>0.133389</td>\n",
       "      <td>0.120529</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.150082</td>\n",
       "      <td>0.081872</td>\n",
       "      <td>0.200082</td>\n",
       "      <td>0.179263</td>\n",
       "      <td>0.082442</td>\n",
       "      <td>0.566941</td>\n",
       "      <td>0.043238</td>\n",
       "      <td>0.406256</td>\n",
       "      <td>0.133389</td>\n",
       "      <td>0.120529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.652996</td>\n",
       "      <td>0.150082</td>\n",
       "      <td>0.081872</td>\n",
       "      <td>0.200082</td>\n",
       "      <td>0.179263</td>\n",
       "      <td>0.078186</td>\n",
       "      <td>0.093237</td>\n",
       "      <td>0.187492</td>\n",
       "      <td>0.140478</td>\n",
       "      <td>0.082442</td>\n",
       "      <td>0.566941</td>\n",
       "      <td>0.043238</td>\n",
       "      <td>0.406256</td>\n",
       "      <td>0.133389</td>\n",
       "      <td>0.120529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.179263</td>\n",
       "      <td>0.043238</td>\n",
       "      <td>0.133389</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.161728</td>\n",
       "      <td>0.150082</td>\n",
       "      <td>0.081872</td>\n",
       "      <td>0.179263</td>\n",
       "      <td>-0.049452</td>\n",
       "      <td>0.082442</td>\n",
       "      <td>0.043238</td>\n",
       "      <td>-0.547686</td>\n",
       "      <td>0.133389</td>\n",
       "      <td>-0.062813</td>\n",
       "      <td>0.161728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.179263</td>\n",
       "      <td>0.043238</td>\n",
       "      <td>0.133389</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.161728</td>\n",
       "      <td>0.150082</td>\n",
       "      <td>0.081872</td>\n",
       "      <td>0.179263</td>\n",
       "      <td>-0.049452</td>\n",
       "      <td>0.082442</td>\n",
       "      <td>0.043238</td>\n",
       "      <td>-0.547686</td>\n",
       "      <td>0.133389</td>\n",
       "      <td>-0.062813</td>\n",
       "      <td>0.161728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.652996</td>\n",
       "      <td>0.150082</td>\n",
       "      <td>0.081872</td>\n",
       "      <td>0.179263</td>\n",
       "      <td>-0.049452</td>\n",
       "      <td>0.078186</td>\n",
       "      <td>0.093237</td>\n",
       "      <td>0.140478</td>\n",
       "      <td>0.053903</td>\n",
       "      <td>0.082442</td>\n",
       "      <td>0.043238</td>\n",
       "      <td>-0.547686</td>\n",
       "      <td>0.133389</td>\n",
       "      <td>-0.062813</td>\n",
       "      <td>0.161728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.078186</td>\n",
       "      <td>0.078186</td>\n",
       "      <td>0.093237</td>\n",
       "      <td>0.187492</td>\n",
       "      <td>0.140478</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.082442</td>\n",
       "      <td>0.566941</td>\n",
       "      <td>0.043238</td>\n",
       "      <td>0.082442</td>\n",
       "      <td>0.566941</td>\n",
       "      <td>0.043238</td>\n",
       "      <td>0.406256</td>\n",
       "      <td>0.133389</td>\n",
       "      <td>0.120529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.078186</td>\n",
       "      <td>0.078186</td>\n",
       "      <td>0.093237</td>\n",
       "      <td>0.187492</td>\n",
       "      <td>0.140478</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.082442</td>\n",
       "      <td>0.566941</td>\n",
       "      <td>0.043238</td>\n",
       "      <td>0.082442</td>\n",
       "      <td>0.566941</td>\n",
       "      <td>0.043238</td>\n",
       "      <td>0.406256</td>\n",
       "      <td>0.133389</td>\n",
       "      <td>0.120529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.078186</td>\n",
       "      <td>0.078186</td>\n",
       "      <td>0.093237</td>\n",
       "      <td>0.187492</td>\n",
       "      <td>0.140478</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.082442</td>\n",
       "      <td>0.566941</td>\n",
       "      <td>0.043238</td>\n",
       "      <td>0.082442</td>\n",
       "      <td>0.566941</td>\n",
       "      <td>0.043238</td>\n",
       "      <td>0.406256</td>\n",
       "      <td>0.133389</td>\n",
       "      <td>0.120529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.078186</td>\n",
       "      <td>0.078186</td>\n",
       "      <td>0.093237</td>\n",
       "      <td>0.187492</td>\n",
       "      <td>0.140478</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.082442</td>\n",
       "      <td>0.566941</td>\n",
       "      <td>0.043238</td>\n",
       "      <td>0.082442</td>\n",
       "      <td>0.566941</td>\n",
       "      <td>0.043238</td>\n",
       "      <td>0.406256</td>\n",
       "      <td>0.133389</td>\n",
       "      <td>0.120529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.078186</td>\n",
       "      <td>0.078186</td>\n",
       "      <td>0.093237</td>\n",
       "      <td>0.187492</td>\n",
       "      <td>0.140478</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.082442</td>\n",
       "      <td>0.566941</td>\n",
       "      <td>0.043238</td>\n",
       "      <td>0.082442</td>\n",
       "      <td>0.566941</td>\n",
       "      <td>0.043238</td>\n",
       "      <td>0.406256</td>\n",
       "      <td>0.133389</td>\n",
       "      <td>0.120529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.078186</td>\n",
       "      <td>0.078186</td>\n",
       "      <td>0.093237</td>\n",
       "      <td>0.187492</td>\n",
       "      <td>0.140478</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.082442</td>\n",
       "      <td>0.566941</td>\n",
       "      <td>0.043238</td>\n",
       "      <td>0.082442</td>\n",
       "      <td>0.566941</td>\n",
       "      <td>0.043238</td>\n",
       "      <td>0.406256</td>\n",
       "      <td>0.133389</td>\n",
       "      <td>0.120529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.078186</td>\n",
       "      <td>0.078186</td>\n",
       "      <td>0.093237</td>\n",
       "      <td>0.187492</td>\n",
       "      <td>0.140478</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.082442</td>\n",
       "      <td>0.566941</td>\n",
       "      <td>0.043238</td>\n",
       "      <td>0.082442</td>\n",
       "      <td>0.566941</td>\n",
       "      <td>0.043238</td>\n",
       "      <td>0.406256</td>\n",
       "      <td>0.133389</td>\n",
       "      <td>0.120529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.078186</td>\n",
       "      <td>0.078186</td>\n",
       "      <td>0.093237</td>\n",
       "      <td>0.187492</td>\n",
       "      <td>0.140478</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.082442</td>\n",
       "      <td>0.566941</td>\n",
       "      <td>0.043238</td>\n",
       "      <td>0.082442</td>\n",
       "      <td>0.566941</td>\n",
       "      <td>0.043238</td>\n",
       "      <td>0.406256</td>\n",
       "      <td>0.133389</td>\n",
       "      <td>0.120529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.078186</td>\n",
       "      <td>0.078186</td>\n",
       "      <td>0.093237</td>\n",
       "      <td>0.187492</td>\n",
       "      <td>0.140478</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.082442</td>\n",
       "      <td>0.566941</td>\n",
       "      <td>0.043238</td>\n",
       "      <td>0.082442</td>\n",
       "      <td>0.566941</td>\n",
       "      <td>0.043238</td>\n",
       "      <td>0.406256</td>\n",
       "      <td>0.133389</td>\n",
       "      <td>0.120529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.277971</td>\n",
       "      <td>0.538989</td>\n",
       "      <td>0.133949</td>\n",
       "      <td>0.133949</td>\n",
       "      <td>-0.273742</td>\n",
       "      <td>0.436944</td>\n",
       "      <td>0.122865</td>\n",
       "      <td>0.122865</td>\n",
       "      <td>-0.205006</td>\n",
       "      <td>0.019484</td>\n",
       "      <td>0.019484</td>\n",
       "      <td>-0.627586</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.174248</td>\n",
       "      <td>0.174248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.158540</td>\n",
       "      <td>0.141801</td>\n",
       "      <td>0.102586</td>\n",
       "      <td>0.102586</td>\n",
       "      <td>0.112341</td>\n",
       "      <td>0.538989</td>\n",
       "      <td>0.133949</td>\n",
       "      <td>0.133949</td>\n",
       "      <td>-0.273742</td>\n",
       "      <td>0.019484</td>\n",
       "      <td>0.019484</td>\n",
       "      <td>-0.627586</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.174248</td>\n",
       "      <td>0.174248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.241893</td>\n",
       "      <td>-0.191597</td>\n",
       "      <td>0.024898</td>\n",
       "      <td>0.024898</td>\n",
       "      <td>0.279824</td>\n",
       "      <td>0.141801</td>\n",
       "      <td>0.102586</td>\n",
       "      <td>0.102586</td>\n",
       "      <td>0.112341</td>\n",
       "      <td>0.019484</td>\n",
       "      <td>0.019484</td>\n",
       "      <td>-0.627586</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.174248</td>\n",
       "      <td>0.174248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.086573</td>\n",
       "      <td>0.219403</td>\n",
       "      <td>0.124957</td>\n",
       "      <td>0.124957</td>\n",
       "      <td>-0.020654</td>\n",
       "      <td>-0.191597</td>\n",
       "      <td>0.024898</td>\n",
       "      <td>0.024898</td>\n",
       "      <td>0.279824</td>\n",
       "      <td>0.019484</td>\n",
       "      <td>0.019484</td>\n",
       "      <td>-0.627586</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.174248</td>\n",
       "      <td>0.174248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.007975</td>\n",
       "      <td>0.012663</td>\n",
       "      <td>0.258598</td>\n",
       "      <td>0.258598</td>\n",
       "      <td>0.130370</td>\n",
       "      <td>0.219403</td>\n",
       "      <td>0.124957</td>\n",
       "      <td>0.124957</td>\n",
       "      <td>-0.020654</td>\n",
       "      <td>0.019484</td>\n",
       "      <td>0.019484</td>\n",
       "      <td>-0.627586</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.174248</td>\n",
       "      <td>0.174248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.436944</td>\n",
       "      <td>0.436944</td>\n",
       "      <td>0.122865</td>\n",
       "      <td>0.122865</td>\n",
       "      <td>-0.205006</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.019484</td>\n",
       "      <td>0.019484</td>\n",
       "      <td>-0.627586</td>\n",
       "      <td>0.019484</td>\n",
       "      <td>0.019484</td>\n",
       "      <td>-0.627586</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.174248</td>\n",
       "      <td>0.174248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.436944</td>\n",
       "      <td>0.436944</td>\n",
       "      <td>0.122865</td>\n",
       "      <td>0.122865</td>\n",
       "      <td>-0.205006</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.019484</td>\n",
       "      <td>0.019484</td>\n",
       "      <td>-0.627586</td>\n",
       "      <td>0.019484</td>\n",
       "      <td>0.019484</td>\n",
       "      <td>-0.627586</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.174248</td>\n",
       "      <td>0.174248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.436944</td>\n",
       "      <td>0.436944</td>\n",
       "      <td>0.122865</td>\n",
       "      <td>0.122865</td>\n",
       "      <td>-0.205006</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.019484</td>\n",
       "      <td>0.019484</td>\n",
       "      <td>-0.627586</td>\n",
       "      <td>0.019484</td>\n",
       "      <td>0.019484</td>\n",
       "      <td>-0.627586</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.174248</td>\n",
       "      <td>0.174248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.436944</td>\n",
       "      <td>0.436944</td>\n",
       "      <td>0.122865</td>\n",
       "      <td>0.122865</td>\n",
       "      <td>-0.205006</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.019484</td>\n",
       "      <td>0.019484</td>\n",
       "      <td>-0.627586</td>\n",
       "      <td>0.019484</td>\n",
       "      <td>0.019484</td>\n",
       "      <td>-0.627586</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.174248</td>\n",
       "      <td>0.174248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.436944</td>\n",
       "      <td>0.436944</td>\n",
       "      <td>0.122865</td>\n",
       "      <td>0.122865</td>\n",
       "      <td>-0.205006</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.019484</td>\n",
       "      <td>0.019484</td>\n",
       "      <td>-0.627586</td>\n",
       "      <td>0.019484</td>\n",
       "      <td>0.019484</td>\n",
       "      <td>-0.627586</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.174248</td>\n",
       "      <td>0.174248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-0.091709</td>\n",
       "      <td>-0.091709</td>\n",
       "      <td>0.098965</td>\n",
       "      <td>0.098965</td>\n",
       "      <td>0.057555</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.784052</td>\n",
       "      <td>0.784052</td>\n",
       "      <td>-0.785275</td>\n",
       "      <td>0.784052</td>\n",
       "      <td>0.784052</td>\n",
       "      <td>-0.785275</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.802526</td>\n",
       "      <td>-0.802526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-0.237460</td>\n",
       "      <td>-0.237460</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.217187</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.237460</td>\n",
       "      <td>-0.237460</td>\n",
       "      <td>0.880309</td>\n",
       "      <td>-0.237460</td>\n",
       "      <td>-0.237460</td>\n",
       "      <td>0.880309</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.217187</td>\n",
       "      <td>-0.217187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149970</th>\n",
       "      <td>0.260650</td>\n",
       "      <td>0.260650</td>\n",
       "      <td>0.361325</td>\n",
       "      <td>-0.033179</td>\n",
       "      <td>0.178913</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.075031</td>\n",
       "      <td>-0.090551</td>\n",
       "      <td>-0.146154</td>\n",
       "      <td>0.075031</td>\n",
       "      <td>-0.090551</td>\n",
       "      <td>-0.146154</td>\n",
       "      <td>0.113037</td>\n",
       "      <td>0.314990</td>\n",
       "      <td>0.455701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149971</th>\n",
       "      <td>0.416075</td>\n",
       "      <td>0.572480</td>\n",
       "      <td>0.037206</td>\n",
       "      <td>0.376692</td>\n",
       "      <td>0.502126</td>\n",
       "      <td>0.464722</td>\n",
       "      <td>-0.106579</td>\n",
       "      <td>0.188050</td>\n",
       "      <td>0.804149</td>\n",
       "      <td>0.039282</td>\n",
       "      <td>0.305001</td>\n",
       "      <td>0.578138</td>\n",
       "      <td>-0.034912</td>\n",
       "      <td>-0.071925</td>\n",
       "      <td>0.236674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149972</th>\n",
       "      <td>0.260650</td>\n",
       "      <td>0.260650</td>\n",
       "      <td>0.361325</td>\n",
       "      <td>-0.033179</td>\n",
       "      <td>0.178913</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.075031</td>\n",
       "      <td>-0.090551</td>\n",
       "      <td>-0.146154</td>\n",
       "      <td>0.075031</td>\n",
       "      <td>-0.090551</td>\n",
       "      <td>-0.146154</td>\n",
       "      <td>0.113037</td>\n",
       "      <td>0.314990</td>\n",
       "      <td>0.455701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149973</th>\n",
       "      <td>0.228585</td>\n",
       "      <td>0.033005</td>\n",
       "      <td>0.228806</td>\n",
       "      <td>0.054794</td>\n",
       "      <td>0.054266</td>\n",
       "      <td>0.572480</td>\n",
       "      <td>0.037206</td>\n",
       "      <td>0.376692</td>\n",
       "      <td>0.502126</td>\n",
       "      <td>0.039282</td>\n",
       "      <td>0.305001</td>\n",
       "      <td>0.578138</td>\n",
       "      <td>-0.034912</td>\n",
       "      <td>-0.071925</td>\n",
       "      <td>0.236674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149974</th>\n",
       "      <td>0.260650</td>\n",
       "      <td>0.260650</td>\n",
       "      <td>0.361325</td>\n",
       "      <td>-0.033179</td>\n",
       "      <td>0.178913</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.075031</td>\n",
       "      <td>-0.090551</td>\n",
       "      <td>-0.146154</td>\n",
       "      <td>0.075031</td>\n",
       "      <td>-0.090551</td>\n",
       "      <td>-0.146154</td>\n",
       "      <td>0.113037</td>\n",
       "      <td>0.314990</td>\n",
       "      <td>0.455701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149975</th>\n",
       "      <td>0.432065</td>\n",
       "      <td>-0.075737</td>\n",
       "      <td>0.196750</td>\n",
       "      <td>0.074731</td>\n",
       "      <td>0.041819</td>\n",
       "      <td>0.033005</td>\n",
       "      <td>0.228806</td>\n",
       "      <td>0.054794</td>\n",
       "      <td>0.054266</td>\n",
       "      <td>0.039282</td>\n",
       "      <td>0.305001</td>\n",
       "      <td>0.578138</td>\n",
       "      <td>-0.034912</td>\n",
       "      <td>-0.071925</td>\n",
       "      <td>0.236674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149976</th>\n",
       "      <td>0.260650</td>\n",
       "      <td>0.260650</td>\n",
       "      <td>0.361325</td>\n",
       "      <td>-0.033179</td>\n",
       "      <td>0.178913</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.075031</td>\n",
       "      <td>-0.090551</td>\n",
       "      <td>-0.146154</td>\n",
       "      <td>0.075031</td>\n",
       "      <td>-0.090551</td>\n",
       "      <td>-0.146154</td>\n",
       "      <td>0.113037</td>\n",
       "      <td>0.314990</td>\n",
       "      <td>0.455701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149977</th>\n",
       "      <td>-0.062263</td>\n",
       "      <td>-0.062263</td>\n",
       "      <td>0.080139</td>\n",
       "      <td>0.234991</td>\n",
       "      <td>-0.077157</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.039282</td>\n",
       "      <td>-0.690870</td>\n",
       "      <td>0.464722</td>\n",
       "      <td>0.039282</td>\n",
       "      <td>-0.690870</td>\n",
       "      <td>0.464722</td>\n",
       "      <td>0.122160</td>\n",
       "      <td>-0.106579</td>\n",
       "      <td>-0.309755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149978</th>\n",
       "      <td>-0.086927</td>\n",
       "      <td>0.385102</td>\n",
       "      <td>0.235047</td>\n",
       "      <td>0.243910</td>\n",
       "      <td>0.028053</td>\n",
       "      <td>0.037691</td>\n",
       "      <td>-0.144684</td>\n",
       "      <td>-0.025378</td>\n",
       "      <td>-0.457406</td>\n",
       "      <td>0.075031</td>\n",
       "      <td>0.172552</td>\n",
       "      <td>-0.090551</td>\n",
       "      <td>0.403070</td>\n",
       "      <td>0.113037</td>\n",
       "      <td>0.057793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149979</th>\n",
       "      <td>-0.062263</td>\n",
       "      <td>-0.062263</td>\n",
       "      <td>0.080139</td>\n",
       "      <td>0.234991</td>\n",
       "      <td>-0.077157</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.039282</td>\n",
       "      <td>-0.690870</td>\n",
       "      <td>0.464722</td>\n",
       "      <td>0.039282</td>\n",
       "      <td>-0.690870</td>\n",
       "      <td>0.464722</td>\n",
       "      <td>0.122160</td>\n",
       "      <td>-0.106579</td>\n",
       "      <td>-0.309755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149980</th>\n",
       "      <td>0.445321</td>\n",
       "      <td>0.343934</td>\n",
       "      <td>0.201220</td>\n",
       "      <td>0.209033</td>\n",
       "      <td>0.006116</td>\n",
       "      <td>0.385102</td>\n",
       "      <td>0.235047</td>\n",
       "      <td>0.243910</td>\n",
       "      <td>0.028053</td>\n",
       "      <td>0.075031</td>\n",
       "      <td>0.172552</td>\n",
       "      <td>-0.090551</td>\n",
       "      <td>0.403070</td>\n",
       "      <td>0.113037</td>\n",
       "      <td>0.057793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149981</th>\n",
       "      <td>-0.062263</td>\n",
       "      <td>-0.062263</td>\n",
       "      <td>0.080139</td>\n",
       "      <td>0.234991</td>\n",
       "      <td>-0.077157</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.039282</td>\n",
       "      <td>-0.690870</td>\n",
       "      <td>0.464722</td>\n",
       "      <td>0.039282</td>\n",
       "      <td>-0.690870</td>\n",
       "      <td>0.464722</td>\n",
       "      <td>0.122160</td>\n",
       "      <td>-0.106579</td>\n",
       "      <td>-0.309755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149982</th>\n",
       "      <td>0.079815</td>\n",
       "      <td>0.028486</td>\n",
       "      <td>0.257777</td>\n",
       "      <td>0.166056</td>\n",
       "      <td>0.020414</td>\n",
       "      <td>0.343934</td>\n",
       "      <td>0.201220</td>\n",
       "      <td>0.209033</td>\n",
       "      <td>0.006116</td>\n",
       "      <td>0.075031</td>\n",
       "      <td>0.172552</td>\n",
       "      <td>-0.090551</td>\n",
       "      <td>0.403070</td>\n",
       "      <td>0.113037</td>\n",
       "      <td>0.057793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149983</th>\n",
       "      <td>0.166632</td>\n",
       "      <td>-0.194921</td>\n",
       "      <td>0.426223</td>\n",
       "      <td>0.426223</td>\n",
       "      <td>0.134720</td>\n",
       "      <td>-0.062263</td>\n",
       "      <td>0.080139</td>\n",
       "      <td>0.080139</td>\n",
       "      <td>-0.147224</td>\n",
       "      <td>0.039282</td>\n",
       "      <td>0.039282</td>\n",
       "      <td>0.305001</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.034912</td>\n",
       "      <td>-0.034912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149984</th>\n",
       "      <td>-0.086927</td>\n",
       "      <td>0.385102</td>\n",
       "      <td>0.235047</td>\n",
       "      <td>0.243910</td>\n",
       "      <td>0.028053</td>\n",
       "      <td>0.037691</td>\n",
       "      <td>-0.144684</td>\n",
       "      <td>-0.025378</td>\n",
       "      <td>-0.457406</td>\n",
       "      <td>0.075031</td>\n",
       "      <td>0.172552</td>\n",
       "      <td>-0.090551</td>\n",
       "      <td>0.403070</td>\n",
       "      <td>0.113037</td>\n",
       "      <td>0.057793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149985</th>\n",
       "      <td>0.166632</td>\n",
       "      <td>-0.194921</td>\n",
       "      <td>0.426223</td>\n",
       "      <td>0.426223</td>\n",
       "      <td>0.134720</td>\n",
       "      <td>-0.062263</td>\n",
       "      <td>0.080139</td>\n",
       "      <td>0.080139</td>\n",
       "      <td>-0.147224</td>\n",
       "      <td>0.039282</td>\n",
       "      <td>0.039282</td>\n",
       "      <td>0.305001</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.034912</td>\n",
       "      <td>-0.034912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149986</th>\n",
       "      <td>0.445321</td>\n",
       "      <td>0.343934</td>\n",
       "      <td>0.201220</td>\n",
       "      <td>0.209033</td>\n",
       "      <td>0.006116</td>\n",
       "      <td>0.385102</td>\n",
       "      <td>0.235047</td>\n",
       "      <td>0.243910</td>\n",
       "      <td>0.028053</td>\n",
       "      <td>0.075031</td>\n",
       "      <td>0.172552</td>\n",
       "      <td>-0.090551</td>\n",
       "      <td>0.403070</td>\n",
       "      <td>0.113037</td>\n",
       "      <td>0.057793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149987</th>\n",
       "      <td>-0.062263</td>\n",
       "      <td>-0.062263</td>\n",
       "      <td>0.080139</td>\n",
       "      <td>0.050598</td>\n",
       "      <td>0.117918</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.039282</td>\n",
       "      <td>0.578138</td>\n",
       "      <td>0.197091</td>\n",
       "      <td>0.039282</td>\n",
       "      <td>0.578138</td>\n",
       "      <td>0.197091</td>\n",
       "      <td>-0.071925</td>\n",
       "      <td>-0.005404</td>\n",
       "      <td>0.524542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149988</th>\n",
       "      <td>-0.086927</td>\n",
       "      <td>0.385102</td>\n",
       "      <td>0.235047</td>\n",
       "      <td>0.243910</td>\n",
       "      <td>0.028053</td>\n",
       "      <td>0.037691</td>\n",
       "      <td>-0.144684</td>\n",
       "      <td>-0.025378</td>\n",
       "      <td>-0.457406</td>\n",
       "      <td>0.075031</td>\n",
       "      <td>0.172552</td>\n",
       "      <td>-0.090551</td>\n",
       "      <td>0.403070</td>\n",
       "      <td>0.113037</td>\n",
       "      <td>0.057793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149989</th>\n",
       "      <td>-0.062263</td>\n",
       "      <td>-0.062263</td>\n",
       "      <td>0.080139</td>\n",
       "      <td>0.234991</td>\n",
       "      <td>-0.077157</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.039282</td>\n",
       "      <td>-0.690870</td>\n",
       "      <td>0.464722</td>\n",
       "      <td>0.039282</td>\n",
       "      <td>-0.690870</td>\n",
       "      <td>0.464722</td>\n",
       "      <td>0.122160</td>\n",
       "      <td>-0.106579</td>\n",
       "      <td>-0.309755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149990</th>\n",
       "      <td>0.286263</td>\n",
       "      <td>-0.012283</td>\n",
       "      <td>0.322190</td>\n",
       "      <td>0.133347</td>\n",
       "      <td>0.032069</td>\n",
       "      <td>0.028486</td>\n",
       "      <td>0.257777</td>\n",
       "      <td>0.166056</td>\n",
       "      <td>0.020414</td>\n",
       "      <td>0.075031</td>\n",
       "      <td>0.172552</td>\n",
       "      <td>-0.090551</td>\n",
       "      <td>0.403070</td>\n",
       "      <td>0.113037</td>\n",
       "      <td>0.057793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149991</th>\n",
       "      <td>0.080723</td>\n",
       "      <td>0.464722</td>\n",
       "      <td>-0.106579</td>\n",
       "      <td>-0.106579</td>\n",
       "      <td>0.188050</td>\n",
       "      <td>-0.194921</td>\n",
       "      <td>0.426223</td>\n",
       "      <td>0.426223</td>\n",
       "      <td>0.134720</td>\n",
       "      <td>0.039282</td>\n",
       "      <td>0.039282</td>\n",
       "      <td>0.305001</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.034912</td>\n",
       "      <td>-0.034912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149992</th>\n",
       "      <td>-0.086927</td>\n",
       "      <td>0.385102</td>\n",
       "      <td>0.235047</td>\n",
       "      <td>0.243910</td>\n",
       "      <td>0.028053</td>\n",
       "      <td>0.037691</td>\n",
       "      <td>-0.144684</td>\n",
       "      <td>-0.025378</td>\n",
       "      <td>-0.457406</td>\n",
       "      <td>0.075031</td>\n",
       "      <td>0.172552</td>\n",
       "      <td>-0.090551</td>\n",
       "      <td>0.403070</td>\n",
       "      <td>0.113037</td>\n",
       "      <td>0.057793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149993</th>\n",
       "      <td>-0.062263</td>\n",
       "      <td>-0.062263</td>\n",
       "      <td>0.080139</td>\n",
       "      <td>0.117918</td>\n",
       "      <td>0.234991</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.039282</td>\n",
       "      <td>0.197091</td>\n",
       "      <td>-0.690870</td>\n",
       "      <td>0.039282</td>\n",
       "      <td>0.197091</td>\n",
       "      <td>-0.690870</td>\n",
       "      <td>-0.005404</td>\n",
       "      <td>0.122160</td>\n",
       "      <td>-0.082016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149994</th>\n",
       "      <td>-0.086927</td>\n",
       "      <td>0.385102</td>\n",
       "      <td>0.235047</td>\n",
       "      <td>0.243910</td>\n",
       "      <td>0.028053</td>\n",
       "      <td>0.037691</td>\n",
       "      <td>-0.144684</td>\n",
       "      <td>-0.025378</td>\n",
       "      <td>-0.457406</td>\n",
       "      <td>0.075031</td>\n",
       "      <td>0.172552</td>\n",
       "      <td>-0.090551</td>\n",
       "      <td>0.403070</td>\n",
       "      <td>0.113037</td>\n",
       "      <td>0.057793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149995</th>\n",
       "      <td>-0.062263</td>\n",
       "      <td>-0.062263</td>\n",
       "      <td>0.080139</td>\n",
       "      <td>0.050598</td>\n",
       "      <td>0.117918</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.039282</td>\n",
       "      <td>0.578138</td>\n",
       "      <td>0.197091</td>\n",
       "      <td>0.039282</td>\n",
       "      <td>0.578138</td>\n",
       "      <td>0.197091</td>\n",
       "      <td>-0.071925</td>\n",
       "      <td>-0.005404</td>\n",
       "      <td>0.524542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149996</th>\n",
       "      <td>0.445321</td>\n",
       "      <td>0.343934</td>\n",
       "      <td>0.201220</td>\n",
       "      <td>0.209033</td>\n",
       "      <td>0.006116</td>\n",
       "      <td>0.385102</td>\n",
       "      <td>0.235047</td>\n",
       "      <td>0.243910</td>\n",
       "      <td>0.028053</td>\n",
       "      <td>0.075031</td>\n",
       "      <td>0.172552</td>\n",
       "      <td>-0.090551</td>\n",
       "      <td>0.403070</td>\n",
       "      <td>0.113037</td>\n",
       "      <td>0.057793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149997</th>\n",
       "      <td>0.416075</td>\n",
       "      <td>0.572480</td>\n",
       "      <td>0.037206</td>\n",
       "      <td>0.037206</td>\n",
       "      <td>0.376692</td>\n",
       "      <td>0.464722</td>\n",
       "      <td>-0.106579</td>\n",
       "      <td>-0.106579</td>\n",
       "      <td>0.188050</td>\n",
       "      <td>0.039282</td>\n",
       "      <td>0.039282</td>\n",
       "      <td>0.305001</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.034912</td>\n",
       "      <td>-0.034912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149998</th>\n",
       "      <td>-0.086927</td>\n",
       "      <td>0.385102</td>\n",
       "      <td>0.235047</td>\n",
       "      <td>0.243910</td>\n",
       "      <td>0.028053</td>\n",
       "      <td>0.037691</td>\n",
       "      <td>-0.144684</td>\n",
       "      <td>-0.025378</td>\n",
       "      <td>-0.457406</td>\n",
       "      <td>0.075031</td>\n",
       "      <td>0.172552</td>\n",
       "      <td>-0.090551</td>\n",
       "      <td>0.403070</td>\n",
       "      <td>0.113037</td>\n",
       "      <td>0.057793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149999</th>\n",
       "      <td>-0.062263</td>\n",
       "      <td>-0.062263</td>\n",
       "      <td>0.080139</td>\n",
       "      <td>-0.077157</td>\n",
       "      <td>0.089700</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.039282</td>\n",
       "      <td>0.464722</td>\n",
       "      <td>-0.629822</td>\n",
       "      <td>0.039282</td>\n",
       "      <td>0.464722</td>\n",
       "      <td>-0.629822</td>\n",
       "      <td>-0.106579</td>\n",
       "      <td>0.157681</td>\n",
       "      <td>-0.778588</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150000 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0         1         2         3         4         5         6  \\\n",
       "0       0.078186  0.078186  0.093237  0.093237  0.187492  1.000000  0.082442   \n",
       "1       0.078186  0.078186  0.093237  0.140478  0.053903  1.000000  0.082442   \n",
       "2       0.179263  0.043238  0.133389  0.133389  0.120529  0.150082  0.081872   \n",
       "3       0.652996  0.150082  0.081872  0.081872  0.200082  0.078186  0.093237   \n",
       "4       0.179263  0.043238  0.133389  0.120529  1.000000  0.150082  0.081872   \n",
       "5       0.652996  0.150082  0.081872  0.200082  0.179263  0.078186  0.093237   \n",
       "6       0.179263  0.043238  0.133389  1.000000  0.161728  0.150082  0.081872   \n",
       "7       0.179263  0.043238  0.133389  1.000000  0.161728  0.150082  0.081872   \n",
       "8       0.652996  0.150082  0.081872  0.179263 -0.049452  0.078186  0.093237   \n",
       "9       0.078186  0.078186  0.093237  0.187492  0.140478  1.000000  0.082442   \n",
       "10      0.078186  0.078186  0.093237  0.187492  0.140478  1.000000  0.082442   \n",
       "11      0.078186  0.078186  0.093237  0.187492  0.140478  1.000000  0.082442   \n",
       "12      0.078186  0.078186  0.093237  0.187492  0.140478  1.000000  0.082442   \n",
       "13      0.078186  0.078186  0.093237  0.187492  0.140478  1.000000  0.082442   \n",
       "14      0.078186  0.078186  0.093237  0.187492  0.140478  1.000000  0.082442   \n",
       "15      0.078186  0.078186  0.093237  0.187492  0.140478  1.000000  0.082442   \n",
       "16      0.078186  0.078186  0.093237  0.187492  0.140478  1.000000  0.082442   \n",
       "17      0.078186  0.078186  0.093237  0.187492  0.140478  1.000000  0.082442   \n",
       "18      0.277971  0.538989  0.133949  0.133949 -0.273742  0.436944  0.122865   \n",
       "19      0.158540  0.141801  0.102586  0.102586  0.112341  0.538989  0.133949   \n",
       "20      0.241893 -0.191597  0.024898  0.024898  0.279824  0.141801  0.102586   \n",
       "21      0.086573  0.219403  0.124957  0.124957 -0.020654 -0.191597  0.024898   \n",
       "22     -0.007975  0.012663  0.258598  0.258598  0.130370  0.219403  0.124957   \n",
       "23      0.436944  0.436944  0.122865  0.122865 -0.205006  1.000000  0.019484   \n",
       "24      0.436944  0.436944  0.122865  0.122865 -0.205006  1.000000  0.019484   \n",
       "25      0.436944  0.436944  0.122865  0.122865 -0.205006  1.000000  0.019484   \n",
       "26      0.436944  0.436944  0.122865  0.122865 -0.205006  1.000000  0.019484   \n",
       "27      0.436944  0.436944  0.122865  0.122865 -0.205006  1.000000  0.019484   \n",
       "28     -0.091709 -0.091709  0.098965  0.098965  0.057555  1.000000  0.784052   \n",
       "29     -0.237460 -0.237460  1.000000  1.000000 -0.217187  1.000000 -0.237460   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "149970  0.260650  0.260650  0.361325 -0.033179  0.178913  1.000000  0.075031   \n",
       "149971  0.416075  0.572480  0.037206  0.376692  0.502126  0.464722 -0.106579   \n",
       "149972  0.260650  0.260650  0.361325 -0.033179  0.178913  1.000000  0.075031   \n",
       "149973  0.228585  0.033005  0.228806  0.054794  0.054266  0.572480  0.037206   \n",
       "149974  0.260650  0.260650  0.361325 -0.033179  0.178913  1.000000  0.075031   \n",
       "149975  0.432065 -0.075737  0.196750  0.074731  0.041819  0.033005  0.228806   \n",
       "149976  0.260650  0.260650  0.361325 -0.033179  0.178913  1.000000  0.075031   \n",
       "149977 -0.062263 -0.062263  0.080139  0.234991 -0.077157  1.000000  0.039282   \n",
       "149978 -0.086927  0.385102  0.235047  0.243910  0.028053  0.037691 -0.144684   \n",
       "149979 -0.062263 -0.062263  0.080139  0.234991 -0.077157  1.000000  0.039282   \n",
       "149980  0.445321  0.343934  0.201220  0.209033  0.006116  0.385102  0.235047   \n",
       "149981 -0.062263 -0.062263  0.080139  0.234991 -0.077157  1.000000  0.039282   \n",
       "149982  0.079815  0.028486  0.257777  0.166056  0.020414  0.343934  0.201220   \n",
       "149983  0.166632 -0.194921  0.426223  0.426223  0.134720 -0.062263  0.080139   \n",
       "149984 -0.086927  0.385102  0.235047  0.243910  0.028053  0.037691 -0.144684   \n",
       "149985  0.166632 -0.194921  0.426223  0.426223  0.134720 -0.062263  0.080139   \n",
       "149986  0.445321  0.343934  0.201220  0.209033  0.006116  0.385102  0.235047   \n",
       "149987 -0.062263 -0.062263  0.080139  0.050598  0.117918  1.000000  0.039282   \n",
       "149988 -0.086927  0.385102  0.235047  0.243910  0.028053  0.037691 -0.144684   \n",
       "149989 -0.062263 -0.062263  0.080139  0.234991 -0.077157  1.000000  0.039282   \n",
       "149990  0.286263 -0.012283  0.322190  0.133347  0.032069  0.028486  0.257777   \n",
       "149991  0.080723  0.464722 -0.106579 -0.106579  0.188050 -0.194921  0.426223   \n",
       "149992 -0.086927  0.385102  0.235047  0.243910  0.028053  0.037691 -0.144684   \n",
       "149993 -0.062263 -0.062263  0.080139  0.117918  0.234991  1.000000  0.039282   \n",
       "149994 -0.086927  0.385102  0.235047  0.243910  0.028053  0.037691 -0.144684   \n",
       "149995 -0.062263 -0.062263  0.080139  0.050598  0.117918  1.000000  0.039282   \n",
       "149996  0.445321  0.343934  0.201220  0.209033  0.006116  0.385102  0.235047   \n",
       "149997  0.416075  0.572480  0.037206  0.037206  0.376692  0.464722 -0.106579   \n",
       "149998 -0.086927  0.385102  0.235047  0.243910  0.028053  0.037691 -0.144684   \n",
       "149999 -0.062263 -0.062263  0.080139 -0.077157  0.089700  1.000000  0.039282   \n",
       "\n",
       "               7         8         9        10        11        12        13  \\\n",
       "0       0.082442  0.566941  0.082442  0.082442  0.566941  1.000000  0.406256   \n",
       "1       0.043238 -0.547686  0.082442  0.043238 -0.547686  0.133389 -0.062813   \n",
       "2       0.081872  0.200082  0.082442  0.082442  0.566941  1.000000  0.406256   \n",
       "3       0.093237  0.187492  0.082442  0.082442  0.566941  1.000000  0.406256   \n",
       "4       0.200082  0.179263  0.082442  0.566941  0.043238  0.406256  0.133389   \n",
       "5       0.187492  0.140478  0.082442  0.566941  0.043238  0.406256  0.133389   \n",
       "6       0.179263 -0.049452  0.082442  0.043238 -0.547686  0.133389 -0.062813   \n",
       "7       0.179263 -0.049452  0.082442  0.043238 -0.547686  0.133389 -0.062813   \n",
       "8       0.140478  0.053903  0.082442  0.043238 -0.547686  0.133389 -0.062813   \n",
       "9       0.566941  0.043238  0.082442  0.566941  0.043238  0.406256  0.133389   \n",
       "10      0.566941  0.043238  0.082442  0.566941  0.043238  0.406256  0.133389   \n",
       "11      0.566941  0.043238  0.082442  0.566941  0.043238  0.406256  0.133389   \n",
       "12      0.566941  0.043238  0.082442  0.566941  0.043238  0.406256  0.133389   \n",
       "13      0.566941  0.043238  0.082442  0.566941  0.043238  0.406256  0.133389   \n",
       "14      0.566941  0.043238  0.082442  0.566941  0.043238  0.406256  0.133389   \n",
       "15      0.566941  0.043238  0.082442  0.566941  0.043238  0.406256  0.133389   \n",
       "16      0.566941  0.043238  0.082442  0.566941  0.043238  0.406256  0.133389   \n",
       "17      0.566941  0.043238  0.082442  0.566941  0.043238  0.406256  0.133389   \n",
       "18      0.122865 -0.205006  0.019484  0.019484 -0.627586  1.000000  0.174248   \n",
       "19      0.133949 -0.273742  0.019484  0.019484 -0.627586  1.000000  0.174248   \n",
       "20      0.102586  0.112341  0.019484  0.019484 -0.627586  1.000000  0.174248   \n",
       "21      0.024898  0.279824  0.019484  0.019484 -0.627586  1.000000  0.174248   \n",
       "22      0.124957 -0.020654  0.019484  0.019484 -0.627586  1.000000  0.174248   \n",
       "23      0.019484 -0.627586  0.019484  0.019484 -0.627586  1.000000  0.174248   \n",
       "24      0.019484 -0.627586  0.019484  0.019484 -0.627586  1.000000  0.174248   \n",
       "25      0.019484 -0.627586  0.019484  0.019484 -0.627586  1.000000  0.174248   \n",
       "26      0.019484 -0.627586  0.019484  0.019484 -0.627586  1.000000  0.174248   \n",
       "27      0.019484 -0.627586  0.019484  0.019484 -0.627586  1.000000  0.174248   \n",
       "28      0.784052 -0.785275  0.784052  0.784052 -0.785275  1.000000 -0.802526   \n",
       "29     -0.237460  0.880309 -0.237460 -0.237460  0.880309  1.000000 -0.217187   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "149970 -0.090551 -0.146154  0.075031 -0.090551 -0.146154  0.113037  0.314990   \n",
       "149971  0.188050  0.804149  0.039282  0.305001  0.578138 -0.034912 -0.071925   \n",
       "149972 -0.090551 -0.146154  0.075031 -0.090551 -0.146154  0.113037  0.314990   \n",
       "149973  0.376692  0.502126  0.039282  0.305001  0.578138 -0.034912 -0.071925   \n",
       "149974 -0.090551 -0.146154  0.075031 -0.090551 -0.146154  0.113037  0.314990   \n",
       "149975  0.054794  0.054266  0.039282  0.305001  0.578138 -0.034912 -0.071925   \n",
       "149976 -0.090551 -0.146154  0.075031 -0.090551 -0.146154  0.113037  0.314990   \n",
       "149977 -0.690870  0.464722  0.039282 -0.690870  0.464722  0.122160 -0.106579   \n",
       "149978 -0.025378 -0.457406  0.075031  0.172552 -0.090551  0.403070  0.113037   \n",
       "149979 -0.690870  0.464722  0.039282 -0.690870  0.464722  0.122160 -0.106579   \n",
       "149980  0.243910  0.028053  0.075031  0.172552 -0.090551  0.403070  0.113037   \n",
       "149981 -0.690870  0.464722  0.039282 -0.690870  0.464722  0.122160 -0.106579   \n",
       "149982  0.209033  0.006116  0.075031  0.172552 -0.090551  0.403070  0.113037   \n",
       "149983  0.080139 -0.147224  0.039282  0.039282  0.305001  1.000000 -0.034912   \n",
       "149984 -0.025378 -0.457406  0.075031  0.172552 -0.090551  0.403070  0.113037   \n",
       "149985  0.080139 -0.147224  0.039282  0.039282  0.305001  1.000000 -0.034912   \n",
       "149986  0.243910  0.028053  0.075031  0.172552 -0.090551  0.403070  0.113037   \n",
       "149987  0.578138  0.197091  0.039282  0.578138  0.197091 -0.071925 -0.005404   \n",
       "149988 -0.025378 -0.457406  0.075031  0.172552 -0.090551  0.403070  0.113037   \n",
       "149989 -0.690870  0.464722  0.039282 -0.690870  0.464722  0.122160 -0.106579   \n",
       "149990  0.166056  0.020414  0.075031  0.172552 -0.090551  0.403070  0.113037   \n",
       "149991  0.426223  0.134720  0.039282  0.039282  0.305001  1.000000 -0.034912   \n",
       "149992 -0.025378 -0.457406  0.075031  0.172552 -0.090551  0.403070  0.113037   \n",
       "149993  0.197091 -0.690870  0.039282  0.197091 -0.690870 -0.005404  0.122160   \n",
       "149994 -0.025378 -0.457406  0.075031  0.172552 -0.090551  0.403070  0.113037   \n",
       "149995  0.578138  0.197091  0.039282  0.578138  0.197091 -0.071925 -0.005404   \n",
       "149996  0.243910  0.028053  0.075031  0.172552 -0.090551  0.403070  0.113037   \n",
       "149997 -0.106579  0.188050  0.039282  0.039282  0.305001  1.000000 -0.034912   \n",
       "149998 -0.025378 -0.457406  0.075031  0.172552 -0.090551  0.403070  0.113037   \n",
       "149999  0.464722 -0.629822  0.039282  0.464722 -0.629822 -0.106579  0.157681   \n",
       "\n",
       "              14  \n",
       "0       0.406256  \n",
       "1       0.161728  \n",
       "2       0.406256  \n",
       "3       0.406256  \n",
       "4       0.120529  \n",
       "5       0.120529  \n",
       "6       0.161728  \n",
       "7       0.161728  \n",
       "8       0.161728  \n",
       "9       0.120529  \n",
       "10      0.120529  \n",
       "11      0.120529  \n",
       "12      0.120529  \n",
       "13      0.120529  \n",
       "14      0.120529  \n",
       "15      0.120529  \n",
       "16      0.120529  \n",
       "17      0.120529  \n",
       "18      0.174248  \n",
       "19      0.174248  \n",
       "20      0.174248  \n",
       "21      0.174248  \n",
       "22      0.174248  \n",
       "23      0.174248  \n",
       "24      0.174248  \n",
       "25      0.174248  \n",
       "26      0.174248  \n",
       "27      0.174248  \n",
       "28     -0.802526  \n",
       "29     -0.217187  \n",
       "...          ...  \n",
       "149970  0.455701  \n",
       "149971  0.236674  \n",
       "149972  0.455701  \n",
       "149973  0.236674  \n",
       "149974  0.455701  \n",
       "149975  0.236674  \n",
       "149976  0.455701  \n",
       "149977 -0.309755  \n",
       "149978  0.057793  \n",
       "149979 -0.309755  \n",
       "149980  0.057793  \n",
       "149981 -0.309755  \n",
       "149982  0.057793  \n",
       "149983 -0.034912  \n",
       "149984  0.057793  \n",
       "149985 -0.034912  \n",
       "149986  0.057793  \n",
       "149987  0.524542  \n",
       "149988  0.057793  \n",
       "149989 -0.309755  \n",
       "149990  0.057793  \n",
       "149991 -0.034912  \n",
       "149992  0.057793  \n",
       "149993 -0.082016  \n",
       "149994  0.057793  \n",
       "149995  0.524542  \n",
       "149996  0.057793  \n",
       "149997 -0.034912  \n",
       "149998  0.057793  \n",
       "149999 -0.778588  \n",
       "\n",
       "[150000 rows x 15 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_word_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "type_df1 = pd.read_csv('train_type_dis1.csv')\n",
    "type_df2 = pd.read_csv('train_type_dis2.csv')\n",
    "type_df2.index = type_df2.index+50000\n",
    "type_df3 = pd.read_csv('train_type_dis3.csv')\n",
    "type_df3.index = type_df3.index+100000\n",
    "train_type_df = pd.concat([type_df1,type_df2,type_df3])\n",
    "del train_type_df['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.186247</td>\n",
       "      <td>0.186247</td>\n",
       "      <td>0.186247</td>\n",
       "      <td>0.186247</td>\n",
       "      <td>0.186247</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.186247</td>\n",
       "      <td>0.186247</td>\n",
       "      <td>0.186247</td>\n",
       "      <td>-0.111000</td>\n",
       "      <td>0.373484</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>0.368714</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>0.368714</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>0.368714</td>\n",
       "      <td>0.219010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.111000</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>0.186247</td>\n",
       "      <td>0.186247</td>\n",
       "      <td>0.186247</td>\n",
       "      <td>0.186247</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.186247</td>\n",
       "      <td>0.186247</td>\n",
       "      <td>0.186247</td>\n",
       "      <td>0.186247</td>\n",
       "      <td>0.186247</td>\n",
       "      <td>0.186247</td>\n",
       "      <td>0.186247</td>\n",
       "      <td>0.186247</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.111000</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.186247</td>\n",
       "      <td>0.186247</td>\n",
       "      <td>0.186247</td>\n",
       "      <td>-0.111000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>0.490511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.186247</td>\n",
       "      <td>0.186247</td>\n",
       "      <td>0.186247</td>\n",
       "      <td>-0.111000</td>\n",
       "      <td>0.186247</td>\n",
       "      <td>0.186247</td>\n",
       "      <td>0.186247</td>\n",
       "      <td>-0.111000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>0.490511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.111000</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.219010</td>\n",
       "      <td>0.186247</td>\n",
       "      <td>0.186247</td>\n",
       "      <td>-0.111000</td>\n",
       "      <td>0.373484</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>0.368714</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>0.368714</td>\n",
       "      <td>0.219010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.111000</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.219010</td>\n",
       "      <td>0.186247</td>\n",
       "      <td>0.186247</td>\n",
       "      <td>-0.111000</td>\n",
       "      <td>0.373484</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>0.368714</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>0.368714</td>\n",
       "      <td>0.219010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.186247</td>\n",
       "      <td>0.186247</td>\n",
       "      <td>-0.111000</td>\n",
       "      <td>0.373484</td>\n",
       "      <td>0.186247</td>\n",
       "      <td>0.186247</td>\n",
       "      <td>-0.111000</td>\n",
       "      <td>0.373484</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>0.368714</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>0.368714</td>\n",
       "      <td>0.219010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.186247</td>\n",
       "      <td>0.186247</td>\n",
       "      <td>0.186247</td>\n",
       "      <td>0.186247</td>\n",
       "      <td>-0.111000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>0.490511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.186247</td>\n",
       "      <td>0.186247</td>\n",
       "      <td>0.186247</td>\n",
       "      <td>0.186247</td>\n",
       "      <td>-0.111000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>0.490511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.186247</td>\n",
       "      <td>0.186247</td>\n",
       "      <td>0.186247</td>\n",
       "      <td>0.186247</td>\n",
       "      <td>-0.111000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>0.490511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.186247</td>\n",
       "      <td>0.186247</td>\n",
       "      <td>0.186247</td>\n",
       "      <td>0.186247</td>\n",
       "      <td>-0.111000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>0.490511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.186247</td>\n",
       "      <td>0.186247</td>\n",
       "      <td>0.186247</td>\n",
       "      <td>0.186247</td>\n",
       "      <td>-0.111000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>0.490511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.186247</td>\n",
       "      <td>0.186247</td>\n",
       "      <td>0.186247</td>\n",
       "      <td>0.186247</td>\n",
       "      <td>-0.111000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>0.490511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.186247</td>\n",
       "      <td>0.186247</td>\n",
       "      <td>0.186247</td>\n",
       "      <td>0.186247</td>\n",
       "      <td>-0.111000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>0.490511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.186247</td>\n",
       "      <td>0.186247</td>\n",
       "      <td>0.186247</td>\n",
       "      <td>0.186247</td>\n",
       "      <td>-0.111000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>0.490511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.186247</td>\n",
       "      <td>0.186247</td>\n",
       "      <td>0.186247</td>\n",
       "      <td>0.186247</td>\n",
       "      <td>-0.111000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>0.490511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.205712</td>\n",
       "      <td>-0.523517</td>\n",
       "      <td>-0.523517</td>\n",
       "      <td>-0.523517</td>\n",
       "      <td>0.045109</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>0.360336</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.468910</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.468910</td>\n",
       "      <td>0.468910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.205712</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>0.360336</td>\n",
       "      <td>-0.523517</td>\n",
       "      <td>-0.523517</td>\n",
       "      <td>-0.523517</td>\n",
       "      <td>0.045109</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.468910</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.468910</td>\n",
       "      <td>0.468910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>0.360336</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>0.360336</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.468910</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.468910</td>\n",
       "      <td>0.468910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.422776</td>\n",
       "      <td>0.536819</td>\n",
       "      <td>0.536819</td>\n",
       "      <td>0.536819</td>\n",
       "      <td>0.521830</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>0.360336</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.468910</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.468910</td>\n",
       "      <td>0.468910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.159761</td>\n",
       "      <td>0.559712</td>\n",
       "      <td>0.559712</td>\n",
       "      <td>0.559712</td>\n",
       "      <td>0.365542</td>\n",
       "      <td>0.536819</td>\n",
       "      <td>0.536819</td>\n",
       "      <td>0.536819</td>\n",
       "      <td>0.521830</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.468910</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.468910</td>\n",
       "      <td>0.468910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.490511</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>0.360336</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.468910</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.468910</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.468910</td>\n",
       "      <td>0.468910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.490511</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>0.360336</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.468910</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.468910</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.468910</td>\n",
       "      <td>0.468910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.490511</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>0.360336</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.468910</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.468910</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.468910</td>\n",
       "      <td>0.468910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.490511</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>0.360336</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.468910</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.468910</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.468910</td>\n",
       "      <td>0.468910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.490511</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>0.360336</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.468910</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.468910</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.468910</td>\n",
       "      <td>0.468910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-0.825199</td>\n",
       "      <td>-0.825199</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.825199</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.825199</td>\n",
       "      <td>-0.825199</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.825199</td>\n",
       "      <td>-0.825199</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.825199</td>\n",
       "      <td>-0.825199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149970</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.144094</td>\n",
       "      <td>0.368714</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.144094</td>\n",
       "      <td>0.368714</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.144094</td>\n",
       "      <td>0.368714</td>\n",
       "      <td>-0.144094</td>\n",
       "      <td>0.368714</td>\n",
       "      <td>0.039240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149971</th>\n",
       "      <td>-0.570782</td>\n",
       "      <td>-0.450803</td>\n",
       "      <td>0.536819</td>\n",
       "      <td>0.323540</td>\n",
       "      <td>-0.423814</td>\n",
       "      <td>0.853670</td>\n",
       "      <td>-0.912187</td>\n",
       "      <td>0.060199</td>\n",
       "      <td>0.809053</td>\n",
       "      <td>-0.825199</td>\n",
       "      <td>0.220871</td>\n",
       "      <td>0.981231</td>\n",
       "      <td>-0.144094</td>\n",
       "      <td>-0.806904</td>\n",
       "      <td>0.246684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149972</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.144094</td>\n",
       "      <td>0.368714</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.144094</td>\n",
       "      <td>0.368714</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.144094</td>\n",
       "      <td>0.368714</td>\n",
       "      <td>-0.144094</td>\n",
       "      <td>0.368714</td>\n",
       "      <td>0.039240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149973</th>\n",
       "      <td>0.536819</td>\n",
       "      <td>-0.825199</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.144094</td>\n",
       "      <td>-0.806904</td>\n",
       "      <td>-0.450803</td>\n",
       "      <td>0.536819</td>\n",
       "      <td>0.323540</td>\n",
       "      <td>-0.423814</td>\n",
       "      <td>-0.825199</td>\n",
       "      <td>0.220871</td>\n",
       "      <td>0.981231</td>\n",
       "      <td>-0.144094</td>\n",
       "      <td>-0.806904</td>\n",
       "      <td>0.246684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149974</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.144094</td>\n",
       "      <td>0.368714</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.144094</td>\n",
       "      <td>0.368714</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.144094</td>\n",
       "      <td>0.368714</td>\n",
       "      <td>-0.144094</td>\n",
       "      <td>0.368714</td>\n",
       "      <td>0.039240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149975</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.825199</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.144094</td>\n",
       "      <td>-0.806904</td>\n",
       "      <td>-0.825199</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.144094</td>\n",
       "      <td>-0.806904</td>\n",
       "      <td>-0.825199</td>\n",
       "      <td>0.220871</td>\n",
       "      <td>0.981231</td>\n",
       "      <td>-0.144094</td>\n",
       "      <td>-0.806904</td>\n",
       "      <td>0.246684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149976</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.144094</td>\n",
       "      <td>0.368714</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.144094</td>\n",
       "      <td>0.368714</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.144094</td>\n",
       "      <td>0.368714</td>\n",
       "      <td>-0.144094</td>\n",
       "      <td>0.368714</td>\n",
       "      <td>0.039240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149977</th>\n",
       "      <td>-0.422913</td>\n",
       "      <td>-0.422913</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>0.084777</td>\n",
       "      <td>-0.595948</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.825199</td>\n",
       "      <td>-0.900818</td>\n",
       "      <td>0.807903</td>\n",
       "      <td>-0.825199</td>\n",
       "      <td>-0.900818</td>\n",
       "      <td>0.807903</td>\n",
       "      <td>0.608865</td>\n",
       "      <td>-0.899872</td>\n",
       "      <td>-0.600251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149978</th>\n",
       "      <td>-0.912187</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.144094</td>\n",
       "      <td>-0.912187</td>\n",
       "      <td>-0.912187</td>\n",
       "      <td>-0.912187</td>\n",
       "      <td>0.060199</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.144094</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.144094</td>\n",
       "      <td>-0.144094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149979</th>\n",
       "      <td>-0.422913</td>\n",
       "      <td>-0.422913</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>0.084777</td>\n",
       "      <td>-0.595948</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.825199</td>\n",
       "      <td>-0.900818</td>\n",
       "      <td>0.807903</td>\n",
       "      <td>-0.825199</td>\n",
       "      <td>-0.900818</td>\n",
       "      <td>0.807903</td>\n",
       "      <td>0.608865</td>\n",
       "      <td>-0.899872</td>\n",
       "      <td>-0.600251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149980</th>\n",
       "      <td>-0.825199</td>\n",
       "      <td>-0.825199</td>\n",
       "      <td>-0.825199</td>\n",
       "      <td>-0.825199</td>\n",
       "      <td>0.220871</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.144094</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.144094</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.144094</td>\n",
       "      <td>-0.144094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149981</th>\n",
       "      <td>-0.422913</td>\n",
       "      <td>-0.422913</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>0.084777</td>\n",
       "      <td>-0.595948</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.825199</td>\n",
       "      <td>-0.900818</td>\n",
       "      <td>0.807903</td>\n",
       "      <td>-0.825199</td>\n",
       "      <td>-0.900818</td>\n",
       "      <td>0.807903</td>\n",
       "      <td>0.608865</td>\n",
       "      <td>-0.899872</td>\n",
       "      <td>-0.600251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149982</th>\n",
       "      <td>-0.422913</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>0.132984</td>\n",
       "      <td>-0.825199</td>\n",
       "      <td>-0.825199</td>\n",
       "      <td>-0.825199</td>\n",
       "      <td>0.220871</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.144094</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.144094</td>\n",
       "      <td>-0.144094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149983</th>\n",
       "      <td>0.490511</td>\n",
       "      <td>-0.825199</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.144094</td>\n",
       "      <td>-0.422913</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>0.132984</td>\n",
       "      <td>-0.825199</td>\n",
       "      <td>-0.825199</td>\n",
       "      <td>0.220871</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.144094</td>\n",
       "      <td>-0.144094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149984</th>\n",
       "      <td>-0.912187</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.144094</td>\n",
       "      <td>-0.912187</td>\n",
       "      <td>-0.912187</td>\n",
       "      <td>-0.912187</td>\n",
       "      <td>0.060199</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.144094</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.144094</td>\n",
       "      <td>-0.144094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149985</th>\n",
       "      <td>0.490511</td>\n",
       "      <td>-0.825199</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.144094</td>\n",
       "      <td>-0.422913</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>0.132984</td>\n",
       "      <td>-0.825199</td>\n",
       "      <td>-0.825199</td>\n",
       "      <td>0.220871</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.144094</td>\n",
       "      <td>-0.144094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149986</th>\n",
       "      <td>-0.825199</td>\n",
       "      <td>-0.825199</td>\n",
       "      <td>-0.825199</td>\n",
       "      <td>-0.825199</td>\n",
       "      <td>0.220871</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.144094</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.144094</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.144094</td>\n",
       "      <td>-0.144094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149987</th>\n",
       "      <td>-0.422913</td>\n",
       "      <td>-0.422913</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>-0.326951</td>\n",
       "      <td>-0.325346</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.825199</td>\n",
       "      <td>0.981231</td>\n",
       "      <td>0.978731</td>\n",
       "      <td>-0.825199</td>\n",
       "      <td>0.981231</td>\n",
       "      <td>0.978731</td>\n",
       "      <td>-0.806904</td>\n",
       "      <td>-0.769874</td>\n",
       "      <td>0.980221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149988</th>\n",
       "      <td>-0.912187</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.144094</td>\n",
       "      <td>-0.912187</td>\n",
       "      <td>-0.912187</td>\n",
       "      <td>-0.912187</td>\n",
       "      <td>0.060199</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.144094</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.144094</td>\n",
       "      <td>-0.144094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149989</th>\n",
       "      <td>-0.422913</td>\n",
       "      <td>-0.422913</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>0.084777</td>\n",
       "      <td>-0.595948</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.825199</td>\n",
       "      <td>-0.900818</td>\n",
       "      <td>0.807903</td>\n",
       "      <td>-0.825199</td>\n",
       "      <td>-0.900818</td>\n",
       "      <td>0.807903</td>\n",
       "      <td>0.608865</td>\n",
       "      <td>-0.899872</td>\n",
       "      <td>-0.600251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149990</th>\n",
       "      <td>0.422776</td>\n",
       "      <td>0.536819</td>\n",
       "      <td>0.536819</td>\n",
       "      <td>0.536819</td>\n",
       "      <td>0.323540</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>0.132984</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.144094</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.144094</td>\n",
       "      <td>-0.144094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149991</th>\n",
       "      <td>-0.912187</td>\n",
       "      <td>0.853670</td>\n",
       "      <td>-0.912187</td>\n",
       "      <td>-0.912187</td>\n",
       "      <td>0.060199</td>\n",
       "      <td>-0.825199</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.144094</td>\n",
       "      <td>-0.825199</td>\n",
       "      <td>-0.825199</td>\n",
       "      <td>0.220871</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.144094</td>\n",
       "      <td>-0.144094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149992</th>\n",
       "      <td>-0.912187</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.144094</td>\n",
       "      <td>-0.912187</td>\n",
       "      <td>-0.912187</td>\n",
       "      <td>-0.912187</td>\n",
       "      <td>0.060199</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.144094</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.144094</td>\n",
       "      <td>-0.144094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149993</th>\n",
       "      <td>-0.422913</td>\n",
       "      <td>-0.422913</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>-0.325346</td>\n",
       "      <td>0.084777</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.825199</td>\n",
       "      <td>0.978731</td>\n",
       "      <td>-0.900818</td>\n",
       "      <td>-0.825199</td>\n",
       "      <td>0.978731</td>\n",
       "      <td>-0.900818</td>\n",
       "      <td>-0.769874</td>\n",
       "      <td>0.608865</td>\n",
       "      <td>-0.928700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149994</th>\n",
       "      <td>-0.912187</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.144094</td>\n",
       "      <td>-0.912187</td>\n",
       "      <td>-0.912187</td>\n",
       "      <td>-0.912187</td>\n",
       "      <td>0.060199</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.144094</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.144094</td>\n",
       "      <td>-0.144094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149995</th>\n",
       "      <td>-0.422913</td>\n",
       "      <td>-0.422913</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>-0.326951</td>\n",
       "      <td>-0.325346</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.825199</td>\n",
       "      <td>0.981231</td>\n",
       "      <td>0.978731</td>\n",
       "      <td>-0.825199</td>\n",
       "      <td>0.981231</td>\n",
       "      <td>0.978731</td>\n",
       "      <td>-0.806904</td>\n",
       "      <td>-0.769874</td>\n",
       "      <td>0.980221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149996</th>\n",
       "      <td>-0.825199</td>\n",
       "      <td>-0.825199</td>\n",
       "      <td>-0.825199</td>\n",
       "      <td>-0.825199</td>\n",
       "      <td>0.220871</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.144094</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.144094</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.144094</td>\n",
       "      <td>-0.144094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149997</th>\n",
       "      <td>-0.570782</td>\n",
       "      <td>-0.450803</td>\n",
       "      <td>0.536819</td>\n",
       "      <td>0.536819</td>\n",
       "      <td>0.323540</td>\n",
       "      <td>0.853670</td>\n",
       "      <td>-0.912187</td>\n",
       "      <td>-0.912187</td>\n",
       "      <td>0.060199</td>\n",
       "      <td>-0.825199</td>\n",
       "      <td>-0.825199</td>\n",
       "      <td>0.220871</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.144094</td>\n",
       "      <td>-0.144094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149998</th>\n",
       "      <td>-0.912187</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.144094</td>\n",
       "      <td>-0.912187</td>\n",
       "      <td>-0.912187</td>\n",
       "      <td>-0.912187</td>\n",
       "      <td>0.060199</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.144094</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.144094</td>\n",
       "      <td>-0.144094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149999</th>\n",
       "      <td>-0.422913</td>\n",
       "      <td>-0.422913</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>-0.595948</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.825199</td>\n",
       "      <td>0.807903</td>\n",
       "      <td>-0.825199</td>\n",
       "      <td>-0.825199</td>\n",
       "      <td>0.807903</td>\n",
       "      <td>-0.825199</td>\n",
       "      <td>-0.899872</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.899872</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150000 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0         1         2         3         4         5         6  \\\n",
       "0       0.186247  0.186247  0.186247  0.186247  0.186247  1.000000  1.000000   \n",
       "1       0.186247  0.186247  0.186247 -0.111000  0.373484  1.000000  1.000000   \n",
       "2      -0.111000  0.490511  0.490511  0.490511  0.490511  0.186247  0.186247   \n",
       "3       1.000000  0.186247  0.186247  0.186247  0.186247  0.186247  0.186247   \n",
       "4      -0.111000  0.490511  0.490511  0.490511  1.000000  0.186247  0.186247   \n",
       "5       1.000000  0.186247  0.186247  0.186247 -0.111000  0.186247  0.186247   \n",
       "6      -0.111000  0.490511  0.490511  1.000000  0.219010  0.186247  0.186247   \n",
       "7      -0.111000  0.490511  0.490511  1.000000  0.219010  0.186247  0.186247   \n",
       "8       1.000000  0.186247  0.186247 -0.111000  0.373484  0.186247  0.186247   \n",
       "9       0.186247  0.186247  0.186247  0.186247 -0.111000  1.000000  1.000000   \n",
       "10      0.186247  0.186247  0.186247  0.186247 -0.111000  1.000000  1.000000   \n",
       "11      0.186247  0.186247  0.186247  0.186247 -0.111000  1.000000  1.000000   \n",
       "12      0.186247  0.186247  0.186247  0.186247 -0.111000  1.000000  1.000000   \n",
       "13      0.186247  0.186247  0.186247  0.186247 -0.111000  1.000000  1.000000   \n",
       "14      0.186247  0.186247  0.186247  0.186247 -0.111000  1.000000  1.000000   \n",
       "15      0.186247  0.186247  0.186247  0.186247 -0.111000  1.000000  1.000000   \n",
       "16      0.186247  0.186247  0.186247  0.186247 -0.111000  1.000000  1.000000   \n",
       "17      0.186247  0.186247  0.186247  0.186247 -0.111000  1.000000  1.000000   \n",
       "18     -0.205712 -0.523517 -0.523517 -0.523517  0.045109  0.490511  0.490511   \n",
       "19     -0.205712  0.490511  0.490511  0.490511  0.360336 -0.523517 -0.523517   \n",
       "20      1.000000  0.490511  0.490511  0.490511  0.360336  0.490511  0.490511   \n",
       "21      0.422776  0.536819  0.536819  0.536819  0.521830  0.490511  0.490511   \n",
       "22      0.159761  0.559712  0.559712  0.559712  0.365542  0.536819  0.536819   \n",
       "23      0.490511  0.490511  0.490511  0.490511  0.360336  1.000000  1.000000   \n",
       "24      0.490511  0.490511  0.490511  0.490511  0.360336  1.000000  1.000000   \n",
       "25      0.490511  0.490511  0.490511  0.490511  0.360336  1.000000  1.000000   \n",
       "26      0.490511  0.490511  0.490511  0.490511  0.360336  1.000000  1.000000   \n",
       "27      0.490511  0.490511  0.490511  0.490511  0.360336  1.000000  1.000000   \n",
       "28      1.000000  1.000000  1.000000  1.000000  1.000000  1.000000  1.000000   \n",
       "29     -0.825199 -0.825199  1.000000  1.000000 -0.825199  1.000000 -0.825199   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "149970  1.000000  1.000000  1.000000 -0.144094  0.368714  1.000000  1.000000   \n",
       "149971 -0.570782 -0.450803  0.536819  0.323540 -0.423814  0.853670 -0.912187   \n",
       "149972  1.000000  1.000000  1.000000 -0.144094  0.368714  1.000000  1.000000   \n",
       "149973  0.536819 -0.825199  1.000000 -0.144094 -0.806904 -0.450803  0.536819   \n",
       "149974  1.000000  1.000000  1.000000 -0.144094  0.368714  1.000000  1.000000   \n",
       "149975  1.000000 -0.825199  1.000000 -0.144094 -0.806904 -0.825199  1.000000   \n",
       "149976  1.000000  1.000000  1.000000 -0.144094  0.368714  1.000000  1.000000   \n",
       "149977 -0.422913 -0.422913  0.490511  0.084777 -0.595948  1.000000 -0.825199   \n",
       "149978 -0.912187  1.000000  1.000000  1.000000 -0.144094 -0.912187 -0.912187   \n",
       "149979 -0.422913 -0.422913  0.490511  0.084777 -0.595948  1.000000 -0.825199   \n",
       "149980 -0.825199 -0.825199 -0.825199 -0.825199  0.220871  1.000000  1.000000   \n",
       "149981 -0.422913 -0.422913  0.490511  0.084777 -0.595948  1.000000 -0.825199   \n",
       "149982 -0.422913  0.490511  0.490511  0.490511  0.132984 -0.825199 -0.825199   \n",
       "149983  0.490511 -0.825199  1.000000  1.000000 -0.144094 -0.422913  0.490511   \n",
       "149984 -0.912187  1.000000  1.000000  1.000000 -0.144094 -0.912187 -0.912187   \n",
       "149985  0.490511 -0.825199  1.000000  1.000000 -0.144094 -0.422913  0.490511   \n",
       "149986 -0.825199 -0.825199 -0.825199 -0.825199  0.220871  1.000000  1.000000   \n",
       "149987 -0.422913 -0.422913  0.490511 -0.326951 -0.325346  1.000000 -0.825199   \n",
       "149988 -0.912187  1.000000  1.000000  1.000000 -0.144094 -0.912187 -0.912187   \n",
       "149989 -0.422913 -0.422913  0.490511  0.084777 -0.595948  1.000000 -0.825199   \n",
       "149990  0.422776  0.536819  0.536819  0.536819  0.323540  0.490511  0.490511   \n",
       "149991 -0.912187  0.853670 -0.912187 -0.912187  0.060199 -0.825199  1.000000   \n",
       "149992 -0.912187  1.000000  1.000000  1.000000 -0.144094 -0.912187 -0.912187   \n",
       "149993 -0.422913 -0.422913  0.490511 -0.325346  0.084777  1.000000 -0.825199   \n",
       "149994 -0.912187  1.000000  1.000000  1.000000 -0.144094 -0.912187 -0.912187   \n",
       "149995 -0.422913 -0.422913  0.490511 -0.326951 -0.325346  1.000000 -0.825199   \n",
       "149996 -0.825199 -0.825199 -0.825199 -0.825199  0.220871  1.000000  1.000000   \n",
       "149997 -0.570782 -0.450803  0.536819  0.536819  0.323540  0.853670 -0.912187   \n",
       "149998 -0.912187  1.000000  1.000000  1.000000 -0.144094 -0.912187 -0.912187   \n",
       "149999 -0.422913 -0.422913  0.490511 -0.595948  0.490511  1.000000 -0.825199   \n",
       "\n",
       "               7         8         9        10        11        12        13  \\\n",
       "0       1.000000  1.000000  1.000000  1.000000  1.000000  1.000000  1.000000   \n",
       "1       0.490511  0.368714  1.000000  0.490511  0.368714  0.490511  0.368714   \n",
       "2       0.186247  0.186247  1.000000  1.000000  1.000000  1.000000  1.000000   \n",
       "3       0.186247  0.186247  1.000000  1.000000  1.000000  1.000000  1.000000   \n",
       "4       0.186247 -0.111000  1.000000  1.000000  0.490511  1.000000  0.490511   \n",
       "5       0.186247 -0.111000  1.000000  1.000000  0.490511  1.000000  0.490511   \n",
       "6      -0.111000  0.373484  1.000000  0.490511  0.368714  0.490511  0.368714   \n",
       "7      -0.111000  0.373484  1.000000  0.490511  0.368714  0.490511  0.368714   \n",
       "8      -0.111000  0.373484  1.000000  0.490511  0.368714  0.490511  0.368714   \n",
       "9       1.000000  0.490511  1.000000  1.000000  0.490511  1.000000  0.490511   \n",
       "10      1.000000  0.490511  1.000000  1.000000  0.490511  1.000000  0.490511   \n",
       "11      1.000000  0.490511  1.000000  1.000000  0.490511  1.000000  0.490511   \n",
       "12      1.000000  0.490511  1.000000  1.000000  0.490511  1.000000  0.490511   \n",
       "13      1.000000  0.490511  1.000000  1.000000  0.490511  1.000000  0.490511   \n",
       "14      1.000000  0.490511  1.000000  1.000000  0.490511  1.000000  0.490511   \n",
       "15      1.000000  0.490511  1.000000  1.000000  0.490511  1.000000  0.490511   \n",
       "16      1.000000  0.490511  1.000000  1.000000  0.490511  1.000000  0.490511   \n",
       "17      1.000000  0.490511  1.000000  1.000000  0.490511  1.000000  0.490511   \n",
       "18      0.490511  0.360336  1.000000  1.000000  0.468910  1.000000  0.468910   \n",
       "19     -0.523517  0.045109  1.000000  1.000000  0.468910  1.000000  0.468910   \n",
       "20      0.490511  0.360336  1.000000  1.000000  0.468910  1.000000  0.468910   \n",
       "21      0.490511  0.360336  1.000000  1.000000  0.468910  1.000000  0.468910   \n",
       "22      0.536819  0.521830  1.000000  1.000000  0.468910  1.000000  0.468910   \n",
       "23      1.000000  0.468910  1.000000  1.000000  0.468910  1.000000  0.468910   \n",
       "24      1.000000  0.468910  1.000000  1.000000  0.468910  1.000000  0.468910   \n",
       "25      1.000000  0.468910  1.000000  1.000000  0.468910  1.000000  0.468910   \n",
       "26      1.000000  0.468910  1.000000  1.000000  0.468910  1.000000  0.468910   \n",
       "27      1.000000  0.468910  1.000000  1.000000  0.468910  1.000000  0.468910   \n",
       "28      1.000000  1.000000  1.000000  1.000000  1.000000  1.000000  1.000000   \n",
       "29     -0.825199  1.000000 -0.825199 -0.825199  1.000000  1.000000 -0.825199   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "149970 -0.144094  0.368714  1.000000 -0.144094  0.368714 -0.144094  0.368714   \n",
       "149971  0.060199  0.809053 -0.825199  0.220871  0.981231 -0.144094 -0.806904   \n",
       "149972 -0.144094  0.368714  1.000000 -0.144094  0.368714 -0.144094  0.368714   \n",
       "149973  0.323540 -0.423814 -0.825199  0.220871  0.981231 -0.144094 -0.806904   \n",
       "149974 -0.144094  0.368714  1.000000 -0.144094  0.368714 -0.144094  0.368714   \n",
       "149975 -0.144094 -0.806904 -0.825199  0.220871  0.981231 -0.144094 -0.806904   \n",
       "149976 -0.144094  0.368714  1.000000 -0.144094  0.368714 -0.144094  0.368714   \n",
       "149977 -0.900818  0.807903 -0.825199 -0.900818  0.807903  0.608865 -0.899872   \n",
       "149978 -0.912187  0.060199  1.000000  1.000000 -0.144094  1.000000 -0.144094   \n",
       "149979 -0.900818  0.807903 -0.825199 -0.900818  0.807903  0.608865 -0.899872   \n",
       "149980  1.000000 -0.144094  1.000000  1.000000 -0.144094  1.000000 -0.144094   \n",
       "149981 -0.900818  0.807903 -0.825199 -0.900818  0.807903  0.608865 -0.899872   \n",
       "149982 -0.825199  0.220871  1.000000  1.000000 -0.144094  1.000000 -0.144094   \n",
       "149983  0.490511  0.132984 -0.825199 -0.825199  0.220871  1.000000 -0.144094   \n",
       "149984 -0.912187  0.060199  1.000000  1.000000 -0.144094  1.000000 -0.144094   \n",
       "149985  0.490511  0.132984 -0.825199 -0.825199  0.220871  1.000000 -0.144094   \n",
       "149986  1.000000 -0.144094  1.000000  1.000000 -0.144094  1.000000 -0.144094   \n",
       "149987  0.981231  0.978731 -0.825199  0.981231  0.978731 -0.806904 -0.769874   \n",
       "149988 -0.912187  0.060199  1.000000  1.000000 -0.144094  1.000000 -0.144094   \n",
       "149989 -0.900818  0.807903 -0.825199 -0.900818  0.807903  0.608865 -0.899872   \n",
       "149990  0.490511  0.132984  1.000000  1.000000 -0.144094  1.000000 -0.144094   \n",
       "149991  1.000000 -0.144094 -0.825199 -0.825199  0.220871  1.000000 -0.144094   \n",
       "149992 -0.912187  0.060199  1.000000  1.000000 -0.144094  1.000000 -0.144094   \n",
       "149993  0.978731 -0.900818 -0.825199  0.978731 -0.900818 -0.769874  0.608865   \n",
       "149994 -0.912187  0.060199  1.000000  1.000000 -0.144094  1.000000 -0.144094   \n",
       "149995  0.981231  0.978731 -0.825199  0.981231  0.978731 -0.806904 -0.769874   \n",
       "149996  1.000000 -0.144094  1.000000  1.000000 -0.144094  1.000000 -0.144094   \n",
       "149997 -0.912187  0.060199 -0.825199 -0.825199  0.220871  1.000000 -0.144094   \n",
       "149998 -0.912187  0.060199  1.000000  1.000000 -0.144094  1.000000 -0.144094   \n",
       "149999  0.807903 -0.825199 -0.825199  0.807903 -0.825199 -0.899872  1.000000   \n",
       "\n",
       "              14  \n",
       "0       1.000000  \n",
       "1       0.219010  \n",
       "2       1.000000  \n",
       "3       1.000000  \n",
       "4       0.490511  \n",
       "5       0.490511  \n",
       "6       0.219010  \n",
       "7       0.219010  \n",
       "8       0.219010  \n",
       "9       0.490511  \n",
       "10      0.490511  \n",
       "11      0.490511  \n",
       "12      0.490511  \n",
       "13      0.490511  \n",
       "14      0.490511  \n",
       "15      0.490511  \n",
       "16      0.490511  \n",
       "17      0.490511  \n",
       "18      0.468910  \n",
       "19      0.468910  \n",
       "20      0.468910  \n",
       "21      0.468910  \n",
       "22      0.468910  \n",
       "23      0.468910  \n",
       "24      0.468910  \n",
       "25      0.468910  \n",
       "26      0.468910  \n",
       "27      0.468910  \n",
       "28      1.000000  \n",
       "29     -0.825199  \n",
       "...          ...  \n",
       "149970  0.039240  \n",
       "149971  0.246684  \n",
       "149972  0.039240  \n",
       "149973  0.246684  \n",
       "149974  0.039240  \n",
       "149975  0.246684  \n",
       "149976  0.039240  \n",
       "149977 -0.600251  \n",
       "149978 -0.144094  \n",
       "149979 -0.600251  \n",
       "149980 -0.144094  \n",
       "149981 -0.600251  \n",
       "149982 -0.144094  \n",
       "149983 -0.144094  \n",
       "149984 -0.144094  \n",
       "149985 -0.144094  \n",
       "149986 -0.144094  \n",
       "149987  0.980221  \n",
       "149988 -0.144094  \n",
       "149989 -0.600251  \n",
       "149990 -0.144094  \n",
       "149991 -0.144094  \n",
       "149992 -0.144094  \n",
       "149993 -0.928700  \n",
       "149994 -0.144094  \n",
       "149995  0.980221  \n",
       "149996 -0.144094  \n",
       "149997 -0.144094  \n",
       "149998 -0.144094  \n",
       "149999 -0.899872  \n",
       "\n",
       "[150000 rows x 15 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_type_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_Y = [([int]) for i in range(train_num)]\n",
    "for i in range(train_num):\n",
    "    train_Y[i] = train_labels[i][2]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#神经网络模型\n",
    "\n",
    "\n",
    "#设置参数  \n",
    "nn_input_dim=8 #输入神经元个数  \n",
    "nn_output_dim=2 #输出神经元个数  \n",
    "nn_hdim=50  #隐层节点数\n",
    "epsilon=0.2 #learning rate  \n",
    "reg_lambda=0.01 #正则化长度  \n",
    "\n",
    "#参数初始化，设置为shared加速计算\n",
    "w1=theano.shared(np.random.randn(nn_input_dim,nn_hdim),name=\"W1\")  \n",
    "b1=theano.shared(np.zeros(nn_hdim),name=\"b1\")  \n",
    "w2=theano.shared(np.random.randn(nn_hdim,nn_output_dim),name=\"W2\")  \n",
    "b2=theano.shared(np.zeros(nn_output_dim),name=\"b2\")  \n",
    "\n",
    "#前馈算法  \n",
    "X=T.matrix('X')  #double类型的矩阵 \n",
    "y=T.lvector('y') #int64类型的向量  \n",
    "z1=X.dot(w1)+b1   #1 输入和w1的加权和\n",
    "a1=T.tanh(z1)     #2 激活函数\n",
    "z2=a1.dot(w2)+b2  #3 隐层输出和w2的加权和  \n",
    "y_hat=T.nnet.softmax(z2) #4 激活函数  \n",
    "loss_reg=1./train_num * reg_lambda/2 * (T.sum(T.square(w1))+T.sum(T.square(w2))) #5 正则化项    \n",
    "loss=T.nnet.categorical_crossentropy(y_hat,y).mean() + loss_reg  #6 损失函数    \n",
    "prediction=T.argmax(y_hat,axis=1) #7 预测结果  \n",
    "\n",
    "forword_prop=theano.function([X],y_hat)  \n",
    "calculate_loss=theano.function([X,y],loss)  \n",
    "predict=theano.function([X],prediction)  \n",
    "\n",
    "#求导  \n",
    "dw2=T.grad(loss,w2)  \n",
    "db2=T.grad(loss,b2)  \n",
    "dw1=T.grad(loss,w1)  \n",
    "db1=T.grad(loss,b1)  \n",
    "#更新值  \n",
    "gradient_step=theano.function(  \n",
    "    [X,y],  \n",
    "    updates=(  \n",
    "        (w2,w2-epsilon*dw2),  \n",
    "        (b2,b2-epsilon*db2),  \n",
    "        (w1,w1-epsilon*dw1),  \n",
    "        (b1,b1-epsilon*db1)  \n",
    "  \n",
    "    )  \n",
    ")  \n",
    "\n",
    "def build_model(num_passes=5000,print_loss=False):  \n",
    "    w1.set_value(np.random.randn(nn_input_dim, nn_hdim) / np.sqrt(nn_input_dim))  \n",
    "    b1.set_value(np.zeros(nn_hdim))  \n",
    "    w2.set_value(np.random.randn(nn_hdim, nn_output_dim) / np.sqrt(nn_hdim))  \n",
    "    b2.set_value(np.zeros(nn_output_dim))  \n",
    "    for i in range(0,num_passes):  \n",
    "        gradient_step(train_X,train_Y)   #TODO change here\n",
    "        if print_loss and i%1==0:  \n",
    "            print(\"Loss after iteration %i: %f\"%(i,calculate_loss(train_X,train_Y)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n",
      "50000\n",
      "51000\n",
      "52000\n",
      "53000\n",
      "54000\n",
      "55000\n",
      "56000\n",
      "57000\n",
      "58000\n",
      "59000\n",
      "60000\n",
      "61000\n",
      "62000\n",
      "63000\n",
      "64000\n",
      "65000\n",
      "66000\n",
      "67000\n",
      "68000\n",
      "69000\n",
      "70000\n",
      "71000\n",
      "72000\n",
      "73000\n",
      "74000\n",
      "75000\n",
      "76000\n",
      "77000\n",
      "78000\n",
      "79000\n",
      "80000\n",
      "81000\n",
      "82000\n",
      "83000\n",
      "84000\n",
      "85000\n",
      "86000\n",
      "87000\n",
      "88000\n",
      "89000\n",
      "90000\n",
      "91000\n",
      "92000\n",
      "93000\n",
      "94000\n",
      "95000\n",
      "96000\n",
      "97000\n",
      "98000\n",
      "99000\n",
      "100000\n",
      "101000\n",
      "102000\n",
      "103000\n",
      "104000\n",
      "105000\n",
      "106000\n",
      "107000\n",
      "108000\n",
      "109000\n",
      "110000\n",
      "111000\n",
      "112000\n",
      "113000\n",
      "114000\n",
      "115000\n",
      "116000\n",
      "117000\n",
      "118000\n",
      "119000\n",
      "120000\n",
      "121000\n",
      "122000\n",
      "123000\n",
      "124000\n",
      "125000\n",
      "126000\n",
      "127000\n",
      "128000\n",
      "129000\n",
      "130000\n",
      "131000\n",
      "132000\n",
      "133000\n",
      "134000\n",
      "135000\n",
      "136000\n",
      "137000\n",
      "138000\n",
      "139000\n",
      "140000\n",
      "141000\n",
      "142000\n",
      "143000\n",
      "144000\n",
      "145000\n",
      "146000\n",
      "147000\n",
      "148000\n",
      "149000\n"
     ]
    }
   ],
   "source": [
    "# 1 2 3 4 5 6\n",
    "#指标1：2 and 4\n",
    "#指标2：3 and 5\n",
    "#指标3：1-2 and 1-4\n",
    "#指标4：3-6 and 5-6\n",
    "\n",
    "#word和type总共8个指标\n",
    "\n",
    "#train_word_df.ix[:,0]-train_word_df.ix[:,0]\n",
    "train_X = np.ndarray(shape=(train_num,8))\n",
    "for i in range(train_num):\n",
    "    train_X[i][0] = train_word_df.ix[i][6]\n",
    "    train_X[i][1] = train_word_df.ix[i][6]\n",
    "    train_X[i][2] = train_word_df.ix[i][0] - train_word_df.ix[i][2]\n",
    "    train_X[i][3] = train_word_df.ix[i][11] - train_word_df.ix[i][14]\n",
    "    train_X[i][4] = train_type_df.ix[i][6]\n",
    "    train_X[i][5] = train_type_df.ix[i][6]\n",
    "    train_X[i][6] = train_type_df.ix[i][0] - train_type_df.ix[i][2]\n",
    "    train_X[i][7] = train_type_df.ix[i][11] - train_type_df.ix[i][14]\n",
    "    if i %1000 ==0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#replace nan with 0\n",
    "for i in range(train_num):\n",
    "    for j in range(8):\n",
    "        if np.isnan(train_X[i][j]):\n",
    "            train_X[i][j] = 0\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.08244176,  0.08244176, -0.01505103, ...,  1.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.08244176,  0.08244176, -0.01505103, ...,  1.        ,\n",
       "         0.        ,  0.14970347],\n",
       "       [ 0.08187206,  0.08187206,  0.04587453, ...,  0.18624691,\n",
       "        -0.60151098,  0.        ],\n",
       "       ..., \n",
       "       [-0.10657906, -0.10657906,  0.37886877, ..., -0.91218657,\n",
       "        -1.10760129,  0.3649651 ],\n",
       "       [-0.14468427, -0.14468427, -0.32197362, ..., -0.91218657,\n",
       "        -1.91218657,  0.        ],\n",
       "       [ 0.03928163,  0.03928163, -0.14240259, ..., -0.82519865,\n",
       "        -0.91342413,  0.07467359]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 0: 0.699326\n",
      "Loss after iteration 1: 0.679547\n",
      "Loss after iteration 2: 0.666285\n",
      "Loss after iteration 3: 0.656753\n",
      "Loss after iteration 4: 0.649816\n",
      "Loss after iteration 5: 0.644688\n",
      "Loss after iteration 6: 0.640828\n",
      "Loss after iteration 7: 0.637865\n",
      "Loss after iteration 8: 0.635545\n",
      "Loss after iteration 9: 0.633693\n",
      "Loss after iteration 10: 0.632187\n",
      "Loss after iteration 11: 0.630944\n",
      "Loss after iteration 12: 0.629900\n",
      "Loss after iteration 13: 0.629013\n",
      "Loss after iteration 14: 0.628250\n",
      "Loss after iteration 15: 0.627585\n",
      "Loss after iteration 16: 0.627001\n",
      "Loss after iteration 17: 0.626484\n",
      "Loss after iteration 18: 0.626022\n",
      "Loss after iteration 19: 0.625607\n",
      "Loss after iteration 20: 0.625231\n",
      "Loss after iteration 21: 0.624889\n",
      "Loss after iteration 22: 0.624576\n",
      "Loss after iteration 23: 0.624287\n",
      "Loss after iteration 24: 0.624021\n",
      "Loss after iteration 25: 0.623773\n",
      "Loss after iteration 26: 0.623542\n",
      "Loss after iteration 27: 0.623325\n",
      "Loss after iteration 28: 0.623122\n",
      "Loss after iteration 29: 0.622929\n",
      "Loss after iteration 30: 0.622746\n",
      "Loss after iteration 31: 0.622573\n",
      "Loss after iteration 32: 0.622407\n",
      "Loss after iteration 33: 0.622248\n",
      "Loss after iteration 34: 0.622095\n",
      "Loss after iteration 35: 0.621948\n",
      "Loss after iteration 36: 0.621805\n",
      "Loss after iteration 37: 0.621667\n",
      "Loss after iteration 38: 0.621533\n",
      "Loss after iteration 39: 0.621402\n",
      "Loss after iteration 40: 0.621275\n",
      "Loss after iteration 41: 0.621150\n",
      "Loss after iteration 42: 0.621028\n",
      "Loss after iteration 43: 0.620907\n",
      "Loss after iteration 44: 0.620789\n",
      "Loss after iteration 45: 0.620673\n",
      "Loss after iteration 46: 0.620558\n",
      "Loss after iteration 47: 0.620445\n",
      "Loss after iteration 48: 0.620333\n",
      "Loss after iteration 49: 0.620222\n",
      "Loss after iteration 50: 0.620112\n",
      "Loss after iteration 51: 0.620004\n",
      "Loss after iteration 52: 0.619896\n",
      "Loss after iteration 53: 0.619788\n",
      "Loss after iteration 54: 0.619682\n",
      "Loss after iteration 55: 0.619576\n",
      "Loss after iteration 56: 0.619470\n",
      "Loss after iteration 57: 0.619365\n",
      "Loss after iteration 58: 0.619260\n",
      "Loss after iteration 59: 0.619156\n",
      "Loss after iteration 60: 0.619052\n",
      "Loss after iteration 61: 0.618949\n",
      "Loss after iteration 62: 0.618845\n",
      "Loss after iteration 63: 0.618742\n",
      "Loss after iteration 64: 0.618639\n",
      "Loss after iteration 65: 0.618536\n",
      "Loss after iteration 66: 0.618434\n",
      "Loss after iteration 67: 0.618331\n",
      "Loss after iteration 68: 0.618229\n",
      "Loss after iteration 69: 0.618127\n",
      "Loss after iteration 70: 0.618024\n",
      "Loss after iteration 71: 0.617922\n",
      "Loss after iteration 72: 0.617820\n",
      "Loss after iteration 73: 0.617718\n",
      "Loss after iteration 74: 0.617616\n",
      "Loss after iteration 75: 0.617514\n",
      "Loss after iteration 76: 0.617412\n",
      "Loss after iteration 77: 0.617310\n",
      "Loss after iteration 78: 0.617208\n",
      "Loss after iteration 79: 0.617106\n",
      "Loss after iteration 80: 0.617004\n",
      "Loss after iteration 81: 0.616902\n",
      "Loss after iteration 82: 0.616799\n",
      "Loss after iteration 83: 0.616697\n",
      "Loss after iteration 84: 0.616595\n",
      "Loss after iteration 85: 0.616492\n",
      "Loss after iteration 86: 0.616390\n",
      "Loss after iteration 87: 0.616287\n",
      "Loss after iteration 88: 0.616185\n",
      "Loss after iteration 89: 0.616082\n",
      "Loss after iteration 90: 0.615979\n",
      "Loss after iteration 91: 0.615877\n",
      "Loss after iteration 92: 0.615774\n",
      "Loss after iteration 93: 0.615671\n",
      "Loss after iteration 94: 0.615568\n",
      "Loss after iteration 95: 0.615464\n",
      "Loss after iteration 96: 0.615361\n",
      "Loss after iteration 97: 0.615258\n",
      "Loss after iteration 98: 0.615154\n",
      "Loss after iteration 99: 0.615051\n",
      "Loss after iteration 100: 0.614947\n",
      "Loss after iteration 101: 0.614843\n",
      "Loss after iteration 102: 0.614739\n",
      "Loss after iteration 103: 0.614635\n",
      "Loss after iteration 104: 0.614531\n",
      "Loss after iteration 105: 0.614427\n",
      "Loss after iteration 106: 0.614322\n",
      "Loss after iteration 107: 0.614218\n",
      "Loss after iteration 108: 0.614113\n",
      "Loss after iteration 109: 0.614009\n",
      "Loss after iteration 110: 0.613904\n",
      "Loss after iteration 111: 0.613799\n",
      "Loss after iteration 112: 0.613694\n",
      "Loss after iteration 113: 0.613589\n",
      "Loss after iteration 114: 0.613483\n",
      "Loss after iteration 115: 0.613378\n",
      "Loss after iteration 116: 0.613273\n",
      "Loss after iteration 117: 0.613167\n",
      "Loss after iteration 118: 0.613061\n",
      "Loss after iteration 119: 0.612955\n",
      "Loss after iteration 120: 0.612849\n",
      "Loss after iteration 121: 0.612743\n",
      "Loss after iteration 122: 0.612637\n",
      "Loss after iteration 123: 0.612530\n",
      "Loss after iteration 124: 0.612424\n",
      "Loss after iteration 125: 0.612317\n",
      "Loss after iteration 126: 0.612211\n",
      "Loss after iteration 127: 0.612104\n",
      "Loss after iteration 128: 0.611997\n",
      "Loss after iteration 129: 0.611890\n",
      "Loss after iteration 130: 0.611782\n",
      "Loss after iteration 131: 0.611675\n",
      "Loss after iteration 132: 0.611568\n",
      "Loss after iteration 133: 0.611460\n",
      "Loss after iteration 134: 0.611352\n",
      "Loss after iteration 135: 0.611244\n",
      "Loss after iteration 136: 0.611136\n",
      "Loss after iteration 137: 0.611028\n",
      "Loss after iteration 138: 0.610920\n",
      "Loss after iteration 139: 0.610812\n",
      "Loss after iteration 140: 0.610703\n",
      "Loss after iteration 141: 0.610595\n",
      "Loss after iteration 142: 0.610486\n",
      "Loss after iteration 143: 0.610377\n",
      "Loss after iteration 144: 0.610268\n",
      "Loss after iteration 145: 0.610159\n",
      "Loss after iteration 146: 0.610050\n",
      "Loss after iteration 147: 0.609941\n",
      "Loss after iteration 148: 0.609832\n",
      "Loss after iteration 149: 0.609722\n",
      "Loss after iteration 150: 0.609613\n",
      "Loss after iteration 151: 0.609503\n",
      "Loss after iteration 152: 0.609393\n",
      "Loss after iteration 153: 0.609283\n",
      "Loss after iteration 154: 0.609173\n",
      "Loss after iteration 155: 0.609063\n",
      "Loss after iteration 156: 0.608953\n",
      "Loss after iteration 157: 0.608842\n",
      "Loss after iteration 158: 0.608732\n",
      "Loss after iteration 159: 0.608621\n",
      "Loss after iteration 160: 0.608511\n",
      "Loss after iteration 161: 0.608400\n",
      "Loss after iteration 162: 0.608289\n",
      "Loss after iteration 163: 0.608178\n",
      "Loss after iteration 164: 0.608067\n",
      "Loss after iteration 165: 0.607956\n",
      "Loss after iteration 166: 0.607845\n",
      "Loss after iteration 167: 0.607733\n",
      "Loss after iteration 168: 0.607622\n",
      "Loss after iteration 169: 0.607511\n",
      "Loss after iteration 170: 0.607399\n",
      "Loss after iteration 171: 0.607287\n",
      "Loss after iteration 172: 0.607176\n",
      "Loss after iteration 173: 0.607064\n",
      "Loss after iteration 174: 0.606952\n",
      "Loss after iteration 175: 0.606840\n",
      "Loss after iteration 176: 0.606728\n",
      "Loss after iteration 177: 0.606616\n",
      "Loss after iteration 178: 0.606503\n",
      "Loss after iteration 179: 0.606391\n",
      "Loss after iteration 180: 0.606279\n",
      "Loss after iteration 181: 0.606166\n",
      "Loss after iteration 182: 0.606054\n",
      "Loss after iteration 183: 0.605941\n",
      "Loss after iteration 184: 0.605829\n",
      "Loss after iteration 185: 0.605716\n",
      "Loss after iteration 186: 0.605603\n",
      "Loss after iteration 187: 0.605490\n",
      "Loss after iteration 188: 0.605378\n",
      "Loss after iteration 189: 0.605265\n",
      "Loss after iteration 190: 0.605152\n",
      "Loss after iteration 191: 0.605039\n",
      "Loss after iteration 192: 0.604926\n",
      "Loss after iteration 193: 0.604813\n",
      "Loss after iteration 194: 0.604699\n",
      "Loss after iteration 195: 0.604586\n",
      "Loss after iteration 196: 0.604473\n",
      "Loss after iteration 197: 0.604360\n",
      "Loss after iteration 198: 0.604246\n",
      "Loss after iteration 199: 0.604133\n",
      "Loss after iteration 200: 0.604020\n",
      "Loss after iteration 201: 0.603906\n",
      "Loss after iteration 202: 0.603793\n",
      "Loss after iteration 203: 0.603679\n",
      "Loss after iteration 204: 0.603566\n",
      "Loss after iteration 205: 0.603452\n",
      "Loss after iteration 206: 0.603339\n",
      "Loss after iteration 207: 0.603225\n",
      "Loss after iteration 208: 0.603112\n",
      "Loss after iteration 209: 0.602998\n",
      "Loss after iteration 210: 0.602884\n",
      "Loss after iteration 211: 0.602771\n",
      "Loss after iteration 212: 0.602657\n",
      "Loss after iteration 213: 0.602544\n",
      "Loss after iteration 214: 0.602430\n",
      "Loss after iteration 215: 0.602316\n",
      "Loss after iteration 216: 0.602203\n",
      "Loss after iteration 217: 0.602089\n",
      "Loss after iteration 218: 0.601976\n",
      "Loss after iteration 219: 0.601862\n",
      "Loss after iteration 220: 0.601748\n",
      "Loss after iteration 221: 0.601635\n",
      "Loss after iteration 222: 0.601521\n",
      "Loss after iteration 223: 0.601408\n",
      "Loss after iteration 224: 0.601294\n",
      "Loss after iteration 225: 0.601181\n",
      "Loss after iteration 226: 0.601067\n",
      "Loss after iteration 227: 0.600954\n",
      "Loss after iteration 228: 0.600840\n",
      "Loss after iteration 229: 0.600727\n",
      "Loss after iteration 230: 0.600614\n",
      "Loss after iteration 231: 0.600500\n",
      "Loss after iteration 232: 0.600387\n",
      "Loss after iteration 233: 0.600274\n",
      "Loss after iteration 234: 0.600160\n",
      "Loss after iteration 235: 0.600047\n",
      "Loss after iteration 236: 0.599934\n",
      "Loss after iteration 237: 0.599821\n",
      "Loss after iteration 238: 0.599708\n",
      "Loss after iteration 239: 0.599595\n",
      "Loss after iteration 240: 0.599482\n",
      "Loss after iteration 241: 0.599369\n",
      "Loss after iteration 242: 0.599257\n",
      "Loss after iteration 243: 0.599144\n",
      "Loss after iteration 244: 0.599031\n",
      "Loss after iteration 245: 0.598918\n",
      "Loss after iteration 246: 0.598806\n",
      "Loss after iteration 247: 0.598693\n",
      "Loss after iteration 248: 0.598581\n",
      "Loss after iteration 249: 0.598469\n",
      "Loss after iteration 250: 0.598356\n",
      "Loss after iteration 251: 0.598244\n",
      "Loss after iteration 252: 0.598132\n",
      "Loss after iteration 253: 0.598020\n",
      "Loss after iteration 254: 0.597908\n",
      "Loss after iteration 255: 0.597796\n",
      "Loss after iteration 256: 0.597684\n",
      "Loss after iteration 257: 0.597573\n",
      "Loss after iteration 258: 0.597461\n",
      "Loss after iteration 259: 0.597349\n",
      "Loss after iteration 260: 0.597238\n",
      "Loss after iteration 261: 0.597127\n",
      "Loss after iteration 262: 0.597015\n",
      "Loss after iteration 263: 0.596904\n",
      "Loss after iteration 264: 0.596793\n",
      "Loss after iteration 265: 0.596682\n",
      "Loss after iteration 266: 0.596571\n",
      "Loss after iteration 267: 0.596461\n",
      "Loss after iteration 268: 0.596350\n",
      "Loss after iteration 269: 0.596240\n",
      "Loss after iteration 270: 0.596129\n",
      "Loss after iteration 271: 0.596019\n",
      "Loss after iteration 272: 0.595909\n",
      "Loss after iteration 273: 0.595799\n",
      "Loss after iteration 274: 0.595689\n",
      "Loss after iteration 275: 0.595579\n",
      "Loss after iteration 276: 0.595469\n",
      "Loss after iteration 277: 0.595359\n",
      "Loss after iteration 278: 0.595250\n",
      "Loss after iteration 279: 0.595141\n",
      "Loss after iteration 280: 0.595031\n",
      "Loss after iteration 281: 0.594922\n",
      "Loss after iteration 282: 0.594813\n",
      "Loss after iteration 283: 0.594704\n",
      "Loss after iteration 284: 0.594596\n",
      "Loss after iteration 285: 0.594487\n",
      "Loss after iteration 286: 0.594379\n",
      "Loss after iteration 287: 0.594270\n",
      "Loss after iteration 288: 0.594162\n",
      "Loss after iteration 289: 0.594054\n",
      "Loss after iteration 290: 0.593946\n",
      "Loss after iteration 291: 0.593838\n",
      "Loss after iteration 292: 0.593731\n",
      "Loss after iteration 293: 0.593623\n",
      "Loss after iteration 294: 0.593516\n",
      "Loss after iteration 295: 0.593409\n",
      "Loss after iteration 296: 0.593302\n",
      "Loss after iteration 297: 0.593195\n",
      "Loss after iteration 298: 0.593088\n",
      "Loss after iteration 299: 0.592981\n",
      "Loss after iteration 300: 0.592875\n",
      "Loss after iteration 301: 0.592769\n",
      "Loss after iteration 302: 0.592662\n",
      "Loss after iteration 303: 0.592556\n",
      "Loss after iteration 304: 0.592451\n",
      "Loss after iteration 305: 0.592345\n",
      "Loss after iteration 306: 0.592239\n",
      "Loss after iteration 307: 0.592134\n",
      "Loss after iteration 308: 0.592029\n",
      "Loss after iteration 309: 0.591924\n",
      "Loss after iteration 310: 0.591819\n",
      "Loss after iteration 311: 0.591714\n",
      "Loss after iteration 312: 0.591609\n",
      "Loss after iteration 313: 0.591505\n",
      "Loss after iteration 314: 0.591401\n",
      "Loss after iteration 315: 0.591296\n",
      "Loss after iteration 316: 0.591192\n",
      "Loss after iteration 317: 0.591089\n",
      "Loss after iteration 318: 0.590985\n",
      "Loss after iteration 319: 0.590881\n",
      "Loss after iteration 320: 0.590778\n",
      "Loss after iteration 321: 0.590675\n",
      "Loss after iteration 322: 0.590572\n",
      "Loss after iteration 323: 0.590469\n",
      "Loss after iteration 324: 0.590366\n",
      "Loss after iteration 325: 0.590264\n",
      "Loss after iteration 326: 0.590162\n",
      "Loss after iteration 327: 0.590059\n",
      "Loss after iteration 328: 0.589957\n",
      "Loss after iteration 329: 0.589856\n",
      "Loss after iteration 330: 0.589754\n",
      "Loss after iteration 331: 0.589652\n",
      "Loss after iteration 332: 0.589551\n",
      "Loss after iteration 333: 0.589450\n",
      "Loss after iteration 334: 0.589349\n",
      "Loss after iteration 335: 0.589248\n",
      "Loss after iteration 336: 0.589148\n",
      "Loss after iteration 337: 0.589047\n",
      "Loss after iteration 338: 0.588947\n",
      "Loss after iteration 339: 0.588847\n",
      "Loss after iteration 340: 0.588747\n",
      "Loss after iteration 341: 0.588647\n",
      "Loss after iteration 342: 0.588547\n",
      "Loss after iteration 343: 0.588448\n",
      "Loss after iteration 344: 0.588348\n",
      "Loss after iteration 345: 0.588249\n",
      "Loss after iteration 346: 0.588150\n",
      "Loss after iteration 347: 0.588051\n",
      "Loss after iteration 348: 0.587953\n",
      "Loss after iteration 349: 0.587854\n",
      "Loss after iteration 350: 0.587756\n",
      "Loss after iteration 351: 0.587658\n",
      "Loss after iteration 352: 0.587560\n",
      "Loss after iteration 353: 0.587462\n",
      "Loss after iteration 354: 0.587365\n",
      "Loss after iteration 355: 0.587267\n",
      "Loss after iteration 356: 0.587170\n",
      "Loss after iteration 357: 0.587073\n",
      "Loss after iteration 358: 0.586976\n",
      "Loss after iteration 359: 0.586879\n",
      "Loss after iteration 360: 0.586783\n",
      "Loss after iteration 361: 0.586686\n",
      "Loss after iteration 362: 0.586590\n",
      "Loss after iteration 363: 0.586494\n",
      "Loss after iteration 364: 0.586398\n",
      "Loss after iteration 365: 0.586302\n",
      "Loss after iteration 366: 0.586207\n",
      "Loss after iteration 367: 0.586111\n",
      "Loss after iteration 368: 0.586016\n",
      "Loss after iteration 369: 0.585921\n",
      "Loss after iteration 370: 0.585826\n",
      "Loss after iteration 371: 0.585731\n",
      "Loss after iteration 372: 0.585637\n",
      "Loss after iteration 373: 0.585543\n",
      "Loss after iteration 374: 0.585448\n",
      "Loss after iteration 375: 0.585354\n",
      "Loss after iteration 376: 0.585260\n",
      "Loss after iteration 377: 0.585167\n",
      "Loss after iteration 378: 0.585073\n",
      "Loss after iteration 379: 0.584980\n",
      "Loss after iteration 380: 0.584886\n",
      "Loss after iteration 381: 0.584793\n",
      "Loss after iteration 382: 0.584700\n",
      "Loss after iteration 383: 0.584608\n",
      "Loss after iteration 384: 0.584515\n",
      "Loss after iteration 385: 0.584423\n",
      "Loss after iteration 386: 0.584330\n",
      "Loss after iteration 387: 0.584238\n",
      "Loss after iteration 388: 0.584146\n",
      "Loss after iteration 389: 0.584054\n",
      "Loss after iteration 390: 0.583963\n",
      "Loss after iteration 391: 0.583871\n",
      "Loss after iteration 392: 0.583780\n",
      "Loss after iteration 393: 0.583689\n",
      "Loss after iteration 394: 0.583598\n",
      "Loss after iteration 395: 0.583507\n",
      "Loss after iteration 396: 0.583416\n",
      "Loss after iteration 397: 0.583326\n",
      "Loss after iteration 398: 0.583235\n",
      "Loss after iteration 399: 0.583145\n",
      "Loss after iteration 400: 0.583055\n",
      "Loss after iteration 401: 0.582965\n",
      "Loss after iteration 402: 0.582876\n",
      "Loss after iteration 403: 0.582786\n",
      "Loss after iteration 404: 0.582697\n",
      "Loss after iteration 405: 0.582607\n",
      "Loss after iteration 406: 0.582518\n",
      "Loss after iteration 407: 0.582429\n",
      "Loss after iteration 408: 0.582340\n",
      "Loss after iteration 409: 0.582252\n",
      "Loss after iteration 410: 0.582163\n",
      "Loss after iteration 411: 0.582075\n",
      "Loss after iteration 412: 0.581986\n",
      "Loss after iteration 413: 0.581898\n",
      "Loss after iteration 414: 0.581810\n",
      "Loss after iteration 415: 0.581723\n",
      "Loss after iteration 416: 0.581635\n",
      "Loss after iteration 417: 0.581547\n",
      "Loss after iteration 418: 0.581460\n",
      "Loss after iteration 419: 0.581373\n",
      "Loss after iteration 420: 0.581286\n",
      "Loss after iteration 421: 0.581199\n",
      "Loss after iteration 422: 0.581112\n",
      "Loss after iteration 423: 0.581025\n",
      "Loss after iteration 424: 0.580939\n",
      "Loss after iteration 425: 0.580853\n",
      "Loss after iteration 426: 0.580766\n",
      "Loss after iteration 427: 0.580680\n",
      "Loss after iteration 428: 0.580594\n",
      "Loss after iteration 429: 0.580509\n",
      "Loss after iteration 430: 0.580423\n",
      "Loss after iteration 431: 0.580337\n",
      "Loss after iteration 432: 0.580252\n",
      "Loss after iteration 433: 0.580167\n",
      "Loss after iteration 434: 0.580082\n",
      "Loss after iteration 435: 0.579997\n",
      "Loss after iteration 436: 0.579912\n",
      "Loss after iteration 437: 0.579827\n",
      "Loss after iteration 438: 0.579742\n",
      "Loss after iteration 439: 0.579658\n",
      "Loss after iteration 440: 0.579574\n",
      "Loss after iteration 441: 0.579489\n",
      "Loss after iteration 442: 0.579405\n",
      "Loss after iteration 443: 0.579321\n",
      "Loss after iteration 444: 0.579238\n",
      "Loss after iteration 445: 0.579154\n",
      "Loss after iteration 446: 0.579070\n",
      "Loss after iteration 447: 0.578987\n",
      "Loss after iteration 448: 0.578904\n",
      "Loss after iteration 449: 0.578821\n",
      "Loss after iteration 450: 0.578738\n",
      "Loss after iteration 451: 0.578655\n",
      "Loss after iteration 452: 0.578572\n",
      "Loss after iteration 453: 0.578489\n",
      "Loss after iteration 454: 0.578407\n",
      "Loss after iteration 455: 0.578324\n",
      "Loss after iteration 456: 0.578242\n",
      "Loss after iteration 457: 0.578160\n",
      "Loss after iteration 458: 0.578078\n",
      "Loss after iteration 459: 0.577996\n",
      "Loss after iteration 460: 0.577914\n",
      "Loss after iteration 461: 0.577832\n",
      "Loss after iteration 462: 0.577751\n",
      "Loss after iteration 463: 0.577669\n",
      "Loss after iteration 464: 0.577588\n",
      "Loss after iteration 465: 0.577507\n",
      "Loss after iteration 466: 0.577425\n",
      "Loss after iteration 467: 0.577344\n",
      "Loss after iteration 468: 0.577263\n",
      "Loss after iteration 469: 0.577183\n",
      "Loss after iteration 470: 0.577102\n",
      "Loss after iteration 471: 0.577021\n",
      "Loss after iteration 472: 0.576941\n",
      "Loss after iteration 473: 0.576861\n",
      "Loss after iteration 474: 0.576780\n",
      "Loss after iteration 475: 0.576700\n",
      "Loss after iteration 476: 0.576620\n",
      "Loss after iteration 477: 0.576540\n",
      "Loss after iteration 478: 0.576461\n",
      "Loss after iteration 479: 0.576381\n",
      "Loss after iteration 480: 0.576301\n",
      "Loss after iteration 481: 0.576222\n",
      "Loss after iteration 482: 0.576142\n",
      "Loss after iteration 483: 0.576063\n",
      "Loss after iteration 484: 0.575984\n",
      "Loss after iteration 485: 0.575905\n",
      "Loss after iteration 486: 0.575826\n",
      "Loss after iteration 487: 0.575747\n",
      "Loss after iteration 488: 0.575668\n",
      "Loss after iteration 489: 0.575589\n",
      "Loss after iteration 490: 0.575511\n",
      "Loss after iteration 491: 0.575432\n",
      "Loss after iteration 492: 0.575354\n",
      "Loss after iteration 493: 0.575276\n",
      "Loss after iteration 494: 0.575197\n",
      "Loss after iteration 495: 0.575119\n",
      "Loss after iteration 496: 0.575041\n",
      "Loss after iteration 497: 0.574963\n",
      "Loss after iteration 498: 0.574886\n",
      "Loss after iteration 499: 0.574808\n",
      "Loss after iteration 500: 0.574730\n",
      "Loss after iteration 501: 0.574653\n",
      "Loss after iteration 502: 0.574575\n",
      "Loss after iteration 503: 0.574498\n",
      "Loss after iteration 504: 0.574420\n",
      "Loss after iteration 505: 0.574343\n",
      "Loss after iteration 506: 0.574266\n",
      "Loss after iteration 507: 0.574189\n",
      "Loss after iteration 508: 0.574112\n",
      "Loss after iteration 509: 0.574035\n",
      "Loss after iteration 510: 0.573958\n",
      "Loss after iteration 511: 0.573882\n",
      "Loss after iteration 512: 0.573805\n",
      "Loss after iteration 513: 0.573729\n",
      "Loss after iteration 514: 0.573652\n",
      "Loss after iteration 515: 0.573576\n",
      "Loss after iteration 516: 0.573499\n",
      "Loss after iteration 517: 0.573423\n",
      "Loss after iteration 518: 0.573347\n",
      "Loss after iteration 519: 0.573271\n",
      "Loss after iteration 520: 0.573195\n",
      "Loss after iteration 521: 0.573119\n",
      "Loss after iteration 522: 0.573043\n",
      "Loss after iteration 523: 0.572967\n",
      "Loss after iteration 524: 0.572892\n",
      "Loss after iteration 525: 0.572816\n",
      "Loss after iteration 526: 0.572741\n",
      "Loss after iteration 527: 0.572665\n",
      "Loss after iteration 528: 0.572590\n",
      "Loss after iteration 529: 0.572514\n",
      "Loss after iteration 530: 0.572439\n",
      "Loss after iteration 531: 0.572364\n",
      "Loss after iteration 532: 0.572289\n",
      "Loss after iteration 533: 0.572214\n",
      "Loss after iteration 534: 0.572139\n",
      "Loss after iteration 535: 0.572064\n",
      "Loss after iteration 536: 0.571989\n",
      "Loss after iteration 537: 0.571914\n",
      "Loss after iteration 538: 0.571840\n",
      "Loss after iteration 539: 0.571765\n",
      "Loss after iteration 540: 0.571690\n",
      "Loss after iteration 541: 0.571616\n",
      "Loss after iteration 542: 0.571541\n",
      "Loss after iteration 543: 0.571467\n",
      "Loss after iteration 544: 0.571393\n",
      "Loss after iteration 545: 0.571318\n",
      "Loss after iteration 546: 0.571244\n",
      "Loss after iteration 547: 0.571170\n",
      "Loss after iteration 548: 0.571096\n",
      "Loss after iteration 549: 0.571022\n",
      "Loss after iteration 550: 0.570948\n",
      "Loss after iteration 551: 0.570874\n",
      "Loss after iteration 552: 0.570800\n",
      "Loss after iteration 553: 0.570726\n",
      "Loss after iteration 554: 0.570652\n",
      "Loss after iteration 555: 0.570579\n",
      "Loss after iteration 556: 0.570505\n",
      "Loss after iteration 557: 0.570431\n",
      "Loss after iteration 558: 0.570358\n",
      "Loss after iteration 559: 0.570284\n",
      "Loss after iteration 560: 0.570211\n",
      "Loss after iteration 561: 0.570137\n",
      "Loss after iteration 562: 0.570064\n",
      "Loss after iteration 563: 0.569991\n",
      "Loss after iteration 564: 0.569917\n",
      "Loss after iteration 565: 0.569844\n",
      "Loss after iteration 566: 0.569771\n",
      "Loss after iteration 567: 0.569698\n",
      "Loss after iteration 568: 0.569625\n",
      "Loss after iteration 569: 0.569552\n",
      "Loss after iteration 570: 0.569479\n",
      "Loss after iteration 571: 0.569406\n",
      "Loss after iteration 572: 0.569333\n",
      "Loss after iteration 573: 0.569260\n",
      "Loss after iteration 574: 0.569187\n",
      "Loss after iteration 575: 0.569115\n",
      "Loss after iteration 576: 0.569042\n",
      "Loss after iteration 577: 0.568969\n",
      "Loss after iteration 578: 0.568897\n",
      "Loss after iteration 579: 0.568824\n",
      "Loss after iteration 580: 0.568752\n",
      "Loss after iteration 581: 0.568679\n",
      "Loss after iteration 582: 0.568607\n",
      "Loss after iteration 583: 0.568534\n",
      "Loss after iteration 584: 0.568462\n",
      "Loss after iteration 585: 0.568389\n",
      "Loss after iteration 586: 0.568317\n",
      "Loss after iteration 587: 0.568245\n",
      "Loss after iteration 588: 0.568173\n",
      "Loss after iteration 589: 0.568100\n",
      "Loss after iteration 590: 0.568028\n",
      "Loss after iteration 591: 0.567956\n",
      "Loss after iteration 592: 0.567884\n",
      "Loss after iteration 593: 0.567812\n",
      "Loss after iteration 594: 0.567740\n",
      "Loss after iteration 595: 0.567668\n",
      "Loss after iteration 596: 0.567596\n",
      "Loss after iteration 597: 0.567524\n",
      "Loss after iteration 598: 0.567452\n",
      "Loss after iteration 599: 0.567380\n",
      "Loss after iteration 600: 0.567308\n",
      "Loss after iteration 601: 0.567237\n",
      "Loss after iteration 602: 0.567165\n",
      "Loss after iteration 603: 0.567093\n",
      "Loss after iteration 604: 0.567021\n",
      "Loss after iteration 605: 0.566950\n",
      "Loss after iteration 606: 0.566878\n",
      "Loss after iteration 607: 0.566806\n",
      "Loss after iteration 608: 0.566735\n",
      "Loss after iteration 609: 0.566663\n",
      "Loss after iteration 610: 0.566592\n",
      "Loss after iteration 611: 0.566520\n",
      "Loss after iteration 612: 0.566449\n",
      "Loss after iteration 613: 0.566377\n",
      "Loss after iteration 614: 0.566306\n",
      "Loss after iteration 615: 0.566235\n",
      "Loss after iteration 616: 0.566163\n",
      "Loss after iteration 617: 0.566092\n",
      "Loss after iteration 618: 0.566021\n",
      "Loss after iteration 619: 0.565949\n",
      "Loss after iteration 620: 0.565878\n",
      "Loss after iteration 621: 0.565807\n",
      "Loss after iteration 622: 0.565736\n",
      "Loss after iteration 623: 0.565665\n",
      "Loss after iteration 624: 0.565594\n",
      "Loss after iteration 625: 0.565522\n",
      "Loss after iteration 626: 0.565451\n",
      "Loss after iteration 627: 0.565380\n",
      "Loss after iteration 628: 0.565309\n",
      "Loss after iteration 629: 0.565238\n",
      "Loss after iteration 630: 0.565167\n",
      "Loss after iteration 631: 0.565096\n",
      "Loss after iteration 632: 0.565025\n",
      "Loss after iteration 633: 0.564955\n",
      "Loss after iteration 634: 0.564884\n",
      "Loss after iteration 635: 0.564813\n",
      "Loss after iteration 636: 0.564742\n",
      "Loss after iteration 637: 0.564671\n",
      "Loss after iteration 638: 0.564600\n",
      "Loss after iteration 639: 0.564530\n",
      "Loss after iteration 640: 0.564459\n",
      "Loss after iteration 641: 0.564388\n",
      "Loss after iteration 642: 0.564318\n",
      "Loss after iteration 643: 0.564247\n",
      "Loss after iteration 644: 0.564176\n",
      "Loss after iteration 645: 0.564106\n",
      "Loss after iteration 646: 0.564035\n",
      "Loss after iteration 647: 0.563965\n",
      "Loss after iteration 648: 0.563894\n",
      "Loss after iteration 649: 0.563824\n",
      "Loss after iteration 650: 0.563753\n",
      "Loss after iteration 651: 0.563683\n",
      "Loss after iteration 652: 0.563612\n",
      "Loss after iteration 653: 0.563542\n",
      "Loss after iteration 654: 0.563472\n",
      "Loss after iteration 655: 0.563401\n",
      "Loss after iteration 656: 0.563331\n",
      "Loss after iteration 657: 0.563261\n",
      "Loss after iteration 658: 0.563190\n",
      "Loss after iteration 659: 0.563120\n",
      "Loss after iteration 660: 0.563050\n",
      "Loss after iteration 661: 0.562980\n",
      "Loss after iteration 662: 0.562910\n",
      "Loss after iteration 663: 0.562839\n",
      "Loss after iteration 664: 0.562769\n",
      "Loss after iteration 665: 0.562699\n",
      "Loss after iteration 666: 0.562629\n",
      "Loss after iteration 667: 0.562559\n",
      "Loss after iteration 668: 0.562489\n",
      "Loss after iteration 669: 0.562419\n",
      "Loss after iteration 670: 0.562349\n",
      "Loss after iteration 671: 0.562279\n",
      "Loss after iteration 672: 0.562210\n",
      "Loss after iteration 673: 0.562140\n",
      "Loss after iteration 674: 0.562070\n",
      "Loss after iteration 675: 0.562000\n",
      "Loss after iteration 676: 0.561930\n",
      "Loss after iteration 677: 0.561861\n",
      "Loss after iteration 678: 0.561791\n",
      "Loss after iteration 679: 0.561721\n",
      "Loss after iteration 680: 0.561652\n",
      "Loss after iteration 681: 0.561582\n",
      "Loss after iteration 682: 0.561512\n",
      "Loss after iteration 683: 0.561443\n",
      "Loss after iteration 684: 0.561373\n",
      "Loss after iteration 685: 0.561304\n",
      "Loss after iteration 686: 0.561234\n",
      "Loss after iteration 687: 0.561165\n",
      "Loss after iteration 688: 0.561096\n",
      "Loss after iteration 689: 0.561026\n",
      "Loss after iteration 690: 0.560957\n",
      "Loss after iteration 691: 0.560888\n",
      "Loss after iteration 692: 0.560818\n",
      "Loss after iteration 693: 0.560749\n",
      "Loss after iteration 694: 0.560680\n",
      "Loss after iteration 695: 0.560611\n",
      "Loss after iteration 696: 0.560542\n",
      "Loss after iteration 697: 0.560473\n",
      "Loss after iteration 698: 0.560404\n",
      "Loss after iteration 699: 0.560335\n",
      "Loss after iteration 700: 0.560266\n",
      "Loss after iteration 701: 0.560197\n",
      "Loss after iteration 702: 0.560128\n",
      "Loss after iteration 703: 0.560059\n",
      "Loss after iteration 704: 0.559990\n",
      "Loss after iteration 705: 0.559922\n",
      "Loss after iteration 706: 0.559853\n",
      "Loss after iteration 707: 0.559784\n",
      "Loss after iteration 708: 0.559716\n",
      "Loss after iteration 709: 0.559647\n",
      "Loss after iteration 710: 0.559579\n",
      "Loss after iteration 711: 0.559510\n",
      "Loss after iteration 712: 0.559442\n",
      "Loss after iteration 713: 0.559373\n",
      "Loss after iteration 714: 0.559305\n",
      "Loss after iteration 715: 0.559237\n",
      "Loss after iteration 716: 0.559168\n",
      "Loss after iteration 717: 0.559100\n",
      "Loss after iteration 718: 0.559032\n",
      "Loss after iteration 719: 0.558964\n",
      "Loss after iteration 720: 0.558896\n",
      "Loss after iteration 721: 0.558828\n",
      "Loss after iteration 722: 0.558760\n",
      "Loss after iteration 723: 0.558692\n",
      "Loss after iteration 724: 0.558624\n",
      "Loss after iteration 725: 0.558556\n",
      "Loss after iteration 726: 0.558488\n",
      "Loss after iteration 727: 0.558421\n",
      "Loss after iteration 728: 0.558353\n",
      "Loss after iteration 729: 0.558285\n",
      "Loss after iteration 730: 0.558218\n",
      "Loss after iteration 731: 0.558150\n",
      "Loss after iteration 732: 0.558083\n",
      "Loss after iteration 733: 0.558015\n",
      "Loss after iteration 734: 0.557948\n",
      "Loss after iteration 735: 0.557881\n",
      "Loss after iteration 736: 0.557814\n",
      "Loss after iteration 737: 0.557746\n",
      "Loss after iteration 738: 0.557679\n",
      "Loss after iteration 739: 0.557612\n",
      "Loss after iteration 740: 0.557545\n",
      "Loss after iteration 741: 0.557478\n",
      "Loss after iteration 742: 0.557411\n",
      "Loss after iteration 743: 0.557345\n",
      "Loss after iteration 744: 0.557278\n",
      "Loss after iteration 745: 0.557211\n",
      "Loss after iteration 746: 0.557144\n",
      "Loss after iteration 747: 0.557078\n",
      "Loss after iteration 748: 0.557011\n",
      "Loss after iteration 749: 0.556945\n",
      "Loss after iteration 750: 0.556879\n",
      "Loss after iteration 751: 0.556812\n",
      "Loss after iteration 752: 0.556746\n",
      "Loss after iteration 753: 0.556680\n",
      "Loss after iteration 754: 0.556614\n",
      "Loss after iteration 755: 0.556548\n",
      "Loss after iteration 756: 0.556482\n",
      "Loss after iteration 757: 0.556416\n",
      "Loss after iteration 758: 0.556350\n",
      "Loss after iteration 759: 0.556284\n",
      "Loss after iteration 760: 0.556219\n",
      "Loss after iteration 761: 0.556153\n",
      "Loss after iteration 762: 0.556087\n",
      "Loss after iteration 763: 0.556022\n",
      "Loss after iteration 764: 0.555957\n",
      "Loss after iteration 765: 0.555891\n",
      "Loss after iteration 766: 0.555826\n",
      "Loss after iteration 767: 0.555761\n",
      "Loss after iteration 768: 0.555696\n",
      "Loss after iteration 769: 0.555631\n",
      "Loss after iteration 770: 0.555566\n",
      "Loss after iteration 771: 0.555501\n",
      "Loss after iteration 772: 0.555436\n",
      "Loss after iteration 773: 0.555371\n",
      "Loss after iteration 774: 0.555307\n",
      "Loss after iteration 775: 0.555242\n",
      "Loss after iteration 776: 0.555178\n",
      "Loss after iteration 777: 0.555113\n",
      "Loss after iteration 778: 0.555049\n",
      "Loss after iteration 779: 0.554985\n",
      "Loss after iteration 780: 0.554921\n",
      "Loss after iteration 781: 0.554856\n",
      "Loss after iteration 782: 0.554793\n",
      "Loss after iteration 783: 0.554729\n",
      "Loss after iteration 784: 0.554665\n",
      "Loss after iteration 785: 0.554601\n",
      "Loss after iteration 786: 0.554537\n",
      "Loss after iteration 787: 0.554474\n",
      "Loss after iteration 788: 0.554410\n",
      "Loss after iteration 789: 0.554347\n",
      "Loss after iteration 790: 0.554284\n",
      "Loss after iteration 791: 0.554220\n",
      "Loss after iteration 792: 0.554157\n",
      "Loss after iteration 793: 0.554094\n",
      "Loss after iteration 794: 0.554031\n",
      "Loss after iteration 795: 0.553969\n",
      "Loss after iteration 796: 0.553906\n",
      "Loss after iteration 797: 0.553843\n",
      "Loss after iteration 798: 0.553781\n",
      "Loss after iteration 799: 0.553718\n",
      "Loss after iteration 800: 0.553656\n",
      "Loss after iteration 801: 0.553593\n",
      "Loss after iteration 802: 0.553531\n",
      "Loss after iteration 803: 0.553469\n",
      "Loss after iteration 804: 0.553407\n",
      "Loss after iteration 805: 0.553345\n",
      "Loss after iteration 806: 0.553283\n",
      "Loss after iteration 807: 0.553221\n",
      "Loss after iteration 808: 0.553160\n",
      "Loss after iteration 809: 0.553098\n",
      "Loss after iteration 810: 0.553037\n",
      "Loss after iteration 811: 0.552975\n",
      "Loss after iteration 812: 0.552914\n",
      "Loss after iteration 813: 0.552853\n",
      "Loss after iteration 814: 0.552792\n",
      "Loss after iteration 815: 0.552731\n",
      "Loss after iteration 816: 0.552670\n",
      "Loss after iteration 817: 0.552609\n",
      "Loss after iteration 818: 0.552549\n",
      "Loss after iteration 819: 0.552488\n",
      "Loss after iteration 820: 0.552428\n",
      "Loss after iteration 821: 0.552367\n",
      "Loss after iteration 822: 0.552307\n",
      "Loss after iteration 823: 0.552247\n",
      "Loss after iteration 824: 0.552187\n",
      "Loss after iteration 825: 0.552127\n",
      "Loss after iteration 826: 0.552067\n",
      "Loss after iteration 827: 0.552007\n",
      "Loss after iteration 828: 0.551948\n",
      "Loss after iteration 829: 0.551888\n",
      "Loss after iteration 830: 0.551829\n",
      "Loss after iteration 831: 0.551769\n",
      "Loss after iteration 832: 0.551710\n",
      "Loss after iteration 833: 0.551651\n",
      "Loss after iteration 834: 0.551592\n",
      "Loss after iteration 835: 0.551533\n",
      "Loss after iteration 836: 0.551474\n",
      "Loss after iteration 837: 0.551416\n",
      "Loss after iteration 838: 0.551357\n",
      "Loss after iteration 839: 0.551299\n",
      "Loss after iteration 840: 0.551240\n",
      "Loss after iteration 841: 0.551182\n",
      "Loss after iteration 842: 0.551124\n",
      "Loss after iteration 843: 0.551066\n",
      "Loss after iteration 844: 0.551008\n",
      "Loss after iteration 845: 0.550950\n",
      "Loss after iteration 846: 0.550892\n",
      "Loss after iteration 847: 0.550835\n",
      "Loss after iteration 848: 0.550777\n",
      "Loss after iteration 849: 0.550720\n",
      "Loss after iteration 850: 0.550662\n",
      "Loss after iteration 851: 0.550605\n",
      "Loss after iteration 852: 0.550548\n",
      "Loss after iteration 853: 0.550491\n",
      "Loss after iteration 854: 0.550434\n",
      "Loss after iteration 855: 0.550378\n",
      "Loss after iteration 856: 0.550321\n",
      "Loss after iteration 857: 0.550264\n",
      "Loss after iteration 858: 0.550208\n",
      "Loss after iteration 859: 0.550152\n",
      "Loss after iteration 860: 0.550095\n",
      "Loss after iteration 861: 0.550039\n",
      "Loss after iteration 862: 0.549983\n",
      "Loss after iteration 863: 0.549927\n",
      "Loss after iteration 864: 0.549872\n",
      "Loss after iteration 865: 0.549816\n",
      "Loss after iteration 866: 0.549760\n",
      "Loss after iteration 867: 0.549705\n",
      "Loss after iteration 868: 0.549650\n",
      "Loss after iteration 869: 0.549595\n",
      "Loss after iteration 870: 0.549539\n",
      "Loss after iteration 871: 0.549484\n",
      "Loss after iteration 872: 0.549430\n",
      "Loss after iteration 873: 0.549375\n",
      "Loss after iteration 874: 0.549320\n",
      "Loss after iteration 875: 0.549266\n",
      "Loss after iteration 876: 0.549211\n",
      "Loss after iteration 877: 0.549157\n",
      "Loss after iteration 878: 0.549103\n",
      "Loss after iteration 879: 0.549048\n",
      "Loss after iteration 880: 0.548994\n",
      "Loss after iteration 881: 0.548941\n",
      "Loss after iteration 882: 0.548887\n",
      "Loss after iteration 883: 0.548833\n",
      "Loss after iteration 884: 0.548780\n",
      "Loss after iteration 885: 0.548726\n",
      "Loss after iteration 886: 0.548673\n",
      "Loss after iteration 887: 0.548620\n",
      "Loss after iteration 888: 0.548567\n",
      "Loss after iteration 889: 0.548514\n",
      "Loss after iteration 890: 0.548461\n",
      "Loss after iteration 891: 0.548408\n",
      "Loss after iteration 892: 0.548355\n",
      "Loss after iteration 893: 0.548303\n",
      "Loss after iteration 894: 0.548250\n",
      "Loss after iteration 895: 0.548198\n",
      "Loss after iteration 896: 0.548146\n",
      "Loss after iteration 897: 0.548094\n",
      "Loss after iteration 898: 0.548042\n",
      "Loss after iteration 899: 0.547990\n",
      "Loss after iteration 900: 0.547938\n",
      "Loss after iteration 901: 0.547886\n",
      "Loss after iteration 902: 0.547835\n",
      "Loss after iteration 903: 0.547783\n",
      "Loss after iteration 904: 0.547732\n",
      "Loss after iteration 905: 0.547681\n",
      "Loss after iteration 906: 0.547629\n",
      "Loss after iteration 907: 0.547578\n",
      "Loss after iteration 908: 0.547528\n",
      "Loss after iteration 909: 0.547477\n",
      "Loss after iteration 910: 0.547426\n",
      "Loss after iteration 911: 0.547375\n",
      "Loss after iteration 912: 0.547325\n",
      "Loss after iteration 913: 0.547275\n",
      "Loss after iteration 914: 0.547224\n",
      "Loss after iteration 915: 0.547174\n",
      "Loss after iteration 916: 0.547124\n",
      "Loss after iteration 917: 0.547074\n",
      "Loss after iteration 918: 0.547024\n",
      "Loss after iteration 919: 0.546975\n",
      "Loss after iteration 920: 0.546925\n",
      "Loss after iteration 921: 0.546875\n",
      "Loss after iteration 922: 0.546826\n",
      "Loss after iteration 923: 0.546777\n",
      "Loss after iteration 924: 0.546728\n",
      "Loss after iteration 925: 0.546678\n",
      "Loss after iteration 926: 0.546629\n",
      "Loss after iteration 927: 0.546581\n",
      "Loss after iteration 928: 0.546532\n",
      "Loss after iteration 929: 0.546483\n",
      "Loss after iteration 930: 0.546435\n",
      "Loss after iteration 931: 0.546386\n",
      "Loss after iteration 932: 0.546338\n",
      "Loss after iteration 933: 0.546289\n",
      "Loss after iteration 934: 0.546241\n",
      "Loss after iteration 935: 0.546193\n",
      "Loss after iteration 936: 0.546145\n",
      "Loss after iteration 937: 0.546097\n",
      "Loss after iteration 938: 0.546050\n",
      "Loss after iteration 939: 0.546002\n",
      "Loss after iteration 940: 0.545955\n",
      "Loss after iteration 941: 0.545907\n",
      "Loss after iteration 942: 0.545860\n",
      "Loss after iteration 943: 0.545813\n",
      "Loss after iteration 944: 0.545765\n",
      "Loss after iteration 945: 0.545718\n",
      "Loss after iteration 946: 0.545671\n",
      "Loss after iteration 947: 0.545625\n",
      "Loss after iteration 948: 0.545578\n",
      "Loss after iteration 949: 0.545531\n",
      "Loss after iteration 950: 0.545485\n",
      "Loss after iteration 951: 0.545438\n",
      "Loss after iteration 952: 0.545392\n",
      "Loss after iteration 953: 0.545346\n",
      "Loss after iteration 954: 0.545300\n",
      "Loss after iteration 955: 0.545254\n",
      "Loss after iteration 956: 0.545208\n",
      "Loss after iteration 957: 0.545162\n",
      "Loss after iteration 958: 0.545116\n",
      "Loss after iteration 959: 0.545070\n",
      "Loss after iteration 960: 0.545025\n",
      "Loss after iteration 961: 0.544979\n",
      "Loss after iteration 962: 0.544934\n",
      "Loss after iteration 963: 0.544889\n",
      "Loss after iteration 964: 0.544844\n",
      "Loss after iteration 965: 0.544798\n",
      "Loss after iteration 966: 0.544753\n",
      "Loss after iteration 967: 0.544709\n",
      "Loss after iteration 968: 0.544664\n",
      "Loss after iteration 969: 0.544619\n",
      "Loss after iteration 970: 0.544574\n",
      "Loss after iteration 971: 0.544530\n",
      "Loss after iteration 972: 0.544486\n",
      "Loss after iteration 973: 0.544441\n",
      "Loss after iteration 974: 0.544397\n",
      "Loss after iteration 975: 0.544353\n",
      "Loss after iteration 976: 0.544309\n",
      "Loss after iteration 977: 0.544265\n",
      "Loss after iteration 978: 0.544221\n",
      "Loss after iteration 979: 0.544177\n",
      "Loss after iteration 980: 0.544133\n",
      "Loss after iteration 981: 0.544090\n",
      "Loss after iteration 982: 0.544046\n",
      "Loss after iteration 983: 0.544003\n",
      "Loss after iteration 984: 0.543960\n",
      "Loss after iteration 985: 0.543916\n",
      "Loss after iteration 986: 0.543873\n",
      "Loss after iteration 987: 0.543830\n",
      "Loss after iteration 988: 0.543787\n",
      "Loss after iteration 989: 0.543744\n",
      "Loss after iteration 990: 0.543702\n",
      "Loss after iteration 991: 0.543659\n",
      "Loss after iteration 992: 0.543616\n",
      "Loss after iteration 993: 0.543574\n",
      "Loss after iteration 994: 0.543531\n",
      "Loss after iteration 995: 0.543489\n",
      "Loss after iteration 996: 0.543447\n",
      "Loss after iteration 997: 0.543404\n",
      "Loss after iteration 998: 0.543362\n",
      "Loss after iteration 999: 0.543320\n",
      "Loss after iteration 1000: 0.543278\n",
      "Loss after iteration 1001: 0.543236\n",
      "Loss after iteration 1002: 0.543195\n",
      "Loss after iteration 1003: 0.543153\n",
      "Loss after iteration 1004: 0.543111\n",
      "Loss after iteration 1005: 0.543070\n",
      "Loss after iteration 1006: 0.543028\n",
      "Loss after iteration 1007: 0.542987\n",
      "Loss after iteration 1008: 0.542946\n",
      "Loss after iteration 1009: 0.542905\n",
      "Loss after iteration 1010: 0.542863\n",
      "Loss after iteration 1011: 0.542822\n",
      "Loss after iteration 1012: 0.542781\n",
      "Loss after iteration 1013: 0.542741\n",
      "Loss after iteration 1014: 0.542700\n",
      "Loss after iteration 1015: 0.542659\n",
      "Loss after iteration 1016: 0.542618\n",
      "Loss after iteration 1017: 0.542578\n",
      "Loss after iteration 1018: 0.542537\n",
      "Loss after iteration 1019: 0.542497\n",
      "Loss after iteration 1020: 0.542457\n",
      "Loss after iteration 1021: 0.542416\n",
      "Loss after iteration 1022: 0.542376\n",
      "Loss after iteration 1023: 0.542336\n",
      "Loss after iteration 1024: 0.542296\n",
      "Loss after iteration 1025: 0.542256\n",
      "Loss after iteration 1026: 0.542216\n",
      "Loss after iteration 1027: 0.542176\n",
      "Loss after iteration 1028: 0.542137\n",
      "Loss after iteration 1029: 0.542097\n",
      "Loss after iteration 1030: 0.542058\n",
      "Loss after iteration 1031: 0.542018\n",
      "Loss after iteration 1032: 0.541979\n",
      "Loss after iteration 1033: 0.541939\n",
      "Loss after iteration 1034: 0.541900\n",
      "Loss after iteration 1035: 0.541861\n",
      "Loss after iteration 1036: 0.541822\n",
      "Loss after iteration 1037: 0.541783\n",
      "Loss after iteration 1038: 0.541744\n",
      "Loss after iteration 1039: 0.541705\n",
      "Loss after iteration 1040: 0.541666\n",
      "Loss after iteration 1041: 0.541627\n",
      "Loss after iteration 1042: 0.541589\n",
      "Loss after iteration 1043: 0.541550\n",
      "Loss after iteration 1044: 0.541511\n",
      "Loss after iteration 1045: 0.541473\n",
      "Loss after iteration 1046: 0.541435\n",
      "Loss after iteration 1047: 0.541396\n",
      "Loss after iteration 1048: 0.541358\n",
      "Loss after iteration 1049: 0.541320\n",
      "Loss after iteration 1050: 0.541282\n",
      "Loss after iteration 1051: 0.541244\n",
      "Loss after iteration 1052: 0.541206\n",
      "Loss after iteration 1053: 0.541168\n",
      "Loss after iteration 1054: 0.541130\n",
      "Loss after iteration 1055: 0.541092\n",
      "Loss after iteration 1056: 0.541054\n",
      "Loss after iteration 1057: 0.541017\n",
      "Loss after iteration 1058: 0.540979\n",
      "Loss after iteration 1059: 0.540942\n",
      "Loss after iteration 1060: 0.540904\n",
      "Loss after iteration 1061: 0.540867\n",
      "Loss after iteration 1062: 0.540829\n",
      "Loss after iteration 1063: 0.540792\n",
      "Loss after iteration 1064: 0.540755\n",
      "Loss after iteration 1065: 0.540718\n",
      "Loss after iteration 1066: 0.540681\n",
      "Loss after iteration 1067: 0.540644\n",
      "Loss after iteration 1068: 0.540607\n",
      "Loss after iteration 1069: 0.540570\n",
      "Loss after iteration 1070: 0.540533\n",
      "Loss after iteration 1071: 0.540496\n",
      "Loss after iteration 1072: 0.540460\n",
      "Loss after iteration 1073: 0.540423\n",
      "Loss after iteration 1074: 0.540387\n",
      "Loss after iteration 1075: 0.540350\n",
      "Loss after iteration 1076: 0.540314\n",
      "Loss after iteration 1077: 0.540277\n",
      "Loss after iteration 1078: 0.540241\n",
      "Loss after iteration 1079: 0.540205\n",
      "Loss after iteration 1080: 0.540169\n",
      "Loss after iteration 1081: 0.540132\n",
      "Loss after iteration 1082: 0.540096\n",
      "Loss after iteration 1083: 0.540060\n",
      "Loss after iteration 1084: 0.540024\n",
      "Loss after iteration 1085: 0.539989\n",
      "Loss after iteration 1086: 0.539953\n",
      "Loss after iteration 1087: 0.539917\n",
      "Loss after iteration 1088: 0.539881\n",
      "Loss after iteration 1089: 0.539846\n",
      "Loss after iteration 1090: 0.539810\n",
      "Loss after iteration 1091: 0.539774\n",
      "Loss after iteration 1092: 0.539739\n",
      "Loss after iteration 1093: 0.539704\n",
      "Loss after iteration 1094: 0.539668\n",
      "Loss after iteration 1095: 0.539633\n",
      "Loss after iteration 1096: 0.539598\n",
      "Loss after iteration 1097: 0.539562\n",
      "Loss after iteration 1098: 0.539527\n",
      "Loss after iteration 1099: 0.539492\n",
      "Loss after iteration 1100: 0.539457\n",
      "Loss after iteration 1101: 0.539422\n",
      "Loss after iteration 1102: 0.539387\n",
      "Loss after iteration 1103: 0.539352\n",
      "Loss after iteration 1104: 0.539318\n",
      "Loss after iteration 1105: 0.539283\n",
      "Loss after iteration 1106: 0.539248\n",
      "Loss after iteration 1107: 0.539213\n",
      "Loss after iteration 1108: 0.539179\n",
      "Loss after iteration 1109: 0.539144\n",
      "Loss after iteration 1110: 0.539110\n",
      "Loss after iteration 1111: 0.539075\n",
      "Loss after iteration 1112: 0.539041\n",
      "Loss after iteration 1113: 0.539007\n",
      "Loss after iteration 1114: 0.538972\n",
      "Loss after iteration 1115: 0.538938\n",
      "Loss after iteration 1116: 0.538904\n",
      "Loss after iteration 1117: 0.538870\n",
      "Loss after iteration 1118: 0.538836\n",
      "Loss after iteration 1119: 0.538802\n",
      "Loss after iteration 1120: 0.538768\n",
      "Loss after iteration 1121: 0.538734\n",
      "Loss after iteration 1122: 0.538700\n",
      "Loss after iteration 1123: 0.538666\n",
      "Loss after iteration 1124: 0.538632\n",
      "Loss after iteration 1125: 0.538599\n",
      "Loss after iteration 1126: 0.538565\n",
      "Loss after iteration 1127: 0.538531\n",
      "Loss after iteration 1128: 0.538498\n",
      "Loss after iteration 1129: 0.538464\n",
      "Loss after iteration 1130: 0.538431\n",
      "Loss after iteration 1131: 0.538397\n",
      "Loss after iteration 1132: 0.538364\n",
      "Loss after iteration 1133: 0.538330\n",
      "Loss after iteration 1134: 0.538297\n",
      "Loss after iteration 1135: 0.538264\n",
      "Loss after iteration 1136: 0.538231\n",
      "Loss after iteration 1137: 0.538198\n",
      "Loss after iteration 1138: 0.538164\n",
      "Loss after iteration 1139: 0.538131\n",
      "Loss after iteration 1140: 0.538098\n",
      "Loss after iteration 1141: 0.538065\n",
      "Loss after iteration 1142: 0.538032\n",
      "Loss after iteration 1143: 0.538000\n",
      "Loss after iteration 1144: 0.537967\n",
      "Loss after iteration 1145: 0.537934\n",
      "Loss after iteration 1146: 0.537901\n",
      "Loss after iteration 1147: 0.537868\n",
      "Loss after iteration 1148: 0.537836\n",
      "Loss after iteration 1149: 0.537803\n",
      "Loss after iteration 1150: 0.537771\n",
      "Loss after iteration 1151: 0.537738\n",
      "Loss after iteration 1152: 0.537706\n",
      "Loss after iteration 1153: 0.537673\n",
      "Loss after iteration 1154: 0.537641\n",
      "Loss after iteration 1155: 0.537608\n",
      "Loss after iteration 1156: 0.537576\n",
      "Loss after iteration 1157: 0.537544\n",
      "Loss after iteration 1158: 0.537512\n",
      "Loss after iteration 1159: 0.537479\n",
      "Loss after iteration 1160: 0.537447\n",
      "Loss after iteration 1161: 0.537415\n",
      "Loss after iteration 1162: 0.537383\n",
      "Loss after iteration 1163: 0.537351\n",
      "Loss after iteration 1164: 0.537319\n",
      "Loss after iteration 1165: 0.537287\n",
      "Loss after iteration 1166: 0.537255\n",
      "Loss after iteration 1167: 0.537223\n",
      "Loss after iteration 1168: 0.537191\n",
      "Loss after iteration 1169: 0.537160\n",
      "Loss after iteration 1170: 0.537128\n",
      "Loss after iteration 1171: 0.537096\n",
      "Loss after iteration 1172: 0.537065\n",
      "Loss after iteration 1173: 0.537033\n",
      "Loss after iteration 1174: 0.537001\n",
      "Loss after iteration 1175: 0.536970\n",
      "Loss after iteration 1176: 0.536938\n",
      "Loss after iteration 1177: 0.536907\n",
      "Loss after iteration 1178: 0.536875\n",
      "Loss after iteration 1179: 0.536844\n",
      "Loss after iteration 1180: 0.536813\n",
      "Loss after iteration 1181: 0.536781\n",
      "Loss after iteration 1182: 0.536750\n",
      "Loss after iteration 1183: 0.536719\n",
      "Loss after iteration 1184: 0.536688\n",
      "Loss after iteration 1185: 0.536656\n",
      "Loss after iteration 1186: 0.536625\n",
      "Loss after iteration 1187: 0.536594\n",
      "Loss after iteration 1188: 0.536563\n",
      "Loss after iteration 1189: 0.536532\n",
      "Loss after iteration 1190: 0.536501\n",
      "Loss after iteration 1191: 0.536470\n",
      "Loss after iteration 1192: 0.536439\n",
      "Loss after iteration 1193: 0.536408\n",
      "Loss after iteration 1194: 0.536377\n",
      "Loss after iteration 1195: 0.536347\n",
      "Loss after iteration 1196: 0.536316\n",
      "Loss after iteration 1197: 0.536285\n",
      "Loss after iteration 1198: 0.536254\n",
      "Loss after iteration 1199: 0.536224\n",
      "Loss after iteration 1200: 0.536193\n",
      "Loss after iteration 1201: 0.536162\n",
      "Loss after iteration 1202: 0.536132\n",
      "Loss after iteration 1203: 0.536101\n",
      "Loss after iteration 1204: 0.536071\n",
      "Loss after iteration 1205: 0.536040\n",
      "Loss after iteration 1206: 0.536010\n",
      "Loss after iteration 1207: 0.535979\n",
      "Loss after iteration 1208: 0.535949\n",
      "Loss after iteration 1209: 0.535919\n",
      "Loss after iteration 1210: 0.535888\n",
      "Loss after iteration 1211: 0.535858\n",
      "Loss after iteration 1212: 0.535828\n",
      "Loss after iteration 1213: 0.535798\n",
      "Loss after iteration 1214: 0.535767\n",
      "Loss after iteration 1215: 0.535737\n",
      "Loss after iteration 1216: 0.535707\n",
      "Loss after iteration 1217: 0.535677\n",
      "Loss after iteration 1218: 0.535647\n",
      "Loss after iteration 1219: 0.535617\n",
      "Loss after iteration 1220: 0.535587\n",
      "Loss after iteration 1221: 0.535557\n",
      "Loss after iteration 1222: 0.535527\n",
      "Loss after iteration 1223: 0.535497\n",
      "Loss after iteration 1224: 0.535467\n",
      "Loss after iteration 1225: 0.535437\n",
      "Loss after iteration 1226: 0.535408\n",
      "Loss after iteration 1227: 0.535378\n",
      "Loss after iteration 1228: 0.535348\n",
      "Loss after iteration 1229: 0.535318\n",
      "Loss after iteration 1230: 0.535289\n",
      "Loss after iteration 1231: 0.535259\n",
      "Loss after iteration 1232: 0.535229\n",
      "Loss after iteration 1233: 0.535200\n",
      "Loss after iteration 1234: 0.535170\n",
      "Loss after iteration 1235: 0.535141\n",
      "Loss after iteration 1236: 0.535111\n",
      "Loss after iteration 1237: 0.535082\n",
      "Loss after iteration 1238: 0.535052\n",
      "Loss after iteration 1239: 0.535023\n",
      "Loss after iteration 1240: 0.534993\n",
      "Loss after iteration 1241: 0.534964\n",
      "Loss after iteration 1242: 0.534935\n",
      "Loss after iteration 1243: 0.534905\n",
      "Loss after iteration 1244: 0.534876\n",
      "Loss after iteration 1245: 0.534847\n",
      "Loss after iteration 1246: 0.534817\n",
      "Loss after iteration 1247: 0.534788\n",
      "Loss after iteration 1248: 0.534759\n",
      "Loss after iteration 1249: 0.534730\n",
      "Loss after iteration 1250: 0.534701\n",
      "Loss after iteration 1251: 0.534671\n",
      "Loss after iteration 1252: 0.534642\n",
      "Loss after iteration 1253: 0.534613\n",
      "Loss after iteration 1254: 0.534584\n",
      "Loss after iteration 1255: 0.534555\n",
      "Loss after iteration 1256: 0.534526\n",
      "Loss after iteration 1257: 0.534497\n",
      "Loss after iteration 1258: 0.534468\n",
      "Loss after iteration 1259: 0.534439\n",
      "Loss after iteration 1260: 0.534411\n",
      "Loss after iteration 1261: 0.534382\n",
      "Loss after iteration 1262: 0.534353\n",
      "Loss after iteration 1263: 0.534324\n",
      "Loss after iteration 1264: 0.534295\n",
      "Loss after iteration 1265: 0.534266\n",
      "Loss after iteration 1266: 0.534238\n",
      "Loss after iteration 1267: 0.534209\n",
      "Loss after iteration 1268: 0.534180\n",
      "Loss after iteration 1269: 0.534152\n",
      "Loss after iteration 1270: 0.534123\n",
      "Loss after iteration 1271: 0.534094\n",
      "Loss after iteration 1272: 0.534066\n",
      "Loss after iteration 1273: 0.534037\n",
      "Loss after iteration 1274: 0.534009\n",
      "Loss after iteration 1275: 0.533980\n",
      "Loss after iteration 1276: 0.533952\n",
      "Loss after iteration 1277: 0.533923\n",
      "Loss after iteration 1278: 0.533895\n",
      "Loss after iteration 1279: 0.533866\n",
      "Loss after iteration 1280: 0.533838\n",
      "Loss after iteration 1281: 0.533809\n",
      "Loss after iteration 1282: 0.533781\n",
      "Loss after iteration 1283: 0.533753\n",
      "Loss after iteration 1284: 0.533724\n",
      "Loss after iteration 1285: 0.533696\n",
      "Loss after iteration 1286: 0.533668\n",
      "Loss after iteration 1287: 0.533639\n",
      "Loss after iteration 1288: 0.533611\n",
      "Loss after iteration 1289: 0.533583\n",
      "Loss after iteration 1290: 0.533555\n",
      "Loss after iteration 1291: 0.533527\n",
      "Loss after iteration 1292: 0.533498\n",
      "Loss after iteration 1293: 0.533470\n",
      "Loss after iteration 1294: 0.533442\n",
      "Loss after iteration 1295: 0.533414\n",
      "Loss after iteration 1296: 0.533386\n",
      "Loss after iteration 1297: 0.533358\n",
      "Loss after iteration 1298: 0.533330\n",
      "Loss after iteration 1299: 0.533302\n",
      "Loss after iteration 1300: 0.533274\n",
      "Loss after iteration 1301: 0.533246\n",
      "Loss after iteration 1302: 0.533218\n",
      "Loss after iteration 1303: 0.533190\n",
      "Loss after iteration 1304: 0.533162\n",
      "Loss after iteration 1305: 0.533134\n",
      "Loss after iteration 1306: 0.533106\n",
      "Loss after iteration 1307: 0.533079\n",
      "Loss after iteration 1308: 0.533051\n",
      "Loss after iteration 1309: 0.533023\n",
      "Loss after iteration 1310: 0.532995\n",
      "Loss after iteration 1311: 0.532967\n",
      "Loss after iteration 1312: 0.532940\n",
      "Loss after iteration 1313: 0.532912\n",
      "Loss after iteration 1314: 0.532884\n",
      "Loss after iteration 1315: 0.532856\n",
      "Loss after iteration 1316: 0.532829\n",
      "Loss after iteration 1317: 0.532801\n",
      "Loss after iteration 1318: 0.532773\n",
      "Loss after iteration 1319: 0.532746\n",
      "Loss after iteration 1320: 0.532718\n",
      "Loss after iteration 1321: 0.532691\n",
      "Loss after iteration 1322: 0.532663\n",
      "Loss after iteration 1323: 0.532635\n",
      "Loss after iteration 1324: 0.532608\n",
      "Loss after iteration 1325: 0.532580\n",
      "Loss after iteration 1326: 0.532553\n",
      "Loss after iteration 1327: 0.532525\n",
      "Loss after iteration 1328: 0.532498\n",
      "Loss after iteration 1329: 0.532470\n",
      "Loss after iteration 1330: 0.532443\n",
      "Loss after iteration 1331: 0.532416\n",
      "Loss after iteration 1332: 0.532388\n",
      "Loss after iteration 1333: 0.532361\n",
      "Loss after iteration 1334: 0.532333\n",
      "Loss after iteration 1335: 0.532306\n",
      "Loss after iteration 1336: 0.532279\n",
      "Loss after iteration 1337: 0.532251\n",
      "Loss after iteration 1338: 0.532224\n",
      "Loss after iteration 1339: 0.532197\n",
      "Loss after iteration 1340: 0.532170\n",
      "Loss after iteration 1341: 0.532142\n",
      "Loss after iteration 1342: 0.532115\n",
      "Loss after iteration 1343: 0.532088\n",
      "Loss after iteration 1344: 0.532061\n",
      "Loss after iteration 1345: 0.532033\n",
      "Loss after iteration 1346: 0.532006\n",
      "Loss after iteration 1347: 0.531979\n",
      "Loss after iteration 1348: 0.531952\n",
      "Loss after iteration 1349: 0.531925\n",
      "Loss after iteration 1350: 0.531898\n",
      "Loss after iteration 1351: 0.531871\n",
      "Loss after iteration 1352: 0.531844\n",
      "Loss after iteration 1353: 0.531816\n",
      "Loss after iteration 1354: 0.531789\n",
      "Loss after iteration 1355: 0.531762\n",
      "Loss after iteration 1356: 0.531735\n",
      "Loss after iteration 1357: 0.531708\n",
      "Loss after iteration 1358: 0.531681\n",
      "Loss after iteration 1359: 0.531654\n",
      "Loss after iteration 1360: 0.531627\n",
      "Loss after iteration 1361: 0.531600\n",
      "Loss after iteration 1362: 0.531574\n",
      "Loss after iteration 1363: 0.531547\n",
      "Loss after iteration 1364: 0.531520\n",
      "Loss after iteration 1365: 0.531493\n",
      "Loss after iteration 1366: 0.531466\n",
      "Loss after iteration 1367: 0.531439\n",
      "Loss after iteration 1368: 0.531412\n",
      "Loss after iteration 1369: 0.531385\n",
      "Loss after iteration 1370: 0.531359\n",
      "Loss after iteration 1371: 0.531332\n",
      "Loss after iteration 1372: 0.531305\n",
      "Loss after iteration 1373: 0.531278\n",
      "Loss after iteration 1374: 0.531251\n",
      "Loss after iteration 1375: 0.531225\n",
      "Loss after iteration 1376: 0.531198\n",
      "Loss after iteration 1377: 0.531171\n",
      "Loss after iteration 1378: 0.531144\n",
      "Loss after iteration 1379: 0.531118\n",
      "Loss after iteration 1380: 0.531091\n",
      "Loss after iteration 1381: 0.531064\n",
      "Loss after iteration 1382: 0.531038\n",
      "Loss after iteration 1383: 0.531011\n",
      "Loss after iteration 1384: 0.530984\n",
      "Loss after iteration 1385: 0.530958\n",
      "Loss after iteration 1386: 0.530931\n",
      "Loss after iteration 1387: 0.530905\n",
      "Loss after iteration 1388: 0.530878\n",
      "Loss after iteration 1389: 0.530851\n",
      "Loss after iteration 1390: 0.530825\n",
      "Loss after iteration 1391: 0.530798\n",
      "Loss after iteration 1392: 0.530772\n",
      "Loss after iteration 1393: 0.530745\n",
      "Loss after iteration 1394: 0.530719\n",
      "Loss after iteration 1395: 0.530692\n",
      "Loss after iteration 1396: 0.530666\n",
      "Loss after iteration 1397: 0.530639\n",
      "Loss after iteration 1398: 0.530613\n",
      "Loss after iteration 1399: 0.530586\n",
      "Loss after iteration 1400: 0.530560\n",
      "Loss after iteration 1401: 0.530534\n",
      "Loss after iteration 1402: 0.530507\n",
      "Loss after iteration 1403: 0.530481\n",
      "Loss after iteration 1404: 0.530454\n",
      "Loss after iteration 1405: 0.530428\n",
      "Loss after iteration 1406: 0.530402\n",
      "Loss after iteration 1407: 0.530375\n",
      "Loss after iteration 1408: 0.530349\n",
      "Loss after iteration 1409: 0.530322\n",
      "Loss after iteration 1410: 0.530296\n",
      "Loss after iteration 1411: 0.530270\n",
      "Loss after iteration 1412: 0.530244\n",
      "Loss after iteration 1413: 0.530217\n",
      "Loss after iteration 1414: 0.530191\n",
      "Loss after iteration 1415: 0.530165\n",
      "Loss after iteration 1416: 0.530138\n",
      "Loss after iteration 1417: 0.530112\n",
      "Loss after iteration 1418: 0.530086\n",
      "Loss after iteration 1419: 0.530060\n",
      "Loss after iteration 1420: 0.530033\n",
      "Loss after iteration 1421: 0.530007\n",
      "Loss after iteration 1422: 0.529981\n",
      "Loss after iteration 1423: 0.529955\n",
      "Loss after iteration 1424: 0.529929\n",
      "Loss after iteration 1425: 0.529903\n",
      "Loss after iteration 1426: 0.529876\n",
      "Loss after iteration 1427: 0.529850\n",
      "Loss after iteration 1428: 0.529824\n",
      "Loss after iteration 1429: 0.529798\n",
      "Loss after iteration 1430: 0.529772\n",
      "Loss after iteration 1431: 0.529746\n",
      "Loss after iteration 1432: 0.529720\n",
      "Loss after iteration 1433: 0.529694\n",
      "Loss after iteration 1434: 0.529667\n",
      "Loss after iteration 1435: 0.529641\n",
      "Loss after iteration 1436: 0.529615\n",
      "Loss after iteration 1437: 0.529589\n",
      "Loss after iteration 1438: 0.529563\n",
      "Loss after iteration 1439: 0.529537\n",
      "Loss after iteration 1440: 0.529511\n",
      "Loss after iteration 1441: 0.529485\n",
      "Loss after iteration 1442: 0.529459\n",
      "Loss after iteration 1443: 0.529433\n",
      "Loss after iteration 1444: 0.529407\n",
      "Loss after iteration 1445: 0.529381\n",
      "Loss after iteration 1446: 0.529355\n",
      "Loss after iteration 1447: 0.529329\n",
      "Loss after iteration 1448: 0.529303\n",
      "Loss after iteration 1449: 0.529277\n",
      "Loss after iteration 1450: 0.529251\n",
      "Loss after iteration 1451: 0.529226\n",
      "Loss after iteration 1452: 0.529200\n",
      "Loss after iteration 1453: 0.529174\n",
      "Loss after iteration 1454: 0.529148\n",
      "Loss after iteration 1455: 0.529122\n",
      "Loss after iteration 1456: 0.529096\n",
      "Loss after iteration 1457: 0.529070\n",
      "Loss after iteration 1458: 0.529044\n",
      "Loss after iteration 1459: 0.529018\n",
      "Loss after iteration 1460: 0.528993\n",
      "Loss after iteration 1461: 0.528967\n",
      "Loss after iteration 1462: 0.528941\n",
      "Loss after iteration 1463: 0.528915\n",
      "Loss after iteration 1464: 0.528889\n",
      "Loss after iteration 1465: 0.528864\n",
      "Loss after iteration 1466: 0.528838\n",
      "Loss after iteration 1467: 0.528812\n",
      "Loss after iteration 1468: 0.528786\n",
      "Loss after iteration 1469: 0.528760\n",
      "Loss after iteration 1470: 0.528735\n",
      "Loss after iteration 1471: 0.528709\n",
      "Loss after iteration 1472: 0.528683\n",
      "Loss after iteration 1473: 0.528657\n",
      "Loss after iteration 1474: 0.528632\n",
      "Loss after iteration 1475: 0.528606\n",
      "Loss after iteration 1476: 0.528580\n",
      "Loss after iteration 1477: 0.528555\n",
      "Loss after iteration 1478: 0.528529\n",
      "Loss after iteration 1479: 0.528503\n",
      "Loss after iteration 1480: 0.528477\n",
      "Loss after iteration 1481: 0.528452\n",
      "Loss after iteration 1482: 0.528426\n",
      "Loss after iteration 1483: 0.528400\n",
      "Loss after iteration 1484: 0.528375\n",
      "Loss after iteration 1485: 0.528349\n",
      "Loss after iteration 1486: 0.528324\n",
      "Loss after iteration 1487: 0.528298\n",
      "Loss after iteration 1488: 0.528272\n",
      "Loss after iteration 1489: 0.528247\n",
      "Loss after iteration 1490: 0.528221\n",
      "Loss after iteration 1491: 0.528195\n",
      "Loss after iteration 1492: 0.528170\n",
      "Loss after iteration 1493: 0.528144\n",
      "Loss after iteration 1494: 0.528119\n",
      "Loss after iteration 1495: 0.528093\n",
      "Loss after iteration 1496: 0.528067\n",
      "Loss after iteration 1497: 0.528042\n",
      "Loss after iteration 1498: 0.528016\n",
      "Loss after iteration 1499: 0.527991\n",
      "Loss after iteration 1500: 0.527965\n",
      "Loss after iteration 1501: 0.527940\n",
      "Loss after iteration 1502: 0.527914\n",
      "Loss after iteration 1503: 0.527889\n",
      "Loss after iteration 1504: 0.527863\n",
      "Loss after iteration 1505: 0.527838\n",
      "Loss after iteration 1506: 0.527812\n",
      "Loss after iteration 1507: 0.527787\n",
      "Loss after iteration 1508: 0.527761\n",
      "Loss after iteration 1509: 0.527736\n",
      "Loss after iteration 1510: 0.527710\n",
      "Loss after iteration 1511: 0.527685\n",
      "Loss after iteration 1512: 0.527659\n",
      "Loss after iteration 1513: 0.527634\n",
      "Loss after iteration 1514: 0.527608\n",
      "Loss after iteration 1515: 0.527583\n",
      "Loss after iteration 1516: 0.527558\n",
      "Loss after iteration 1517: 0.527532\n",
      "Loss after iteration 1518: 0.527507\n",
      "Loss after iteration 1519: 0.527481\n",
      "Loss after iteration 1520: 0.527456\n",
      "Loss after iteration 1521: 0.527430\n",
      "Loss after iteration 1522: 0.527405\n",
      "Loss after iteration 1523: 0.527380\n",
      "Loss after iteration 1524: 0.527354\n",
      "Loss after iteration 1525: 0.527329\n",
      "Loss after iteration 1526: 0.527303\n",
      "Loss after iteration 1527: 0.527278\n",
      "Loss after iteration 1528: 0.527253\n",
      "Loss after iteration 1529: 0.527227\n",
      "Loss after iteration 1530: 0.527202\n",
      "Loss after iteration 1531: 0.527177\n",
      "Loss after iteration 1532: 0.527151\n",
      "Loss after iteration 1533: 0.527126\n",
      "Loss after iteration 1534: 0.527101\n",
      "Loss after iteration 1535: 0.527075\n",
      "Loss after iteration 1536: 0.527050\n",
      "Loss after iteration 1537: 0.527025\n",
      "Loss after iteration 1538: 0.526999\n",
      "Loss after iteration 1539: 0.526974\n",
      "Loss after iteration 1540: 0.526949\n",
      "Loss after iteration 1541: 0.526924\n",
      "Loss after iteration 1542: 0.526898\n",
      "Loss after iteration 1543: 0.526873\n",
      "Loss after iteration 1544: 0.526848\n",
      "Loss after iteration 1545: 0.526822\n",
      "Loss after iteration 1546: 0.526797\n",
      "Loss after iteration 1547: 0.526772\n",
      "Loss after iteration 1548: 0.526747\n",
      "Loss after iteration 1549: 0.526721\n",
      "Loss after iteration 1550: 0.526696\n",
      "Loss after iteration 1551: 0.526671\n",
      "Loss after iteration 1552: 0.526646\n",
      "Loss after iteration 1553: 0.526620\n",
      "Loss after iteration 1554: 0.526595\n",
      "Loss after iteration 1555: 0.526570\n",
      "Loss after iteration 1556: 0.526545\n",
      "Loss after iteration 1557: 0.526520\n",
      "Loss after iteration 1558: 0.526494\n",
      "Loss after iteration 1559: 0.526469\n",
      "Loss after iteration 1560: 0.526444\n",
      "Loss after iteration 1561: 0.526419\n",
      "Loss after iteration 1562: 0.526394\n",
      "Loss after iteration 1563: 0.526369\n",
      "Loss after iteration 1564: 0.526343\n",
      "Loss after iteration 1565: 0.526318\n",
      "Loss after iteration 1566: 0.526293\n",
      "Loss after iteration 1567: 0.526268\n",
      "Loss after iteration 1568: 0.526243\n",
      "Loss after iteration 1569: 0.526218\n",
      "Loss after iteration 1570: 0.526192\n",
      "Loss after iteration 1571: 0.526167\n",
      "Loss after iteration 1572: 0.526142\n",
      "Loss after iteration 1573: 0.526117\n",
      "Loss after iteration 1574: 0.526092\n",
      "Loss after iteration 1575: 0.526067\n",
      "Loss after iteration 1576: 0.526042\n",
      "Loss after iteration 1577: 0.526017\n",
      "Loss after iteration 1578: 0.525991\n",
      "Loss after iteration 1579: 0.525966\n",
      "Loss after iteration 1580: 0.525941\n",
      "Loss after iteration 1581: 0.525916\n",
      "Loss after iteration 1582: 0.525891\n",
      "Loss after iteration 1583: 0.525866\n",
      "Loss after iteration 1584: 0.525841\n",
      "Loss after iteration 1585: 0.525816\n",
      "Loss after iteration 1586: 0.525791\n",
      "Loss after iteration 1587: 0.525766\n",
      "Loss after iteration 1588: 0.525741\n",
      "Loss after iteration 1589: 0.525716\n",
      "Loss after iteration 1590: 0.525691\n",
      "Loss after iteration 1591: 0.525666\n",
      "Loss after iteration 1592: 0.525641\n",
      "Loss after iteration 1593: 0.525615\n",
      "Loss after iteration 1594: 0.525590\n",
      "Loss after iteration 1595: 0.525565\n",
      "Loss after iteration 1596: 0.525540\n",
      "Loss after iteration 1597: 0.525515\n",
      "Loss after iteration 1598: 0.525490\n",
      "Loss after iteration 1599: 0.525465\n",
      "Loss after iteration 1600: 0.525440\n",
      "Loss after iteration 1601: 0.525415\n",
      "Loss after iteration 1602: 0.525390\n",
      "Loss after iteration 1603: 0.525365\n",
      "Loss after iteration 1604: 0.525340\n",
      "Loss after iteration 1605: 0.525315\n",
      "Loss after iteration 1606: 0.525290\n",
      "Loss after iteration 1607: 0.525265\n",
      "Loss after iteration 1608: 0.525240\n",
      "Loss after iteration 1609: 0.525215\n",
      "Loss after iteration 1610: 0.525191\n",
      "Loss after iteration 1611: 0.525166\n",
      "Loss after iteration 1612: 0.525141\n",
      "Loss after iteration 1613: 0.525116\n",
      "Loss after iteration 1614: 0.525091\n",
      "Loss after iteration 1615: 0.525066\n",
      "Loss after iteration 1616: 0.525041\n",
      "Loss after iteration 1617: 0.525016\n",
      "Loss after iteration 1618: 0.524991\n",
      "Loss after iteration 1619: 0.524966\n",
      "Loss after iteration 1620: 0.524941\n",
      "Loss after iteration 1621: 0.524916\n",
      "Loss after iteration 1622: 0.524891\n",
      "Loss after iteration 1623: 0.524866\n",
      "Loss after iteration 1624: 0.524841\n",
      "Loss after iteration 1625: 0.524817\n",
      "Loss after iteration 1626: 0.524792\n",
      "Loss after iteration 1627: 0.524767\n",
      "Loss after iteration 1628: 0.524742\n",
      "Loss after iteration 1629: 0.524717\n",
      "Loss after iteration 1630: 0.524692\n",
      "Loss after iteration 1631: 0.524667\n",
      "Loss after iteration 1632: 0.524642\n",
      "Loss after iteration 1633: 0.524617\n",
      "Loss after iteration 1634: 0.524593\n",
      "Loss after iteration 1635: 0.524568\n",
      "Loss after iteration 1636: 0.524543\n",
      "Loss after iteration 1637: 0.524518\n",
      "Loss after iteration 1638: 0.524493\n",
      "Loss after iteration 1639: 0.524468\n",
      "Loss after iteration 1640: 0.524443\n",
      "Loss after iteration 1641: 0.524419\n",
      "Loss after iteration 1642: 0.524394\n",
      "Loss after iteration 1643: 0.524369\n",
      "Loss after iteration 1644: 0.524344\n",
      "Loss after iteration 1645: 0.524319\n",
      "Loss after iteration 1646: 0.524294\n",
      "Loss after iteration 1647: 0.524270\n",
      "Loss after iteration 1648: 0.524245\n",
      "Loss after iteration 1649: 0.524220\n",
      "Loss after iteration 1650: 0.524195\n",
      "Loss after iteration 1651: 0.524170\n",
      "Loss after iteration 1652: 0.524145\n",
      "Loss after iteration 1653: 0.524121\n",
      "Loss after iteration 1654: 0.524096\n",
      "Loss after iteration 1655: 0.524071\n",
      "Loss after iteration 1656: 0.524046\n",
      "Loss after iteration 1657: 0.524021\n",
      "Loss after iteration 1658: 0.523997\n",
      "Loss after iteration 1659: 0.523972\n",
      "Loss after iteration 1660: 0.523947\n",
      "Loss after iteration 1661: 0.523922\n",
      "Loss after iteration 1662: 0.523898\n",
      "Loss after iteration 1663: 0.523873\n",
      "Loss after iteration 1664: 0.523848\n",
      "Loss after iteration 1665: 0.523823\n",
      "Loss after iteration 1666: 0.523799\n",
      "Loss after iteration 1667: 0.523774\n",
      "Loss after iteration 1668: 0.523749\n",
      "Loss after iteration 1669: 0.523724\n",
      "Loss after iteration 1670: 0.523700\n",
      "Loss after iteration 1671: 0.523675\n",
      "Loss after iteration 1672: 0.523650\n",
      "Loss after iteration 1673: 0.523625\n",
      "Loss after iteration 1674: 0.523601\n",
      "Loss after iteration 1675: 0.523576\n",
      "Loss after iteration 1676: 0.523551\n",
      "Loss after iteration 1677: 0.523526\n",
      "Loss after iteration 1678: 0.523502\n",
      "Loss after iteration 1679: 0.523477\n",
      "Loss after iteration 1680: 0.523452\n",
      "Loss after iteration 1681: 0.523428\n",
      "Loss after iteration 1682: 0.523403\n",
      "Loss after iteration 1683: 0.523378\n",
      "Loss after iteration 1684: 0.523353\n",
      "Loss after iteration 1685: 0.523329\n",
      "Loss after iteration 1686: 0.523304\n",
      "Loss after iteration 1687: 0.523279\n",
      "Loss after iteration 1688: 0.523255\n",
      "Loss after iteration 1689: 0.523230\n",
      "Loss after iteration 1690: 0.523205\n",
      "Loss after iteration 1691: 0.523181\n",
      "Loss after iteration 1692: 0.523156\n",
      "Loss after iteration 1693: 0.523131\n",
      "Loss after iteration 1694: 0.523107\n",
      "Loss after iteration 1695: 0.523082\n",
      "Loss after iteration 1696: 0.523057\n",
      "Loss after iteration 1697: 0.523033\n",
      "Loss after iteration 1698: 0.523008\n",
      "Loss after iteration 1699: 0.522983\n",
      "Loss after iteration 1700: 0.522959\n",
      "Loss after iteration 1701: 0.522934\n",
      "Loss after iteration 1702: 0.522909\n",
      "Loss after iteration 1703: 0.522885\n",
      "Loss after iteration 1704: 0.522860\n",
      "Loss after iteration 1705: 0.522835\n",
      "Loss after iteration 1706: 0.522811\n",
      "Loss after iteration 1707: 0.522786\n",
      "Loss after iteration 1708: 0.522762\n",
      "Loss after iteration 1709: 0.522737\n",
      "Loss after iteration 1710: 0.522712\n",
      "Loss after iteration 1711: 0.522688\n",
      "Loss after iteration 1712: 0.522663\n",
      "Loss after iteration 1713: 0.522638\n",
      "Loss after iteration 1714: 0.522614\n",
      "Loss after iteration 1715: 0.522589\n",
      "Loss after iteration 1716: 0.522565\n",
      "Loss after iteration 1717: 0.522540\n",
      "Loss after iteration 1718: 0.522516\n",
      "Loss after iteration 1719: 0.522491\n",
      "Loss after iteration 1720: 0.522466\n",
      "Loss after iteration 1721: 0.522442\n",
      "Loss after iteration 1722: 0.522417\n",
      "Loss after iteration 1723: 0.522393\n",
      "Loss after iteration 1724: 0.522368\n",
      "Loss after iteration 1725: 0.522343\n",
      "Loss after iteration 1726: 0.522319\n",
      "Loss after iteration 1727: 0.522294\n",
      "Loss after iteration 1728: 0.522270\n",
      "Loss after iteration 1729: 0.522245\n",
      "Loss after iteration 1730: 0.522221\n",
      "Loss after iteration 1731: 0.522196\n",
      "Loss after iteration 1732: 0.522172\n",
      "Loss after iteration 1733: 0.522147\n",
      "Loss after iteration 1734: 0.522122\n",
      "Loss after iteration 1735: 0.522098\n",
      "Loss after iteration 1736: 0.522073\n",
      "Loss after iteration 1737: 0.522049\n",
      "Loss after iteration 1738: 0.522024\n",
      "Loss after iteration 1739: 0.522000\n",
      "Loss after iteration 1740: 0.521975\n",
      "Loss after iteration 1741: 0.521951\n",
      "Loss after iteration 1742: 0.521926\n",
      "Loss after iteration 1743: 0.521902\n",
      "Loss after iteration 1744: 0.521877\n",
      "Loss after iteration 1745: 0.521853\n",
      "Loss after iteration 1746: 0.521828\n",
      "Loss after iteration 1747: 0.521804\n",
      "Loss after iteration 1748: 0.521779\n",
      "Loss after iteration 1749: 0.521755\n",
      "Loss after iteration 1750: 0.521730\n",
      "Loss after iteration 1751: 0.521706\n",
      "Loss after iteration 1752: 0.521681\n",
      "Loss after iteration 1753: 0.521657\n",
      "Loss after iteration 1754: 0.521632\n",
      "Loss after iteration 1755: 0.521608\n",
      "Loss after iteration 1756: 0.521583\n",
      "Loss after iteration 1757: 0.521559\n",
      "Loss after iteration 1758: 0.521534\n",
      "Loss after iteration 1759: 0.521510\n",
      "Loss after iteration 1760: 0.521485\n",
      "Loss after iteration 1761: 0.521461\n",
      "Loss after iteration 1762: 0.521436\n",
      "Loss after iteration 1763: 0.521412\n",
      "Loss after iteration 1764: 0.521388\n",
      "Loss after iteration 1765: 0.521363\n",
      "Loss after iteration 1766: 0.521339\n",
      "Loss after iteration 1767: 0.521314\n",
      "Loss after iteration 1768: 0.521290\n",
      "Loss after iteration 1769: 0.521265\n",
      "Loss after iteration 1770: 0.521241\n",
      "Loss after iteration 1771: 0.521216\n",
      "Loss after iteration 1772: 0.521192\n",
      "Loss after iteration 1773: 0.521168\n",
      "Loss after iteration 1774: 0.521143\n",
      "Loss after iteration 1775: 0.521119\n",
      "Loss after iteration 1776: 0.521094\n",
      "Loss after iteration 1777: 0.521070\n",
      "Loss after iteration 1778: 0.521046\n",
      "Loss after iteration 1779: 0.521021\n",
      "Loss after iteration 1780: 0.520997\n",
      "Loss after iteration 1781: 0.520972\n",
      "Loss after iteration 1782: 0.520948\n",
      "Loss after iteration 1783: 0.520924\n",
      "Loss after iteration 1784: 0.520899\n",
      "Loss after iteration 1785: 0.520875\n",
      "Loss after iteration 1786: 0.520850\n",
      "Loss after iteration 1787: 0.520826\n",
      "Loss after iteration 1788: 0.520802\n",
      "Loss after iteration 1789: 0.520777\n",
      "Loss after iteration 1790: 0.520753\n",
      "Loss after iteration 1791: 0.520728\n",
      "Loss after iteration 1792: 0.520704\n",
      "Loss after iteration 1793: 0.520680\n",
      "Loss after iteration 1794: 0.520655\n",
      "Loss after iteration 1795: 0.520631\n",
      "Loss after iteration 1796: 0.520607\n",
      "Loss after iteration 1797: 0.520582\n",
      "Loss after iteration 1798: 0.520558\n",
      "Loss after iteration 1799: 0.520534\n",
      "Loss after iteration 1800: 0.520509\n",
      "Loss after iteration 1801: 0.520485\n",
      "Loss after iteration 1802: 0.520461\n",
      "Loss after iteration 1803: 0.520436\n",
      "Loss after iteration 1804: 0.520412\n",
      "Loss after iteration 1805: 0.520388\n",
      "Loss after iteration 1806: 0.520363\n",
      "Loss after iteration 1807: 0.520339\n",
      "Loss after iteration 1808: 0.520315\n",
      "Loss after iteration 1809: 0.520290\n",
      "Loss after iteration 1810: 0.520266\n",
      "Loss after iteration 1811: 0.520242\n",
      "Loss after iteration 1812: 0.520217\n",
      "Loss after iteration 1813: 0.520193\n",
      "Loss after iteration 1814: 0.520169\n",
      "Loss after iteration 1815: 0.520145\n",
      "Loss after iteration 1816: 0.520120\n",
      "Loss after iteration 1817: 0.520096\n",
      "Loss after iteration 1818: 0.520072\n",
      "Loss after iteration 1819: 0.520047\n",
      "Loss after iteration 1820: 0.520023\n",
      "Loss after iteration 1821: 0.519999\n",
      "Loss after iteration 1822: 0.519975\n",
      "Loss after iteration 1823: 0.519950\n",
      "Loss after iteration 1824: 0.519926\n",
      "Loss after iteration 1825: 0.519902\n",
      "Loss after iteration 1826: 0.519878\n",
      "Loss after iteration 1827: 0.519853\n",
      "Loss after iteration 1828: 0.519829\n",
      "Loss after iteration 1829: 0.519805\n",
      "Loss after iteration 1830: 0.519781\n",
      "Loss after iteration 1831: 0.519756\n",
      "Loss after iteration 1832: 0.519732\n",
      "Loss after iteration 1833: 0.519708\n",
      "Loss after iteration 1834: 0.519684\n",
      "Loss after iteration 1835: 0.519659\n",
      "Loss after iteration 1836: 0.519635\n",
      "Loss after iteration 1837: 0.519611\n",
      "Loss after iteration 1838: 0.519587\n",
      "Loss after iteration 1839: 0.519563\n",
      "Loss after iteration 1840: 0.519538\n",
      "Loss after iteration 1841: 0.519514\n",
      "Loss after iteration 1842: 0.519490\n",
      "Loss after iteration 1843: 0.519466\n",
      "Loss after iteration 1844: 0.519442\n",
      "Loss after iteration 1845: 0.519417\n",
      "Loss after iteration 1846: 0.519393\n",
      "Loss after iteration 1847: 0.519369\n",
      "Loss after iteration 1848: 0.519345\n",
      "Loss after iteration 1849: 0.519321\n",
      "Loss after iteration 1850: 0.519296\n",
      "Loss after iteration 1851: 0.519272\n",
      "Loss after iteration 1852: 0.519248\n",
      "Loss after iteration 1853: 0.519224\n",
      "Loss after iteration 1854: 0.519200\n",
      "Loss after iteration 1855: 0.519176\n",
      "Loss after iteration 1856: 0.519151\n",
      "Loss after iteration 1857: 0.519127\n",
      "Loss after iteration 1858: 0.519103\n",
      "Loss after iteration 1859: 0.519079\n",
      "Loss after iteration 1860: 0.519055\n",
      "Loss after iteration 1861: 0.519031\n",
      "Loss after iteration 1862: 0.519007\n",
      "Loss after iteration 1863: 0.518982\n",
      "Loss after iteration 1864: 0.518958\n",
      "Loss after iteration 1865: 0.518934\n",
      "Loss after iteration 1866: 0.518910\n",
      "Loss after iteration 1867: 0.518886\n",
      "Loss after iteration 1868: 0.518862\n",
      "Loss after iteration 1869: 0.518838\n",
      "Loss after iteration 1870: 0.518814\n",
      "Loss after iteration 1871: 0.518790\n",
      "Loss after iteration 1872: 0.518765\n",
      "Loss after iteration 1873: 0.518741\n",
      "Loss after iteration 1874: 0.518717\n",
      "Loss after iteration 1875: 0.518693\n",
      "Loss after iteration 1876: 0.518669\n",
      "Loss after iteration 1877: 0.518645\n",
      "Loss after iteration 1878: 0.518621\n",
      "Loss after iteration 1879: 0.518597\n",
      "Loss after iteration 1880: 0.518573\n",
      "Loss after iteration 1881: 0.518549\n",
      "Loss after iteration 1882: 0.518525\n",
      "Loss after iteration 1883: 0.518501\n",
      "Loss after iteration 1884: 0.518477\n",
      "Loss after iteration 1885: 0.518452\n",
      "Loss after iteration 1886: 0.518428\n",
      "Loss after iteration 1887: 0.518404\n",
      "Loss after iteration 1888: 0.518380\n",
      "Loss after iteration 1889: 0.518356\n",
      "Loss after iteration 1890: 0.518332\n",
      "Loss after iteration 1891: 0.518308\n",
      "Loss after iteration 1892: 0.518284\n",
      "Loss after iteration 1893: 0.518260\n",
      "Loss after iteration 1894: 0.518236\n",
      "Loss after iteration 1895: 0.518212\n",
      "Loss after iteration 1896: 0.518188\n",
      "Loss after iteration 1897: 0.518164\n",
      "Loss after iteration 1898: 0.518140\n",
      "Loss after iteration 1899: 0.518116\n",
      "Loss after iteration 1900: 0.518092\n",
      "Loss after iteration 1901: 0.518068\n",
      "Loss after iteration 1902: 0.518044\n",
      "Loss after iteration 1903: 0.518020\n",
      "Loss after iteration 1904: 0.517996\n",
      "Loss after iteration 1905: 0.517972\n",
      "Loss after iteration 1906: 0.517948\n",
      "Loss after iteration 1907: 0.517924\n",
      "Loss after iteration 1908: 0.517900\n",
      "Loss after iteration 1909: 0.517876\n",
      "Loss after iteration 1910: 0.517852\n",
      "Loss after iteration 1911: 0.517828\n",
      "Loss after iteration 1912: 0.517805\n",
      "Loss after iteration 1913: 0.517781\n",
      "Loss after iteration 1914: 0.517757\n",
      "Loss after iteration 1915: 0.517733\n",
      "Loss after iteration 1916: 0.517709\n",
      "Loss after iteration 1917: 0.517685\n",
      "Loss after iteration 1918: 0.517661\n",
      "Loss after iteration 1919: 0.517637\n",
      "Loss after iteration 1920: 0.517613\n",
      "Loss after iteration 1921: 0.517589\n",
      "Loss after iteration 1922: 0.517565\n",
      "Loss after iteration 1923: 0.517541\n",
      "Loss after iteration 1924: 0.517517\n",
      "Loss after iteration 1925: 0.517494\n",
      "Loss after iteration 1926: 0.517470\n",
      "Loss after iteration 1927: 0.517446\n",
      "Loss after iteration 1928: 0.517422\n",
      "Loss after iteration 1929: 0.517398\n",
      "Loss after iteration 1930: 0.517374\n",
      "Loss after iteration 1931: 0.517350\n",
      "Loss after iteration 1932: 0.517326\n",
      "Loss after iteration 1933: 0.517303\n",
      "Loss after iteration 1934: 0.517279\n",
      "Loss after iteration 1935: 0.517255\n",
      "Loss after iteration 1936: 0.517231\n",
      "Loss after iteration 1937: 0.517207\n",
      "Loss after iteration 1938: 0.517183\n",
      "Loss after iteration 1939: 0.517159\n",
      "Loss after iteration 1940: 0.517136\n",
      "Loss after iteration 1941: 0.517112\n",
      "Loss after iteration 1942: 0.517088\n",
      "Loss after iteration 1943: 0.517064\n",
      "Loss after iteration 1944: 0.517040\n",
      "Loss after iteration 1945: 0.517017\n",
      "Loss after iteration 1946: 0.516993\n",
      "Loss after iteration 1947: 0.516969\n",
      "Loss after iteration 1948: 0.516945\n",
      "Loss after iteration 1949: 0.516921\n",
      "Loss after iteration 1950: 0.516898\n",
      "Loss after iteration 1951: 0.516874\n",
      "Loss after iteration 1952: 0.516850\n",
      "Loss after iteration 1953: 0.516826\n",
      "Loss after iteration 1954: 0.516802\n",
      "Loss after iteration 1955: 0.516779\n",
      "Loss after iteration 1956: 0.516755\n",
      "Loss after iteration 1957: 0.516731\n",
      "Loss after iteration 1958: 0.516707\n",
      "Loss after iteration 1959: 0.516684\n",
      "Loss after iteration 1960: 0.516660\n",
      "Loss after iteration 1961: 0.516636\n",
      "Loss after iteration 1962: 0.516612\n",
      "Loss after iteration 1963: 0.516589\n",
      "Loss after iteration 1964: 0.516565\n",
      "Loss after iteration 1965: 0.516541\n",
      "Loss after iteration 1966: 0.516517\n",
      "Loss after iteration 1967: 0.516494\n",
      "Loss after iteration 1968: 0.516470\n",
      "Loss after iteration 1969: 0.516446\n",
      "Loss after iteration 1970: 0.516423\n",
      "Loss after iteration 1971: 0.516399\n",
      "Loss after iteration 1972: 0.516375\n",
      "Loss after iteration 1973: 0.516352\n",
      "Loss after iteration 1974: 0.516328\n",
      "Loss after iteration 1975: 0.516304\n",
      "Loss after iteration 1976: 0.516281\n",
      "Loss after iteration 1977: 0.516257\n",
      "Loss after iteration 1978: 0.516233\n",
      "Loss after iteration 1979: 0.516210\n",
      "Loss after iteration 1980: 0.516186\n",
      "Loss after iteration 1981: 0.516162\n",
      "Loss after iteration 1982: 0.516139\n",
      "Loss after iteration 1983: 0.516115\n",
      "Loss after iteration 1984: 0.516091\n",
      "Loss after iteration 1985: 0.516068\n",
      "Loss after iteration 1986: 0.516044\n",
      "Loss after iteration 1987: 0.516020\n",
      "Loss after iteration 1988: 0.515997\n",
      "Loss after iteration 1989: 0.515973\n",
      "Loss after iteration 1990: 0.515950\n",
      "Loss after iteration 1991: 0.515926\n",
      "Loss after iteration 1992: 0.515902\n",
      "Loss after iteration 1993: 0.515879\n",
      "Loss after iteration 1994: 0.515855\n",
      "Loss after iteration 1995: 0.515832\n",
      "Loss after iteration 1996: 0.515808\n",
      "Loss after iteration 1997: 0.515784\n",
      "Loss after iteration 1998: 0.515761\n",
      "Loss after iteration 1999: 0.515737\n",
      "Loss after iteration 2000: 0.515714\n",
      "Loss after iteration 2001: 0.515690\n",
      "Loss after iteration 2002: 0.515667\n",
      "Loss after iteration 2003: 0.515643\n",
      "Loss after iteration 2004: 0.515620\n",
      "Loss after iteration 2005: 0.515596\n",
      "Loss after iteration 2006: 0.515572\n",
      "Loss after iteration 2007: 0.515549\n",
      "Loss after iteration 2008: 0.515525\n",
      "Loss after iteration 2009: 0.515502\n",
      "Loss after iteration 2010: 0.515478\n",
      "Loss after iteration 2011: 0.515455\n",
      "Loss after iteration 2012: 0.515431\n",
      "Loss after iteration 2013: 0.515408\n",
      "Loss after iteration 2014: 0.515384\n",
      "Loss after iteration 2015: 0.515361\n",
      "Loss after iteration 2016: 0.515337\n",
      "Loss after iteration 2017: 0.515314\n",
      "Loss after iteration 2018: 0.515290\n",
      "Loss after iteration 2019: 0.515267\n",
      "Loss after iteration 2020: 0.515244\n",
      "Loss after iteration 2021: 0.515220\n",
      "Loss after iteration 2022: 0.515197\n",
      "Loss after iteration 2023: 0.515173\n",
      "Loss after iteration 2024: 0.515150\n",
      "Loss after iteration 2025: 0.515126\n",
      "Loss after iteration 2026: 0.515103\n",
      "Loss after iteration 2027: 0.515079\n",
      "Loss after iteration 2028: 0.515056\n",
      "Loss after iteration 2029: 0.515033\n",
      "Loss after iteration 2030: 0.515009\n",
      "Loss after iteration 2031: 0.514986\n",
      "Loss after iteration 2032: 0.514962\n",
      "Loss after iteration 2033: 0.514939\n",
      "Loss after iteration 2034: 0.514916\n",
      "Loss after iteration 2035: 0.514892\n",
      "Loss after iteration 2036: 0.514869\n",
      "Loss after iteration 2037: 0.514845\n",
      "Loss after iteration 2038: 0.514822\n",
      "Loss after iteration 2039: 0.514799\n",
      "Loss after iteration 2040: 0.514775\n",
      "Loss after iteration 2041: 0.514752\n",
      "Loss after iteration 2042: 0.514729\n",
      "Loss after iteration 2043: 0.514705\n",
      "Loss after iteration 2044: 0.514682\n",
      "Loss after iteration 2045: 0.514659\n",
      "Loss after iteration 2046: 0.514635\n",
      "Loss after iteration 2047: 0.514612\n",
      "Loss after iteration 2048: 0.514589\n",
      "Loss after iteration 2049: 0.514565\n",
      "Loss after iteration 2050: 0.514542\n",
      "Loss after iteration 2051: 0.514519\n",
      "Loss after iteration 2052: 0.514495\n",
      "Loss after iteration 2053: 0.514472\n",
      "Loss after iteration 2054: 0.514449\n",
      "Loss after iteration 2055: 0.514426\n",
      "Loss after iteration 2056: 0.514402\n",
      "Loss after iteration 2057: 0.514379\n",
      "Loss after iteration 2058: 0.514356\n",
      "Loss after iteration 2059: 0.514333\n",
      "Loss after iteration 2060: 0.514309\n",
      "Loss after iteration 2061: 0.514286\n",
      "Loss after iteration 2062: 0.514263\n",
      "Loss after iteration 2063: 0.514240\n",
      "Loss after iteration 2064: 0.514216\n",
      "Loss after iteration 2065: 0.514193\n",
      "Loss after iteration 2066: 0.514170\n",
      "Loss after iteration 2067: 0.514147\n",
      "Loss after iteration 2068: 0.514123\n",
      "Loss after iteration 2069: 0.514100\n",
      "Loss after iteration 2070: 0.514077\n",
      "Loss after iteration 2071: 0.514054\n",
      "Loss after iteration 2072: 0.514031\n",
      "Loss after iteration 2073: 0.514008\n",
      "Loss after iteration 2074: 0.513984\n",
      "Loss after iteration 2075: 0.513961\n",
      "Loss after iteration 2076: 0.513938\n",
      "Loss after iteration 2077: 0.513915\n",
      "Loss after iteration 2078: 0.513892\n",
      "Loss after iteration 2079: 0.513869\n",
      "Loss after iteration 2080: 0.513845\n",
      "Loss after iteration 2081: 0.513822\n",
      "Loss after iteration 2082: 0.513799\n",
      "Loss after iteration 2083: 0.513776\n",
      "Loss after iteration 2084: 0.513753\n",
      "Loss after iteration 2085: 0.513730\n",
      "Loss after iteration 2086: 0.513707\n",
      "Loss after iteration 2087: 0.513684\n",
      "Loss after iteration 2088: 0.513660\n",
      "Loss after iteration 2089: 0.513637\n",
      "Loss after iteration 2090: 0.513614\n",
      "Loss after iteration 2091: 0.513591\n",
      "Loss after iteration 2092: 0.513568\n",
      "Loss after iteration 2093: 0.513545\n",
      "Loss after iteration 2094: 0.513522\n",
      "Loss after iteration 2095: 0.513499\n",
      "Loss after iteration 2096: 0.513476\n",
      "Loss after iteration 2097: 0.513453\n",
      "Loss after iteration 2098: 0.513430\n",
      "Loss after iteration 2099: 0.513407\n",
      "Loss after iteration 2100: 0.513384\n",
      "Loss after iteration 2101: 0.513361\n",
      "Loss after iteration 2102: 0.513338\n",
      "Loss after iteration 2103: 0.513315\n",
      "Loss after iteration 2104: 0.513292\n",
      "Loss after iteration 2105: 0.513269\n",
      "Loss after iteration 2106: 0.513246\n",
      "Loss after iteration 2107: 0.513223\n",
      "Loss after iteration 2108: 0.513200\n",
      "Loss after iteration 2109: 0.513177\n",
      "Loss after iteration 2110: 0.513154\n",
      "Loss after iteration 2111: 0.513131\n",
      "Loss after iteration 2112: 0.513108\n",
      "Loss after iteration 2113: 0.513085\n",
      "Loss after iteration 2114: 0.513062\n",
      "Loss after iteration 2115: 0.513039\n",
      "Loss after iteration 2116: 0.513016\n",
      "Loss after iteration 2117: 0.512993\n",
      "Loss after iteration 2118: 0.512970\n",
      "Loss after iteration 2119: 0.512947\n",
      "Loss after iteration 2120: 0.512924\n",
      "Loss after iteration 2121: 0.512902\n",
      "Loss after iteration 2122: 0.512879\n",
      "Loss after iteration 2123: 0.512856\n",
      "Loss after iteration 2124: 0.512833\n",
      "Loss after iteration 2125: 0.512810\n",
      "Loss after iteration 2126: 0.512787\n",
      "Loss after iteration 2127: 0.512764\n",
      "Loss after iteration 2128: 0.512741\n",
      "Loss after iteration 2129: 0.512719\n",
      "Loss after iteration 2130: 0.512696\n",
      "Loss after iteration 2131: 0.512673\n",
      "Loss after iteration 2132: 0.512650\n",
      "Loss after iteration 2133: 0.512627\n",
      "Loss after iteration 2134: 0.512604\n",
      "Loss after iteration 2135: 0.512582\n",
      "Loss after iteration 2136: 0.512559\n",
      "Loss after iteration 2137: 0.512536\n",
      "Loss after iteration 2138: 0.512513\n",
      "Loss after iteration 2139: 0.512490\n",
      "Loss after iteration 2140: 0.512468\n",
      "Loss after iteration 2141: 0.512445\n",
      "Loss after iteration 2142: 0.512422\n",
      "Loss after iteration 2143: 0.512399\n",
      "Loss after iteration 2144: 0.512376\n",
      "Loss after iteration 2145: 0.512354\n",
      "Loss after iteration 2146: 0.512331\n",
      "Loss after iteration 2147: 0.512308\n",
      "Loss after iteration 2148: 0.512285\n",
      "Loss after iteration 2149: 0.512263\n",
      "Loss after iteration 2150: 0.512240\n",
      "Loss after iteration 2151: 0.512217\n",
      "Loss after iteration 2152: 0.512195\n",
      "Loss after iteration 2153: 0.512172\n",
      "Loss after iteration 2154: 0.512149\n",
      "Loss after iteration 2155: 0.512126\n",
      "Loss after iteration 2156: 0.512104\n",
      "Loss after iteration 2157: 0.512081\n",
      "Loss after iteration 2158: 0.512058\n",
      "Loss after iteration 2159: 0.512036\n",
      "Loss after iteration 2160: 0.512013\n",
      "Loss after iteration 2161: 0.511990\n",
      "Loss after iteration 2162: 0.511968\n",
      "Loss after iteration 2163: 0.511945\n",
      "Loss after iteration 2164: 0.511922\n",
      "Loss after iteration 2165: 0.511900\n",
      "Loss after iteration 2166: 0.511877\n",
      "Loss after iteration 2167: 0.511855\n",
      "Loss after iteration 2168: 0.511832\n",
      "Loss after iteration 2169: 0.511809\n",
      "Loss after iteration 2170: 0.511787\n",
      "Loss after iteration 2171: 0.511764\n",
      "Loss after iteration 2172: 0.511742\n",
      "Loss after iteration 2173: 0.511719\n",
      "Loss after iteration 2174: 0.511696\n",
      "Loss after iteration 2175: 0.511674\n",
      "Loss after iteration 2176: 0.511651\n",
      "Loss after iteration 2177: 0.511629\n",
      "Loss after iteration 2178: 0.511606\n",
      "Loss after iteration 2179: 0.511584\n",
      "Loss after iteration 2180: 0.511561\n",
      "Loss after iteration 2181: 0.511539\n",
      "Loss after iteration 2182: 0.511516\n",
      "Loss after iteration 2183: 0.511494\n",
      "Loss after iteration 2184: 0.511471\n",
      "Loss after iteration 2185: 0.511449\n",
      "Loss after iteration 2186: 0.511426\n",
      "Loss after iteration 2187: 0.511404\n",
      "Loss after iteration 2188: 0.511381\n",
      "Loss after iteration 2189: 0.511359\n",
      "Loss after iteration 2190: 0.511336\n",
      "Loss after iteration 2191: 0.511314\n",
      "Loss after iteration 2192: 0.511291\n",
      "Loss after iteration 2193: 0.511269\n",
      "Loss after iteration 2194: 0.511246\n",
      "Loss after iteration 2195: 0.511224\n",
      "Loss after iteration 2196: 0.511201\n",
      "Loss after iteration 2197: 0.511179\n",
      "Loss after iteration 2198: 0.511157\n",
      "Loss after iteration 2199: 0.511134\n",
      "Loss after iteration 2200: 0.511112\n",
      "Loss after iteration 2201: 0.511089\n",
      "Loss after iteration 2202: 0.511067\n",
      "Loss after iteration 2203: 0.511045\n",
      "Loss after iteration 2204: 0.511022\n",
      "Loss after iteration 2205: 0.511000\n",
      "Loss after iteration 2206: 0.510977\n",
      "Loss after iteration 2207: 0.510955\n",
      "Loss after iteration 2208: 0.510933\n",
      "Loss after iteration 2209: 0.510910\n",
      "Loss after iteration 2210: 0.510888\n",
      "Loss after iteration 2211: 0.510866\n",
      "Loss after iteration 2212: 0.510843\n",
      "Loss after iteration 2213: 0.510821\n",
      "Loss after iteration 2214: 0.510799\n",
      "Loss after iteration 2215: 0.510776\n",
      "Loss after iteration 2216: 0.510754\n",
      "Loss after iteration 2217: 0.510732\n",
      "Loss after iteration 2218: 0.510710\n",
      "Loss after iteration 2219: 0.510687\n",
      "Loss after iteration 2220: 0.510665\n",
      "Loss after iteration 2221: 0.510643\n",
      "Loss after iteration 2222: 0.510620\n",
      "Loss after iteration 2223: 0.510598\n",
      "Loss after iteration 2224: 0.510576\n",
      "Loss after iteration 2225: 0.510554\n",
      "Loss after iteration 2226: 0.510531\n",
      "Loss after iteration 2227: 0.510509\n",
      "Loss after iteration 2228: 0.510487\n",
      "Loss after iteration 2229: 0.510465\n",
      "Loss after iteration 2230: 0.510443\n",
      "Loss after iteration 2231: 0.510420\n",
      "Loss after iteration 2232: 0.510398\n",
      "Loss after iteration 2233: 0.510376\n",
      "Loss after iteration 2234: 0.510354\n",
      "Loss after iteration 2235: 0.510332\n",
      "Loss after iteration 2236: 0.510309\n",
      "Loss after iteration 2237: 0.510287\n",
      "Loss after iteration 2238: 0.510265\n",
      "Loss after iteration 2239: 0.510243\n",
      "Loss after iteration 2240: 0.510221\n",
      "Loss after iteration 2241: 0.510199\n",
      "Loss after iteration 2242: 0.510177\n",
      "Loss after iteration 2243: 0.510154\n",
      "Loss after iteration 2244: 0.510132\n",
      "Loss after iteration 2245: 0.510110\n",
      "Loss after iteration 2246: 0.510088\n",
      "Loss after iteration 2247: 0.510066\n",
      "Loss after iteration 2248: 0.510044\n",
      "Loss after iteration 2249: 0.510022\n",
      "Loss after iteration 2250: 0.510000\n",
      "Loss after iteration 2251: 0.509978\n",
      "Loss after iteration 2252: 0.509956\n",
      "Loss after iteration 2253: 0.509934\n",
      "Loss after iteration 2254: 0.509912\n",
      "Loss after iteration 2255: 0.509890\n",
      "Loss after iteration 2256: 0.509868\n",
      "Loss after iteration 2257: 0.509846\n",
      "Loss after iteration 2258: 0.509823\n",
      "Loss after iteration 2259: 0.509801\n",
      "Loss after iteration 2260: 0.509779\n",
      "Loss after iteration 2261: 0.509757\n",
      "Loss after iteration 2262: 0.509735\n",
      "Loss after iteration 2263: 0.509714\n",
      "Loss after iteration 2264: 0.509692\n",
      "Loss after iteration 2265: 0.509670\n",
      "Loss after iteration 2266: 0.509648\n",
      "Loss after iteration 2267: 0.509626\n",
      "Loss after iteration 2268: 0.509604\n",
      "Loss after iteration 2269: 0.509582\n",
      "Loss after iteration 2270: 0.509560\n",
      "Loss after iteration 2271: 0.509538\n",
      "Loss after iteration 2272: 0.509516\n",
      "Loss after iteration 2273: 0.509494\n",
      "Loss after iteration 2274: 0.509472\n",
      "Loss after iteration 2275: 0.509450\n",
      "Loss after iteration 2276: 0.509428\n",
      "Loss after iteration 2277: 0.509406\n",
      "Loss after iteration 2278: 0.509385\n",
      "Loss after iteration 2279: 0.509363\n",
      "Loss after iteration 2280: 0.509341\n",
      "Loss after iteration 2281: 0.509319\n",
      "Loss after iteration 2282: 0.509297\n",
      "Loss after iteration 2283: 0.509275\n",
      "Loss after iteration 2284: 0.509253\n",
      "Loss after iteration 2285: 0.509232\n",
      "Loss after iteration 2286: 0.509210\n",
      "Loss after iteration 2287: 0.509188\n",
      "Loss after iteration 2288: 0.509166\n",
      "Loss after iteration 2289: 0.509144\n",
      "Loss after iteration 2290: 0.509123\n",
      "Loss after iteration 2291: 0.509101\n",
      "Loss after iteration 2292: 0.509079\n",
      "Loss after iteration 2293: 0.509057\n",
      "Loss after iteration 2294: 0.509035\n",
      "Loss after iteration 2295: 0.509014\n",
      "Loss after iteration 2296: 0.508992\n",
      "Loss after iteration 2297: 0.508970\n",
      "Loss after iteration 2298: 0.508948\n",
      "Loss after iteration 2299: 0.508927\n",
      "Loss after iteration 2300: 0.508905\n",
      "Loss after iteration 2301: 0.508883\n",
      "Loss after iteration 2302: 0.508862\n",
      "Loss after iteration 2303: 0.508840\n",
      "Loss after iteration 2304: 0.508818\n",
      "Loss after iteration 2305: 0.508796\n",
      "Loss after iteration 2306: 0.508775\n",
      "Loss after iteration 2307: 0.508753\n",
      "Loss after iteration 2308: 0.508731\n",
      "Loss after iteration 2309: 0.508710\n",
      "Loss after iteration 2310: 0.508688\n",
      "Loss after iteration 2311: 0.508666\n",
      "Loss after iteration 2312: 0.508645\n",
      "Loss after iteration 2313: 0.508623\n",
      "Loss after iteration 2314: 0.508602\n",
      "Loss after iteration 2315: 0.508580\n",
      "Loss after iteration 2316: 0.508558\n",
      "Loss after iteration 2317: 0.508537\n",
      "Loss after iteration 2318: 0.508515\n",
      "Loss after iteration 2319: 0.508493\n",
      "Loss after iteration 2320: 0.508472\n",
      "Loss after iteration 2321: 0.508450\n",
      "Loss after iteration 2322: 0.508429\n",
      "Loss after iteration 2323: 0.508407\n",
      "Loss after iteration 2324: 0.508386\n",
      "Loss after iteration 2325: 0.508364\n",
      "Loss after iteration 2326: 0.508343\n",
      "Loss after iteration 2327: 0.508321\n",
      "Loss after iteration 2328: 0.508299\n",
      "Loss after iteration 2329: 0.508278\n",
      "Loss after iteration 2330: 0.508256\n",
      "Loss after iteration 2331: 0.508235\n",
      "Loss after iteration 2332: 0.508213\n",
      "Loss after iteration 2333: 0.508192\n",
      "Loss after iteration 2334: 0.508170\n",
      "Loss after iteration 2335: 0.508149\n",
      "Loss after iteration 2336: 0.508127\n",
      "Loss after iteration 2337: 0.508106\n",
      "Loss after iteration 2338: 0.508085\n",
      "Loss after iteration 2339: 0.508063\n",
      "Loss after iteration 2340: 0.508042\n",
      "Loss after iteration 2341: 0.508020\n",
      "Loss after iteration 2342: 0.507999\n",
      "Loss after iteration 2343: 0.507977\n",
      "Loss after iteration 2344: 0.507956\n",
      "Loss after iteration 2345: 0.507935\n",
      "Loss after iteration 2346: 0.507913\n",
      "Loss after iteration 2347: 0.507892\n",
      "Loss after iteration 2348: 0.507870\n",
      "Loss after iteration 2349: 0.507849\n",
      "Loss after iteration 2350: 0.507828\n",
      "Loss after iteration 2351: 0.507806\n",
      "Loss after iteration 2352: 0.507785\n",
      "Loss after iteration 2353: 0.507764\n",
      "Loss after iteration 2354: 0.507742\n",
      "Loss after iteration 2355: 0.507721\n",
      "Loss after iteration 2356: 0.507700\n",
      "Loss after iteration 2357: 0.507678\n",
      "Loss after iteration 2358: 0.507657\n",
      "Loss after iteration 2359: 0.507636\n",
      "Loss after iteration 2360: 0.507614\n",
      "Loss after iteration 2361: 0.507593\n",
      "Loss after iteration 2362: 0.507572\n",
      "Loss after iteration 2363: 0.507550\n",
      "Loss after iteration 2364: 0.507529\n",
      "Loss after iteration 2365: 0.507508\n",
      "Loss after iteration 2366: 0.507487\n",
      "Loss after iteration 2367: 0.507465\n",
      "Loss after iteration 2368: 0.507444\n",
      "Loss after iteration 2369: 0.507423\n",
      "Loss after iteration 2370: 0.507402\n",
      "Loss after iteration 2371: 0.507380\n",
      "Loss after iteration 2372: 0.507359\n",
      "Loss after iteration 2373: 0.507338\n",
      "Loss after iteration 2374: 0.507317\n",
      "Loss after iteration 2375: 0.507296\n",
      "Loss after iteration 2376: 0.507274\n",
      "Loss after iteration 2377: 0.507253\n",
      "Loss after iteration 2378: 0.507232\n",
      "Loss after iteration 2379: 0.507211\n",
      "Loss after iteration 2380: 0.507190\n",
      "Loss after iteration 2381: 0.507169\n",
      "Loss after iteration 2382: 0.507147\n",
      "Loss after iteration 2383: 0.507126\n",
      "Loss after iteration 2384: 0.507105\n",
      "Loss after iteration 2385: 0.507084\n",
      "Loss after iteration 2386: 0.507063\n",
      "Loss after iteration 2387: 0.507042\n",
      "Loss after iteration 2388: 0.507021\n",
      "Loss after iteration 2389: 0.507000\n",
      "Loss after iteration 2390: 0.506979\n",
      "Loss after iteration 2391: 0.506957\n",
      "Loss after iteration 2392: 0.506936\n",
      "Loss after iteration 2393: 0.506915\n",
      "Loss after iteration 2394: 0.506894\n",
      "Loss after iteration 2395: 0.506873\n",
      "Loss after iteration 2396: 0.506852\n",
      "Loss after iteration 2397: 0.506831\n",
      "Loss after iteration 2398: 0.506810\n",
      "Loss after iteration 2399: 0.506789\n",
      "Loss after iteration 2400: 0.506768\n",
      "Loss after iteration 2401: 0.506747\n",
      "Loss after iteration 2402: 0.506726\n",
      "Loss after iteration 2403: 0.506705\n",
      "Loss after iteration 2404: 0.506684\n",
      "Loss after iteration 2405: 0.506663\n",
      "Loss after iteration 2406: 0.506642\n",
      "Loss after iteration 2407: 0.506621\n",
      "Loss after iteration 2408: 0.506600\n",
      "Loss after iteration 2409: 0.506579\n",
      "Loss after iteration 2410: 0.506558\n",
      "Loss after iteration 2411: 0.506537\n",
      "Loss after iteration 2412: 0.506516\n",
      "Loss after iteration 2413: 0.506496\n",
      "Loss after iteration 2414: 0.506475\n",
      "Loss after iteration 2415: 0.506454\n",
      "Loss after iteration 2416: 0.506433\n",
      "Loss after iteration 2417: 0.506412\n",
      "Loss after iteration 2418: 0.506391\n",
      "Loss after iteration 2419: 0.506370\n",
      "Loss after iteration 2420: 0.506349\n",
      "Loss after iteration 2421: 0.506328\n",
      "Loss after iteration 2422: 0.506308\n",
      "Loss after iteration 2423: 0.506287\n",
      "Loss after iteration 2424: 0.506266\n",
      "Loss after iteration 2425: 0.506245\n",
      "Loss after iteration 2426: 0.506224\n",
      "Loss after iteration 2427: 0.506203\n",
      "Loss after iteration 2428: 0.506183\n",
      "Loss after iteration 2429: 0.506162\n",
      "Loss after iteration 2430: 0.506141\n",
      "Loss after iteration 2431: 0.506120\n",
      "Loss after iteration 2432: 0.506099\n",
      "Loss after iteration 2433: 0.506079\n",
      "Loss after iteration 2434: 0.506058\n",
      "Loss after iteration 2435: 0.506037\n",
      "Loss after iteration 2436: 0.506016\n",
      "Loss after iteration 2437: 0.505996\n",
      "Loss after iteration 2438: 0.505975\n",
      "Loss after iteration 2439: 0.505954\n",
      "Loss after iteration 2440: 0.505933\n",
      "Loss after iteration 2441: 0.505913\n",
      "Loss after iteration 2442: 0.505892\n",
      "Loss after iteration 2443: 0.505871\n",
      "Loss after iteration 2444: 0.505851\n",
      "Loss after iteration 2445: 0.505830\n",
      "Loss after iteration 2446: 0.505809\n",
      "Loss after iteration 2447: 0.505789\n",
      "Loss after iteration 2448: 0.505768\n",
      "Loss after iteration 2449: 0.505747\n",
      "Loss after iteration 2450: 0.505727\n",
      "Loss after iteration 2451: 0.505706\n",
      "Loss after iteration 2452: 0.505685\n",
      "Loss after iteration 2453: 0.505665\n",
      "Loss after iteration 2454: 0.505644\n",
      "Loss after iteration 2455: 0.505623\n",
      "Loss after iteration 2456: 0.505603\n",
      "Loss after iteration 2457: 0.505582\n",
      "Loss after iteration 2458: 0.505562\n",
      "Loss after iteration 2459: 0.505541\n",
      "Loss after iteration 2460: 0.505520\n",
      "Loss after iteration 2461: 0.505500\n",
      "Loss after iteration 2462: 0.505479\n",
      "Loss after iteration 2463: 0.505459\n",
      "Loss after iteration 2464: 0.505438\n",
      "Loss after iteration 2465: 0.505418\n",
      "Loss after iteration 2466: 0.505397\n",
      "Loss after iteration 2467: 0.505377\n",
      "Loss after iteration 2468: 0.505356\n",
      "Loss after iteration 2469: 0.505336\n",
      "Loss after iteration 2470: 0.505315\n",
      "Loss after iteration 2471: 0.505294\n",
      "Loss after iteration 2472: 0.505274\n",
      "Loss after iteration 2473: 0.505254\n",
      "Loss after iteration 2474: 0.505233\n",
      "Loss after iteration 2475: 0.505213\n",
      "Loss after iteration 2476: 0.505192\n",
      "Loss after iteration 2477: 0.505172\n",
      "Loss after iteration 2478: 0.505151\n",
      "Loss after iteration 2479: 0.505131\n",
      "Loss after iteration 2480: 0.505110\n",
      "Loss after iteration 2481: 0.505090\n",
      "Loss after iteration 2482: 0.505069\n",
      "Loss after iteration 2483: 0.505049\n",
      "Loss after iteration 2484: 0.505029\n",
      "Loss after iteration 2485: 0.505008\n",
      "Loss after iteration 2486: 0.504988\n",
      "Loss after iteration 2487: 0.504967\n",
      "Loss after iteration 2488: 0.504947\n",
      "Loss after iteration 2489: 0.504927\n",
      "Loss after iteration 2490: 0.504906\n",
      "Loss after iteration 2491: 0.504886\n",
      "Loss after iteration 2492: 0.504866\n",
      "Loss after iteration 2493: 0.504845\n",
      "Loss after iteration 2494: 0.504825\n",
      "Loss after iteration 2495: 0.504805\n",
      "Loss after iteration 2496: 0.504784\n",
      "Loss after iteration 2497: 0.504764\n",
      "Loss after iteration 2498: 0.504744\n",
      "Loss after iteration 2499: 0.504723\n",
      "Loss after iteration 2500: 0.504703\n",
      "Loss after iteration 2501: 0.504683\n",
      "Loss after iteration 2502: 0.504663\n",
      "Loss after iteration 2503: 0.504642\n",
      "Loss after iteration 2504: 0.504622\n",
      "Loss after iteration 2505: 0.504602\n",
      "Loss after iteration 2506: 0.504582\n",
      "Loss after iteration 2507: 0.504561\n",
      "Loss after iteration 2508: 0.504541\n",
      "Loss after iteration 2509: 0.504521\n",
      "Loss after iteration 2510: 0.504501\n",
      "Loss after iteration 2511: 0.504480\n",
      "Loss after iteration 2512: 0.504460\n",
      "Loss after iteration 2513: 0.504440\n",
      "Loss after iteration 2514: 0.504420\n",
      "Loss after iteration 2515: 0.504400\n",
      "Loss after iteration 2516: 0.504379\n",
      "Loss after iteration 2517: 0.504359\n",
      "Loss after iteration 2518: 0.504339\n",
      "Loss after iteration 2519: 0.504319\n",
      "Loss after iteration 2520: 0.504299\n",
      "Loss after iteration 2521: 0.504279\n",
      "Loss after iteration 2522: 0.504259\n",
      "Loss after iteration 2523: 0.504238\n",
      "Loss after iteration 2524: 0.504218\n",
      "Loss after iteration 2525: 0.504198\n",
      "Loss after iteration 2526: 0.504178\n",
      "Loss after iteration 2527: 0.504158\n",
      "Loss after iteration 2528: 0.504138\n",
      "Loss after iteration 2529: 0.504118\n",
      "Loss after iteration 2530: 0.504098\n",
      "Loss after iteration 2531: 0.504078\n",
      "Loss after iteration 2532: 0.504058\n",
      "Loss after iteration 2533: 0.504038\n",
      "Loss after iteration 2534: 0.504018\n",
      "Loss after iteration 2535: 0.503998\n",
      "Loss after iteration 2536: 0.503977\n",
      "Loss after iteration 2537: 0.503957\n",
      "Loss after iteration 2538: 0.503937\n",
      "Loss after iteration 2539: 0.503917\n",
      "Loss after iteration 2540: 0.503897\n",
      "Loss after iteration 2541: 0.503877\n",
      "Loss after iteration 2542: 0.503857\n",
      "Loss after iteration 2543: 0.503837\n",
      "Loss after iteration 2544: 0.503818\n",
      "Loss after iteration 2545: 0.503798\n",
      "Loss after iteration 2546: 0.503778\n",
      "Loss after iteration 2547: 0.503758\n",
      "Loss after iteration 2548: 0.503738\n",
      "Loss after iteration 2549: 0.503718\n",
      "Loss after iteration 2550: 0.503698\n",
      "Loss after iteration 2551: 0.503678\n",
      "Loss after iteration 2552: 0.503658\n",
      "Loss after iteration 2553: 0.503638\n",
      "Loss after iteration 2554: 0.503618\n",
      "Loss after iteration 2555: 0.503598\n",
      "Loss after iteration 2556: 0.503578\n",
      "Loss after iteration 2557: 0.503559\n",
      "Loss after iteration 2558: 0.503539\n",
      "Loss after iteration 2559: 0.503519\n",
      "Loss after iteration 2560: 0.503499\n",
      "Loss after iteration 2561: 0.503479\n",
      "Loss after iteration 2562: 0.503459\n",
      "Loss after iteration 2563: 0.503439\n",
      "Loss after iteration 2564: 0.503420\n",
      "Loss after iteration 2565: 0.503400\n",
      "Loss after iteration 2566: 0.503380\n",
      "Loss after iteration 2567: 0.503360\n",
      "Loss after iteration 2568: 0.503340\n",
      "Loss after iteration 2569: 0.503321\n",
      "Loss after iteration 2570: 0.503301\n",
      "Loss after iteration 2571: 0.503281\n",
      "Loss after iteration 2572: 0.503261\n",
      "Loss after iteration 2573: 0.503241\n",
      "Loss after iteration 2574: 0.503222\n",
      "Loss after iteration 2575: 0.503202\n",
      "Loss after iteration 2576: 0.503182\n",
      "Loss after iteration 2577: 0.503163\n",
      "Loss after iteration 2578: 0.503143\n",
      "Loss after iteration 2579: 0.503123\n",
      "Loss after iteration 2580: 0.503103\n",
      "Loss after iteration 2581: 0.503084\n",
      "Loss after iteration 2582: 0.503064\n",
      "Loss after iteration 2583: 0.503044\n",
      "Loss after iteration 2584: 0.503025\n",
      "Loss after iteration 2585: 0.503005\n",
      "Loss after iteration 2586: 0.502985\n",
      "Loss after iteration 2587: 0.502966\n",
      "Loss after iteration 2588: 0.502946\n",
      "Loss after iteration 2589: 0.502926\n",
      "Loss after iteration 2590: 0.502907\n",
      "Loss after iteration 2591: 0.502887\n",
      "Loss after iteration 2592: 0.502867\n",
      "Loss after iteration 2593: 0.502848\n",
      "Loss after iteration 2594: 0.502828\n",
      "Loss after iteration 2595: 0.502809\n",
      "Loss after iteration 2596: 0.502789\n",
      "Loss after iteration 2597: 0.502769\n",
      "Loss after iteration 2598: 0.502750\n",
      "Loss after iteration 2599: 0.502730\n",
      "Loss after iteration 2600: 0.502711\n",
      "Loss after iteration 2601: 0.502691\n",
      "Loss after iteration 2602: 0.502672\n",
      "Loss after iteration 2603: 0.502652\n",
      "Loss after iteration 2604: 0.502633\n",
      "Loss after iteration 2605: 0.502613\n",
      "Loss after iteration 2606: 0.502593\n",
      "Loss after iteration 2607: 0.502574\n",
      "Loss after iteration 2608: 0.502554\n",
      "Loss after iteration 2609: 0.502535\n",
      "Loss after iteration 2610: 0.502515\n",
      "Loss after iteration 2611: 0.502496\n",
      "Loss after iteration 2612: 0.502476\n",
      "Loss after iteration 2613: 0.502457\n",
      "Loss after iteration 2614: 0.502438\n",
      "Loss after iteration 2615: 0.502418\n",
      "Loss after iteration 2616: 0.502399\n",
      "Loss after iteration 2617: 0.502379\n",
      "Loss after iteration 2618: 0.502360\n",
      "Loss after iteration 2619: 0.502340\n",
      "Loss after iteration 2620: 0.502321\n",
      "Loss after iteration 2621: 0.502302\n",
      "Loss after iteration 2622: 0.502282\n",
      "Loss after iteration 2623: 0.502263\n",
      "Loss after iteration 2624: 0.502243\n",
      "Loss after iteration 2625: 0.502224\n",
      "Loss after iteration 2626: 0.502205\n",
      "Loss after iteration 2627: 0.502185\n",
      "Loss after iteration 2628: 0.502166\n",
      "Loss after iteration 2629: 0.502146\n",
      "Loss after iteration 2630: 0.502127\n",
      "Loss after iteration 2631: 0.502108\n",
      "Loss after iteration 2632: 0.502088\n",
      "Loss after iteration 2633: 0.502069\n",
      "Loss after iteration 2634: 0.502050\n",
      "Loss after iteration 2635: 0.502030\n",
      "Loss after iteration 2636: 0.502011\n",
      "Loss after iteration 2637: 0.501992\n",
      "Loss after iteration 2638: 0.501973\n",
      "Loss after iteration 2639: 0.501953\n",
      "Loss after iteration 2640: 0.501934\n",
      "Loss after iteration 2641: 0.501915\n",
      "Loss after iteration 2642: 0.501896\n",
      "Loss after iteration 2643: 0.501876\n",
      "Loss after iteration 2644: 0.501857\n",
      "Loss after iteration 2645: 0.501838\n",
      "Loss after iteration 2646: 0.501819\n",
      "Loss after iteration 2647: 0.501799\n",
      "Loss after iteration 2648: 0.501780\n",
      "Loss after iteration 2649: 0.501761\n",
      "Loss after iteration 2650: 0.501742\n",
      "Loss after iteration 2651: 0.501723\n",
      "Loss after iteration 2652: 0.501703\n",
      "Loss after iteration 2653: 0.501684\n",
      "Loss after iteration 2654: 0.501665\n",
      "Loss after iteration 2655: 0.501646\n",
      "Loss after iteration 2656: 0.501627\n",
      "Loss after iteration 2657: 0.501607\n",
      "Loss after iteration 2658: 0.501588\n",
      "Loss after iteration 2659: 0.501569\n",
      "Loss after iteration 2660: 0.501550\n",
      "Loss after iteration 2661: 0.501531\n",
      "Loss after iteration 2662: 0.501512\n",
      "Loss after iteration 2663: 0.501493\n",
      "Loss after iteration 2664: 0.501474\n",
      "Loss after iteration 2665: 0.501455\n",
      "Loss after iteration 2666: 0.501435\n",
      "Loss after iteration 2667: 0.501416\n",
      "Loss after iteration 2668: 0.501397\n",
      "Loss after iteration 2669: 0.501378\n",
      "Loss after iteration 2670: 0.501359\n",
      "Loss after iteration 2671: 0.501340\n",
      "Loss after iteration 2672: 0.501321\n",
      "Loss after iteration 2673: 0.501302\n",
      "Loss after iteration 2674: 0.501283\n",
      "Loss after iteration 2675: 0.501264\n",
      "Loss after iteration 2676: 0.501245\n",
      "Loss after iteration 2677: 0.501226\n",
      "Loss after iteration 2678: 0.501207\n",
      "Loss after iteration 2679: 0.501188\n",
      "Loss after iteration 2680: 0.501169\n",
      "Loss after iteration 2681: 0.501150\n",
      "Loss after iteration 2682: 0.501131\n",
      "Loss after iteration 2683: 0.501112\n",
      "Loss after iteration 2684: 0.501093\n",
      "Loss after iteration 2685: 0.501074\n",
      "Loss after iteration 2686: 0.501055\n",
      "Loss after iteration 2687: 0.501036\n",
      "Loss after iteration 2688: 0.501017\n",
      "Loss after iteration 2689: 0.500998\n",
      "Loss after iteration 2690: 0.500980\n",
      "Loss after iteration 2691: 0.500961\n",
      "Loss after iteration 2692: 0.500942\n",
      "Loss after iteration 2693: 0.500923\n",
      "Loss after iteration 2694: 0.500904\n",
      "Loss after iteration 2695: 0.500885\n",
      "Loss after iteration 2696: 0.500866\n",
      "Loss after iteration 2697: 0.500847\n",
      "Loss after iteration 2698: 0.500829\n",
      "Loss after iteration 2699: 0.500810\n",
      "Loss after iteration 2700: 0.500791\n",
      "Loss after iteration 2701: 0.500772\n",
      "Loss after iteration 2702: 0.500753\n",
      "Loss after iteration 2703: 0.500734\n",
      "Loss after iteration 2704: 0.500716\n",
      "Loss after iteration 2705: 0.500697\n",
      "Loss after iteration 2706: 0.500678\n",
      "Loss after iteration 2707: 0.500659\n",
      "Loss after iteration 2708: 0.500640\n",
      "Loss after iteration 2709: 0.500622\n",
      "Loss after iteration 2710: 0.500603\n",
      "Loss after iteration 2711: 0.500584\n",
      "Loss after iteration 2712: 0.500565\n",
      "Loss after iteration 2713: 0.500547\n",
      "Loss after iteration 2714: 0.500528\n",
      "Loss after iteration 2715: 0.500509\n",
      "Loss after iteration 2716: 0.500490\n",
      "Loss after iteration 2717: 0.500472\n",
      "Loss after iteration 2718: 0.500453\n",
      "Loss after iteration 2719: 0.500434\n",
      "Loss after iteration 2720: 0.500416\n",
      "Loss after iteration 2721: 0.500397\n",
      "Loss after iteration 2722: 0.500378\n",
      "Loss after iteration 2723: 0.500360\n",
      "Loss after iteration 2724: 0.500341\n",
      "Loss after iteration 2725: 0.500322\n",
      "Loss after iteration 2726: 0.500304\n",
      "Loss after iteration 2727: 0.500285\n",
      "Loss after iteration 2728: 0.500266\n",
      "Loss after iteration 2729: 0.500248\n",
      "Loss after iteration 2730: 0.500229\n",
      "Loss after iteration 2731: 0.500210\n",
      "Loss after iteration 2732: 0.500192\n",
      "Loss after iteration 2733: 0.500173\n",
      "Loss after iteration 2734: 0.500155\n",
      "Loss after iteration 2735: 0.500136\n",
      "Loss after iteration 2736: 0.500118\n",
      "Loss after iteration 2737: 0.500099\n",
      "Loss after iteration 2738: 0.500080\n",
      "Loss after iteration 2739: 0.500062\n",
      "Loss after iteration 2740: 0.500043\n",
      "Loss after iteration 2741: 0.500025\n",
      "Loss after iteration 2742: 0.500006\n",
      "Loss after iteration 2743: 0.499988\n",
      "Loss after iteration 2744: 0.499969\n",
      "Loss after iteration 2745: 0.499951\n",
      "Loss after iteration 2746: 0.499932\n",
      "Loss after iteration 2747: 0.499914\n",
      "Loss after iteration 2748: 0.499895\n",
      "Loss after iteration 2749: 0.499877\n",
      "Loss after iteration 2750: 0.499858\n",
      "Loss after iteration 2751: 0.499840\n",
      "Loss after iteration 2752: 0.499822\n",
      "Loss after iteration 2753: 0.499803\n",
      "Loss after iteration 2754: 0.499785\n",
      "Loss after iteration 2755: 0.499767\n",
      "Loss after iteration 2756: 0.499749\n",
      "Loss after iteration 2757: 0.499731\n",
      "Loss after iteration 2758: 0.499713\n",
      "Loss after iteration 2759: 0.499695\n",
      "Loss after iteration 2760: 0.499678\n",
      "Loss after iteration 2761: 0.499661\n",
      "Loss after iteration 2762: 0.499645\n",
      "Loss after iteration 2763: 0.499630\n",
      "Loss after iteration 2764: 0.499616\n",
      "Loss after iteration 2765: 0.499604\n",
      "Loss after iteration 2766: 0.499594\n",
      "Loss after iteration 2767: 0.499587\n",
      "Loss after iteration 2768: 0.499585\n",
      "Loss after iteration 2769: 0.499588\n",
      "Loss after iteration 2770: 0.499602\n",
      "Loss after iteration 2771: 0.499624\n",
      "Loss after iteration 2772: 0.499669\n",
      "Loss after iteration 2773: 0.499728\n",
      "Loss after iteration 2774: 0.499834\n",
      "Loss after iteration 2775: 0.499960\n",
      "Loss after iteration 2776: 0.500189\n",
      "Loss after iteration 2777: 0.500438\n",
      "Loss after iteration 2778: 0.500912\n",
      "Loss after iteration 2779: 0.501371\n",
      "Loss after iteration 2780: 0.502335\n",
      "Loss after iteration 2781: 0.503125\n",
      "Loss after iteration 2782: 0.505048\n",
      "Loss after iteration 2783: 0.506251\n",
      "Loss after iteration 2784: 0.509966\n",
      "Loss after iteration 2785: 0.511372\n",
      "Loss after iteration 2786: 0.518122\n",
      "Loss after iteration 2787: 0.518663\n",
      "Loss after iteration 2788: 0.529730\n",
      "Loss after iteration 2789: 0.526987\n",
      "Loss after iteration 2790: 0.542718\n",
      "Loss after iteration 2791: 0.533952\n",
      "Loss after iteration 2792: 0.553085\n",
      "Loss after iteration 2793: 0.537851\n",
      "Loss after iteration 2794: 0.558301\n",
      "Loss after iteration 2795: 0.538950\n",
      "Loss after iteration 2796: 0.559106\n",
      "Loss after iteration 2797: 0.538414\n",
      "Loss after iteration 2798: 0.557486\n",
      "Loss after iteration 2799: 0.537147\n",
      "Loss after iteration 2800: 0.554885\n",
      "Loss after iteration 2801: 0.535622\n",
      "Loss after iteration 2802: 0.552018\n",
      "Loss after iteration 2803: 0.534050\n",
      "Loss after iteration 2804: 0.549190\n",
      "Loss after iteration 2805: 0.532519\n",
      "Loss after iteration 2806: 0.546512\n",
      "Loss after iteration 2807: 0.531062\n",
      "Loss after iteration 2808: 0.544018\n",
      "Loss after iteration 2809: 0.529689\n",
      "Loss after iteration 2810: 0.541710\n",
      "Loss after iteration 2811: 0.528399\n",
      "Loss after iteration 2812: 0.539577\n",
      "Loss after iteration 2813: 0.527189\n",
      "Loss after iteration 2814: 0.537605\n",
      "Loss after iteration 2815: 0.526053\n",
      "Loss after iteration 2816: 0.535779\n",
      "Loss after iteration 2817: 0.524987\n",
      "Loss after iteration 2818: 0.534087\n",
      "Loss after iteration 2819: 0.523986\n",
      "Loss after iteration 2820: 0.532515\n",
      "Loss after iteration 2821: 0.523043\n",
      "Loss after iteration 2822: 0.531054\n",
      "Loss after iteration 2823: 0.522156\n",
      "Loss after iteration 2824: 0.529693\n",
      "Loss after iteration 2825: 0.521321\n",
      "Loss after iteration 2826: 0.528424\n",
      "Loss after iteration 2827: 0.520533\n",
      "Loss after iteration 2828: 0.527240\n",
      "Loss after iteration 2829: 0.519790\n",
      "Loss after iteration 2830: 0.526134\n",
      "Loss after iteration 2831: 0.519089\n",
      "Loss after iteration 2832: 0.525099\n",
      "Loss after iteration 2833: 0.518427\n",
      "Loss after iteration 2834: 0.524130\n",
      "Loss after iteration 2835: 0.517802\n",
      "Loss after iteration 2836: 0.523222\n",
      "Loss after iteration 2837: 0.517211\n",
      "Loss after iteration 2838: 0.522371\n",
      "Loss after iteration 2839: 0.516652\n",
      "Loss after iteration 2840: 0.521573\n",
      "Loss after iteration 2841: 0.516123\n",
      "Loss after iteration 2842: 0.520823\n",
      "Loss after iteration 2843: 0.515623\n",
      "Loss after iteration 2844: 0.520119\n",
      "Loss after iteration 2845: 0.515149\n",
      "Loss after iteration 2846: 0.519457\n",
      "Loss after iteration 2847: 0.514701\n",
      "Loss after iteration 2848: 0.518834\n",
      "Loss after iteration 2849: 0.514276\n",
      "Loss after iteration 2850: 0.518248\n",
      "Loss after iteration 2851: 0.513874\n",
      "Loss after iteration 2852: 0.517696\n",
      "Loss after iteration 2853: 0.513493\n",
      "Loss after iteration 2854: 0.517176\n",
      "Loss after iteration 2855: 0.513131\n",
      "Loss after iteration 2856: 0.516686\n",
      "Loss after iteration 2857: 0.512788\n",
      "Loss after iteration 2858: 0.516224\n",
      "Loss after iteration 2859: 0.512463\n",
      "Loss after iteration 2860: 0.515788\n",
      "Loss after iteration 2861: 0.512154\n",
      "Loss after iteration 2862: 0.515376\n",
      "Loss after iteration 2863: 0.511861\n",
      "Loss after iteration 2864: 0.514987\n",
      "Loss after iteration 2865: 0.511583\n",
      "Loss after iteration 2866: 0.514619\n",
      "Loss after iteration 2867: 0.511319\n",
      "Loss after iteration 2868: 0.514272\n",
      "Loss after iteration 2869: 0.511068\n",
      "Loss after iteration 2870: 0.513943\n",
      "Loss after iteration 2871: 0.510829\n",
      "Loss after iteration 2872: 0.513632\n",
      "Loss after iteration 2873: 0.510602\n",
      "Loss after iteration 2874: 0.513338\n",
      "Loss after iteration 2875: 0.510386\n",
      "Loss after iteration 2876: 0.513060\n",
      "Loss after iteration 2877: 0.510181\n",
      "Loss after iteration 2878: 0.512796\n",
      "Loss after iteration 2879: 0.509985\n",
      "Loss after iteration 2880: 0.512545\n",
      "Loss after iteration 2881: 0.509799\n",
      "Loss after iteration 2882: 0.512308\n",
      "Loss after iteration 2883: 0.509622\n",
      "Loss after iteration 2884: 0.512083\n",
      "Loss after iteration 2885: 0.509453\n",
      "Loss after iteration 2886: 0.511870\n",
      "Loss after iteration 2887: 0.509292\n",
      "Loss after iteration 2888: 0.511667\n",
      "Loss after iteration 2889: 0.509138\n",
      "Loss after iteration 2890: 0.511474\n",
      "Loss after iteration 2891: 0.508991\n",
      "Loss after iteration 2892: 0.511291\n",
      "Loss after iteration 2893: 0.508851\n",
      "Loss after iteration 2894: 0.511117\n",
      "Loss after iteration 2895: 0.508717\n",
      "Loss after iteration 2896: 0.510951\n",
      "Loss after iteration 2897: 0.508589\n",
      "Loss after iteration 2898: 0.510793\n",
      "Loss after iteration 2899: 0.508467\n",
      "Loss after iteration 2900: 0.510643\n",
      "Loss after iteration 2901: 0.508350\n",
      "Loss after iteration 2902: 0.510499\n",
      "Loss after iteration 2903: 0.508238\n",
      "Loss after iteration 2904: 0.510363\n",
      "Loss after iteration 2905: 0.508130\n",
      "Loss after iteration 2906: 0.510232\n",
      "Loss after iteration 2907: 0.508027\n",
      "Loss after iteration 2908: 0.510107\n",
      "Loss after iteration 2909: 0.507928\n",
      "Loss after iteration 2910: 0.509988\n",
      "Loss after iteration 2911: 0.507833\n",
      "Loss after iteration 2912: 0.509874\n",
      "Loss after iteration 2913: 0.507741\n",
      "Loss after iteration 2914: 0.509765\n",
      "Loss after iteration 2915: 0.507653\n",
      "Loss after iteration 2916: 0.509660\n",
      "Loss after iteration 2917: 0.507569\n",
      "Loss after iteration 2918: 0.509559\n",
      "Loss after iteration 2919: 0.507487\n",
      "Loss after iteration 2920: 0.509463\n",
      "Loss after iteration 2921: 0.507408\n",
      "Loss after iteration 2922: 0.509370\n",
      "Loss after iteration 2923: 0.507332\n",
      "Loss after iteration 2924: 0.509281\n",
      "Loss after iteration 2925: 0.507258\n",
      "Loss after iteration 2926: 0.509195\n",
      "Loss after iteration 2927: 0.507187\n",
      "Loss after iteration 2928: 0.509112\n",
      "Loss after iteration 2929: 0.507118\n",
      "Loss after iteration 2930: 0.509032\n",
      "Loss after iteration 2931: 0.507051\n",
      "Loss after iteration 2932: 0.508954\n",
      "Loss after iteration 2933: 0.506986\n",
      "Loss after iteration 2934: 0.508879\n",
      "Loss after iteration 2935: 0.506923\n",
      "Loss after iteration 2936: 0.508807\n",
      "Loss after iteration 2937: 0.506861\n",
      "Loss after iteration 2938: 0.508736\n",
      "Loss after iteration 2939: 0.506801\n",
      "Loss after iteration 2940: 0.508668\n",
      "Loss after iteration 2941: 0.506743\n",
      "Loss after iteration 2942: 0.508601\n",
      "Loss after iteration 2943: 0.506686\n",
      "Loss after iteration 2944: 0.508537\n",
      "Loss after iteration 2945: 0.506630\n",
      "Loss after iteration 2946: 0.508474\n",
      "Loss after iteration 2947: 0.506575\n",
      "Loss after iteration 2948: 0.508412\n",
      "Loss after iteration 2949: 0.506522\n",
      "Loss after iteration 2950: 0.508352\n",
      "Loss after iteration 2951: 0.506470\n",
      "Loss after iteration 2952: 0.508294\n",
      "Loss after iteration 2953: 0.506418\n",
      "Loss after iteration 2954: 0.508236\n",
      "Loss after iteration 2955: 0.506368\n",
      "Loss after iteration 2956: 0.508180\n",
      "Loss after iteration 2957: 0.506318\n",
      "Loss after iteration 2958: 0.508125\n",
      "Loss after iteration 2959: 0.506269\n",
      "Loss after iteration 2960: 0.508070\n",
      "Loss after iteration 2961: 0.506221\n",
      "Loss after iteration 2962: 0.508017\n",
      "Loss after iteration 2963: 0.506173\n",
      "Loss after iteration 2964: 0.507965\n",
      "Loss after iteration 2965: 0.506127\n",
      "Loss after iteration 2966: 0.507913\n",
      "Loss after iteration 2967: 0.506080\n",
      "Loss after iteration 2968: 0.507862\n",
      "Loss after iteration 2969: 0.506035\n",
      "Loss after iteration 2970: 0.507812\n",
      "Loss after iteration 2971: 0.505990\n",
      "Loss after iteration 2972: 0.507763\n",
      "Loss after iteration 2973: 0.505945\n",
      "Loss after iteration 2974: 0.507714\n",
      "Loss after iteration 2975: 0.505901\n",
      "Loss after iteration 2976: 0.507666\n",
      "Loss after iteration 2977: 0.505857\n",
      "Loss after iteration 2978: 0.507618\n",
      "Loss after iteration 2979: 0.505814\n",
      "Loss after iteration 2980: 0.507571\n",
      "Loss after iteration 2981: 0.505771\n",
      "Loss after iteration 2982: 0.507524\n",
      "Loss after iteration 2983: 0.505728\n",
      "Loss after iteration 2984: 0.507478\n",
      "Loss after iteration 2985: 0.505686\n",
      "Loss after iteration 2986: 0.507432\n",
      "Loss after iteration 2987: 0.505644\n",
      "Loss after iteration 2988: 0.507387\n",
      "Loss after iteration 2989: 0.505602\n",
      "Loss after iteration 2990: 0.507342\n",
      "Loss after iteration 2991: 0.505561\n",
      "Loss after iteration 2992: 0.507297\n",
      "Loss after iteration 2993: 0.505520\n",
      "Loss after iteration 2994: 0.507253\n",
      "Loss after iteration 2995: 0.505479\n",
      "Loss after iteration 2996: 0.507209\n",
      "Loss after iteration 2997: 0.505439\n",
      "Loss after iteration 2998: 0.507165\n",
      "Loss after iteration 2999: 0.505399\n",
      "Loss after iteration 3000: 0.507122\n",
      "Loss after iteration 3001: 0.505358\n",
      "Loss after iteration 3002: 0.507079\n",
      "Loss after iteration 3003: 0.505319\n",
      "Loss after iteration 3004: 0.507036\n",
      "Loss after iteration 3005: 0.505279\n",
      "Loss after iteration 3006: 0.506994\n",
      "Loss after iteration 3007: 0.505240\n",
      "Loss after iteration 3008: 0.506951\n",
      "Loss after iteration 3009: 0.505200\n",
      "Loss after iteration 3010: 0.506909\n",
      "Loss after iteration 3011: 0.505161\n",
      "Loss after iteration 3012: 0.506868\n",
      "Loss after iteration 3013: 0.505123\n",
      "Loss after iteration 3014: 0.506826\n",
      "Loss after iteration 3015: 0.505084\n",
      "Loss after iteration 3016: 0.506785\n",
      "Loss after iteration 3017: 0.505046\n",
      "Loss after iteration 3018: 0.506744\n",
      "Loss after iteration 3019: 0.505007\n",
      "Loss after iteration 3020: 0.506703\n",
      "Loss after iteration 3021: 0.504969\n",
      "Loss after iteration 3022: 0.506663\n",
      "Loss after iteration 3023: 0.504931\n",
      "Loss after iteration 3024: 0.506622\n",
      "Loss after iteration 3025: 0.504894\n",
      "Loss after iteration 3026: 0.506582\n",
      "Loss after iteration 3027: 0.504856\n",
      "Loss after iteration 3028: 0.506542\n",
      "Loss after iteration 3029: 0.504819\n",
      "Loss after iteration 3030: 0.506503\n",
      "Loss after iteration 3031: 0.504782\n",
      "Loss after iteration 3032: 0.506463\n",
      "Loss after iteration 3033: 0.504744\n",
      "Loss after iteration 3034: 0.506424\n",
      "Loss after iteration 3035: 0.504708\n",
      "Loss after iteration 3036: 0.506385\n",
      "Loss after iteration 3037: 0.504671\n",
      "Loss after iteration 3038: 0.506346\n",
      "Loss after iteration 3039: 0.504634\n",
      "Loss after iteration 3040: 0.506307\n",
      "Loss after iteration 3041: 0.504598\n",
      "Loss after iteration 3042: 0.506269\n",
      "Loss after iteration 3043: 0.504561\n",
      "Loss after iteration 3044: 0.506230\n",
      "Loss after iteration 3045: 0.504525\n",
      "Loss after iteration 3046: 0.506192\n",
      "Loss after iteration 3047: 0.504489\n",
      "Loss after iteration 3048: 0.506154\n",
      "Loss after iteration 3049: 0.504453\n",
      "Loss after iteration 3050: 0.506116\n",
      "Loss after iteration 3051: 0.504418\n",
      "Loss after iteration 3052: 0.506079\n",
      "Loss after iteration 3053: 0.504382\n",
      "Loss after iteration 3054: 0.506041\n",
      "Loss after iteration 3055: 0.504347\n",
      "Loss after iteration 3056: 0.506004\n",
      "Loss after iteration 3057: 0.504311\n",
      "Loss after iteration 3058: 0.505967\n",
      "Loss after iteration 3059: 0.504276\n",
      "Loss after iteration 3060: 0.505930\n",
      "Loss after iteration 3061: 0.504241\n",
      "Loss after iteration 3062: 0.505893\n",
      "Loss after iteration 3063: 0.504206\n",
      "Loss after iteration 3064: 0.505857\n",
      "Loss after iteration 3065: 0.504171\n",
      "Loss after iteration 3066: 0.505820\n",
      "Loss after iteration 3067: 0.504137\n",
      "Loss after iteration 3068: 0.505784\n",
      "Loss after iteration 3069: 0.504102\n",
      "Loss after iteration 3070: 0.505748\n",
      "Loss after iteration 3071: 0.504068\n",
      "Loss after iteration 3072: 0.505712\n",
      "Loss after iteration 3073: 0.504034\n",
      "Loss after iteration 3074: 0.505676\n",
      "Loss after iteration 3075: 0.503999\n",
      "Loss after iteration 3076: 0.505640\n",
      "Loss after iteration 3077: 0.503965\n",
      "Loss after iteration 3078: 0.505605\n",
      "Loss after iteration 3079: 0.503932\n",
      "Loss after iteration 3080: 0.505569\n",
      "Loss after iteration 3081: 0.503898\n",
      "Loss after iteration 3082: 0.505534\n",
      "Loss after iteration 3083: 0.503864\n",
      "Loss after iteration 3084: 0.505499\n",
      "Loss after iteration 3085: 0.503831\n",
      "Loss after iteration 3086: 0.505464\n",
      "Loss after iteration 3087: 0.503797\n",
      "Loss after iteration 3088: 0.505429\n",
      "Loss after iteration 3089: 0.503764\n",
      "Loss after iteration 3090: 0.505395\n",
      "Loss after iteration 3091: 0.503731\n",
      "Loss after iteration 3092: 0.505360\n",
      "Loss after iteration 3093: 0.503698\n",
      "Loss after iteration 3094: 0.505326\n",
      "Loss after iteration 3095: 0.503665\n",
      "Loss after iteration 3096: 0.505292\n",
      "Loss after iteration 3097: 0.503632\n",
      "Loss after iteration 3098: 0.505258\n",
      "Loss after iteration 3099: 0.503599\n",
      "Loss after iteration 3100: 0.505224\n",
      "Loss after iteration 3101: 0.503566\n",
      "Loss after iteration 3102: 0.505190\n",
      "Loss after iteration 3103: 0.503534\n",
      "Loss after iteration 3104: 0.505156\n",
      "Loss after iteration 3105: 0.503501\n",
      "Loss after iteration 3106: 0.505122\n",
      "Loss after iteration 3107: 0.503469\n",
      "Loss after iteration 3108: 0.505089\n",
      "Loss after iteration 3109: 0.503437\n",
      "Loss after iteration 3110: 0.505056\n",
      "Loss after iteration 3111: 0.503404\n",
      "Loss after iteration 3112: 0.505022\n",
      "Loss after iteration 3113: 0.503372\n",
      "Loss after iteration 3114: 0.504989\n",
      "Loss after iteration 3115: 0.503340\n",
      "Loss after iteration 3116: 0.504956\n",
      "Loss after iteration 3117: 0.503308\n",
      "Loss after iteration 3118: 0.504923\n",
      "Loss after iteration 3119: 0.503277\n",
      "Loss after iteration 3120: 0.504890\n",
      "Loss after iteration 3121: 0.503245\n",
      "Loss after iteration 3122: 0.504858\n",
      "Loss after iteration 3123: 0.503213\n",
      "Loss after iteration 3124: 0.504825\n",
      "Loss after iteration 3125: 0.503182\n",
      "Loss after iteration 3126: 0.504793\n",
      "Loss after iteration 3127: 0.503150\n",
      "Loss after iteration 3128: 0.504760\n",
      "Loss after iteration 3129: 0.503119\n",
      "Loss after iteration 3130: 0.504728\n",
      "Loss after iteration 3131: 0.503088\n",
      "Loss after iteration 3132: 0.504696\n",
      "Loss after iteration 3133: 0.503056\n",
      "Loss after iteration 3134: 0.504664\n",
      "Loss after iteration 3135: 0.503025\n",
      "Loss after iteration 3136: 0.504632\n",
      "Loss after iteration 3137: 0.502994\n",
      "Loss after iteration 3138: 0.504600\n",
      "Loss after iteration 3139: 0.502963\n",
      "Loss after iteration 3140: 0.504568\n",
      "Loss after iteration 3141: 0.502932\n",
      "Loss after iteration 3142: 0.504537\n",
      "Loss after iteration 3143: 0.502902\n",
      "Loss after iteration 3144: 0.504505\n",
      "Loss after iteration 3145: 0.502871\n",
      "Loss after iteration 3146: 0.504473\n",
      "Loss after iteration 3147: 0.502840\n",
      "Loss after iteration 3148: 0.504442\n",
      "Loss after iteration 3149: 0.502810\n",
      "Loss after iteration 3150: 0.504411\n",
      "Loss after iteration 3151: 0.502779\n",
      "Loss after iteration 3152: 0.504380\n",
      "Loss after iteration 3153: 0.502749\n",
      "Loss after iteration 3154: 0.504348\n",
      "Loss after iteration 3155: 0.502718\n",
      "Loss after iteration 3156: 0.504317\n",
      "Loss after iteration 3157: 0.502688\n",
      "Loss after iteration 3158: 0.504286\n",
      "Loss after iteration 3159: 0.502658\n",
      "Loss after iteration 3160: 0.504255\n",
      "Loss after iteration 3161: 0.502628\n",
      "Loss after iteration 3162: 0.504225\n",
      "Loss after iteration 3163: 0.502598\n",
      "Loss after iteration 3164: 0.504194\n",
      "Loss after iteration 3165: 0.502568\n",
      "Loss after iteration 3166: 0.504163\n",
      "Loss after iteration 3167: 0.502538\n",
      "Loss after iteration 3168: 0.504133\n",
      "Loss after iteration 3169: 0.502508\n",
      "Loss after iteration 3170: 0.504102\n",
      "Loss after iteration 3171: 0.502478\n",
      "Loss after iteration 3172: 0.504072\n",
      "Loss after iteration 3173: 0.502448\n",
      "Loss after iteration 3174: 0.504041\n",
      "Loss after iteration 3175: 0.502418\n",
      "Loss after iteration 3176: 0.504011\n",
      "Loss after iteration 3177: 0.502389\n",
      "Loss after iteration 3178: 0.503981\n",
      "Loss after iteration 3179: 0.502359\n",
      "Loss after iteration 3180: 0.503951\n",
      "Loss after iteration 3181: 0.502330\n",
      "Loss after iteration 3182: 0.503921\n",
      "Loss after iteration 3183: 0.502300\n",
      "Loss after iteration 3184: 0.503891\n",
      "Loss after iteration 3185: 0.502271\n",
      "Loss after iteration 3186: 0.503861\n",
      "Loss after iteration 3187: 0.502241\n",
      "Loss after iteration 3188: 0.503831\n",
      "Loss after iteration 3189: 0.502212\n",
      "Loss after iteration 3190: 0.503801\n",
      "Loss after iteration 3191: 0.502183\n",
      "Loss after iteration 3192: 0.503771\n",
      "Loss after iteration 3193: 0.502154\n",
      "Loss after iteration 3194: 0.503742\n",
      "Loss after iteration 3195: 0.502125\n",
      "Loss after iteration 3196: 0.503712\n",
      "Loss after iteration 3197: 0.502096\n",
      "Loss after iteration 3198: 0.503682\n",
      "Loss after iteration 3199: 0.502066\n",
      "Loss after iteration 3200: 0.503653\n",
      "Loss after iteration 3201: 0.502038\n",
      "Loss after iteration 3202: 0.503623\n",
      "Loss after iteration 3203: 0.502009\n",
      "Loss after iteration 3204: 0.503594\n",
      "Loss after iteration 3205: 0.501980\n",
      "Loss after iteration 3206: 0.503565\n",
      "Loss after iteration 3207: 0.501951\n",
      "Loss after iteration 3208: 0.503536\n",
      "Loss after iteration 3209: 0.501922\n",
      "Loss after iteration 3210: 0.503506\n",
      "Loss after iteration 3211: 0.501893\n",
      "Loss after iteration 3212: 0.503477\n",
      "Loss after iteration 3213: 0.501865\n",
      "Loss after iteration 3214: 0.503448\n",
      "Loss after iteration 3215: 0.501836\n",
      "Loss after iteration 3216: 0.503419\n",
      "Loss after iteration 3217: 0.501808\n",
      "Loss after iteration 3218: 0.503390\n",
      "Loss after iteration 3219: 0.501779\n",
      "Loss after iteration 3220: 0.503361\n",
      "Loss after iteration 3221: 0.501751\n",
      "Loss after iteration 3222: 0.503332\n",
      "Loss after iteration 3223: 0.501722\n",
      "Loss after iteration 3224: 0.503304\n",
      "Loss after iteration 3225: 0.501694\n",
      "Loss after iteration 3226: 0.503275\n",
      "Loss after iteration 3227: 0.501666\n",
      "Loss after iteration 3228: 0.503246\n",
      "Loss after iteration 3229: 0.501637\n",
      "Loss after iteration 3230: 0.503218\n",
      "Loss after iteration 3231: 0.501609\n",
      "Loss after iteration 3232: 0.503189\n",
      "Loss after iteration 3233: 0.501581\n",
      "Loss after iteration 3234: 0.503161\n",
      "Loss after iteration 3235: 0.501553\n",
      "Loss after iteration 3236: 0.503132\n",
      "Loss after iteration 3237: 0.501525\n",
      "Loss after iteration 3238: 0.503104\n",
      "Loss after iteration 3239: 0.501497\n",
      "Loss after iteration 3240: 0.503075\n",
      "Loss after iteration 3241: 0.501469\n",
      "Loss after iteration 3242: 0.503047\n",
      "Loss after iteration 3243: 0.501441\n",
      "Loss after iteration 3244: 0.503019\n",
      "Loss after iteration 3245: 0.501413\n",
      "Loss after iteration 3246: 0.502990\n",
      "Loss after iteration 3247: 0.501385\n",
      "Loss after iteration 3248: 0.502962\n",
      "Loss after iteration 3249: 0.501357\n",
      "Loss after iteration 3250: 0.502934\n",
      "Loss after iteration 3251: 0.501329\n",
      "Loss after iteration 3252: 0.502906\n",
      "Loss after iteration 3253: 0.501302\n",
      "Loss after iteration 3254: 0.502878\n",
      "Loss after iteration 3255: 0.501274\n",
      "Loss after iteration 3256: 0.502850\n",
      "Loss after iteration 3257: 0.501246\n",
      "Loss after iteration 3258: 0.502822\n",
      "Loss after iteration 3259: 0.501218\n",
      "Loss after iteration 3260: 0.502794\n",
      "Loss after iteration 3261: 0.501191\n",
      "Loss after iteration 3262: 0.502766\n",
      "Loss after iteration 3263: 0.501163\n",
      "Loss after iteration 3264: 0.502738\n",
      "Loss after iteration 3265: 0.501136\n",
      "Loss after iteration 3266: 0.502711\n",
      "Loss after iteration 3267: 0.501108\n",
      "Loss after iteration 3268: 0.502683\n",
      "Loss after iteration 3269: 0.501081\n",
      "Loss after iteration 3270: 0.502655\n",
      "Loss after iteration 3271: 0.501054\n",
      "Loss after iteration 3272: 0.502628\n",
      "Loss after iteration 3273: 0.501026\n",
      "Loss after iteration 3274: 0.502600\n",
      "Loss after iteration 3275: 0.500999\n",
      "Loss after iteration 3276: 0.502572\n",
      "Loss after iteration 3277: 0.500972\n",
      "Loss after iteration 3278: 0.502545\n",
      "Loss after iteration 3279: 0.500944\n",
      "Loss after iteration 3280: 0.502517\n",
      "Loss after iteration 3281: 0.500917\n",
      "Loss after iteration 3282: 0.502490\n",
      "Loss after iteration 3283: 0.500890\n",
      "Loss after iteration 3284: 0.502463\n",
      "Loss after iteration 3285: 0.500863\n",
      "Loss after iteration 3286: 0.502435\n",
      "Loss after iteration 3287: 0.500836\n",
      "Loss after iteration 3288: 0.502408\n",
      "Loss after iteration 3289: 0.500809\n",
      "Loss after iteration 3290: 0.502381\n",
      "Loss after iteration 3291: 0.500781\n",
      "Loss after iteration 3292: 0.502353\n",
      "Loss after iteration 3293: 0.500754\n",
      "Loss after iteration 3294: 0.502326\n",
      "Loss after iteration 3295: 0.500728\n",
      "Loss after iteration 3296: 0.502299\n",
      "Loss after iteration 3297: 0.500701\n",
      "Loss after iteration 3298: 0.502272\n",
      "Loss after iteration 3299: 0.500674\n",
      "Loss after iteration 3300: 0.502245\n",
      "Loss after iteration 3301: 0.500647\n",
      "Loss after iteration 3302: 0.502218\n",
      "Loss after iteration 3303: 0.500620\n",
      "Loss after iteration 3304: 0.502191\n",
      "Loss after iteration 3305: 0.500593\n",
      "Loss after iteration 3306: 0.502164\n",
      "Loss after iteration 3307: 0.500566\n",
      "Loss after iteration 3308: 0.502137\n",
      "Loss after iteration 3309: 0.500540\n",
      "Loss after iteration 3310: 0.502110\n",
      "Loss after iteration 3311: 0.500513\n",
      "Loss after iteration 3312: 0.502083\n",
      "Loss after iteration 3313: 0.500486\n",
      "Loss after iteration 3314: 0.502056\n",
      "Loss after iteration 3315: 0.500460\n",
      "Loss after iteration 3316: 0.502029\n",
      "Loss after iteration 3317: 0.500433\n",
      "Loss after iteration 3318: 0.502002\n",
      "Loss after iteration 3319: 0.500406\n",
      "Loss after iteration 3320: 0.501976\n",
      "Loss after iteration 3321: 0.500380\n",
      "Loss after iteration 3322: 0.501949\n",
      "Loss after iteration 3323: 0.500353\n",
      "Loss after iteration 3324: 0.501922\n",
      "Loss after iteration 3325: 0.500327\n",
      "Loss after iteration 3326: 0.501895\n",
      "Loss after iteration 3327: 0.500300\n",
      "Loss after iteration 3328: 0.501869\n",
      "Loss after iteration 3329: 0.500274\n",
      "Loss after iteration 3330: 0.501842\n",
      "Loss after iteration 3331: 0.500248\n",
      "Loss after iteration 3332: 0.501816\n",
      "Loss after iteration 3333: 0.500221\n",
      "Loss after iteration 3334: 0.501789\n",
      "Loss after iteration 3335: 0.500195\n",
      "Loss after iteration 3336: 0.501763\n",
      "Loss after iteration 3337: 0.500169\n",
      "Loss after iteration 3338: 0.501736\n",
      "Loss after iteration 3339: 0.500142\n",
      "Loss after iteration 3340: 0.501710\n",
      "Loss after iteration 3341: 0.500116\n",
      "Loss after iteration 3342: 0.501683\n",
      "Loss after iteration 3343: 0.500090\n",
      "Loss after iteration 3344: 0.501657\n",
      "Loss after iteration 3345: 0.500064\n",
      "Loss after iteration 3346: 0.501631\n",
      "Loss after iteration 3347: 0.500037\n",
      "Loss after iteration 3348: 0.501604\n",
      "Loss after iteration 3349: 0.500011\n",
      "Loss after iteration 3350: 0.501578\n",
      "Loss after iteration 3351: 0.499985\n",
      "Loss after iteration 3352: 0.501552\n",
      "Loss after iteration 3353: 0.499959\n",
      "Loss after iteration 3354: 0.501526\n",
      "Loss after iteration 3355: 0.499933\n",
      "Loss after iteration 3356: 0.501499\n",
      "Loss after iteration 3357: 0.499907\n",
      "Loss after iteration 3358: 0.501473\n",
      "Loss after iteration 3359: 0.499881\n",
      "Loss after iteration 3360: 0.501447\n",
      "Loss after iteration 3361: 0.499855\n",
      "Loss after iteration 3362: 0.501421\n",
      "Loss after iteration 3363: 0.499829\n",
      "Loss after iteration 3364: 0.501395\n",
      "Loss after iteration 3365: 0.499803\n",
      "Loss after iteration 3366: 0.501369\n",
      "Loss after iteration 3367: 0.499777\n",
      "Loss after iteration 3368: 0.501343\n",
      "Loss after iteration 3369: 0.499751\n",
      "Loss after iteration 3370: 0.501317\n",
      "Loss after iteration 3371: 0.499726\n",
      "Loss after iteration 3372: 0.501291\n",
      "Loss after iteration 3373: 0.499700\n",
      "Loss after iteration 3374: 0.501265\n",
      "Loss after iteration 3375: 0.499674\n",
      "Loss after iteration 3376: 0.501239\n",
      "Loss after iteration 3377: 0.499648\n",
      "Loss after iteration 3378: 0.501213\n",
      "Loss after iteration 3379: 0.499623\n",
      "Loss after iteration 3380: 0.501187\n",
      "Loss after iteration 3381: 0.499597\n",
      "Loss after iteration 3382: 0.501161\n",
      "Loss after iteration 3383: 0.499571\n",
      "Loss after iteration 3384: 0.501136\n",
      "Loss after iteration 3385: 0.499546\n",
      "Loss after iteration 3386: 0.501110\n",
      "Loss after iteration 3387: 0.499520\n",
      "Loss after iteration 3388: 0.501084\n",
      "Loss after iteration 3389: 0.499494\n",
      "Loss after iteration 3390: 0.501058\n",
      "Loss after iteration 3391: 0.499469\n",
      "Loss after iteration 3392: 0.501033\n",
      "Loss after iteration 3393: 0.499443\n",
      "Loss after iteration 3394: 0.501007\n",
      "Loss after iteration 3395: 0.499418\n",
      "Loss after iteration 3396: 0.500981\n",
      "Loss after iteration 3397: 0.499392\n",
      "Loss after iteration 3398: 0.500956\n",
      "Loss after iteration 3399: 0.499367\n",
      "Loss after iteration 3400: 0.500930\n",
      "Loss after iteration 3401: 0.499341\n",
      "Loss after iteration 3402: 0.500905\n",
      "Loss after iteration 3403: 0.499316\n",
      "Loss after iteration 3404: 0.500879\n",
      "Loss after iteration 3405: 0.499290\n",
      "Loss after iteration 3406: 0.500854\n",
      "Loss after iteration 3407: 0.499265\n",
      "Loss after iteration 3408: 0.500828\n",
      "Loss after iteration 3409: 0.499240\n",
      "Loss after iteration 3410: 0.500803\n",
      "Loss after iteration 3411: 0.499214\n",
      "Loss after iteration 3412: 0.500777\n",
      "Loss after iteration 3413: 0.499189\n",
      "Loss after iteration 3414: 0.500752\n",
      "Loss after iteration 3415: 0.499164\n",
      "Loss after iteration 3416: 0.500726\n",
      "Loss after iteration 3417: 0.499138\n",
      "Loss after iteration 3418: 0.500701\n",
      "Loss after iteration 3419: 0.499113\n",
      "Loss after iteration 3420: 0.500676\n",
      "Loss after iteration 3421: 0.499088\n",
      "Loss after iteration 3422: 0.500650\n",
      "Loss after iteration 3423: 0.499063\n",
      "Loss after iteration 3424: 0.500625\n",
      "Loss after iteration 3425: 0.499038\n",
      "Loss after iteration 3426: 0.500600\n",
      "Loss after iteration 3427: 0.499012\n",
      "Loss after iteration 3428: 0.500575\n",
      "Loss after iteration 3429: 0.498987\n",
      "Loss after iteration 3430: 0.500549\n",
      "Loss after iteration 3431: 0.498962\n",
      "Loss after iteration 3432: 0.500524\n",
      "Loss after iteration 3433: 0.498937\n",
      "Loss after iteration 3434: 0.500499\n",
      "Loss after iteration 3435: 0.498912\n",
      "Loss after iteration 3436: 0.500474\n",
      "Loss after iteration 3437: 0.498887\n",
      "Loss after iteration 3438: 0.500449\n",
      "Loss after iteration 3439: 0.498862\n",
      "Loss after iteration 3440: 0.500424\n",
      "Loss after iteration 3441: 0.498837\n",
      "Loss after iteration 3442: 0.500398\n",
      "Loss after iteration 3443: 0.498812\n",
      "Loss after iteration 3444: 0.500373\n",
      "Loss after iteration 3445: 0.498787\n",
      "Loss after iteration 3446: 0.500348\n",
      "Loss after iteration 3447: 0.498762\n",
      "Loss after iteration 3448: 0.500323\n",
      "Loss after iteration 3449: 0.498737\n",
      "Loss after iteration 3450: 0.500298\n",
      "Loss after iteration 3451: 0.498713\n",
      "Loss after iteration 3452: 0.500273\n",
      "Loss after iteration 3453: 0.498688\n",
      "Loss after iteration 3454: 0.500248\n",
      "Loss after iteration 3455: 0.498663\n",
      "Loss after iteration 3456: 0.500224\n",
      "Loss after iteration 3457: 0.498638\n",
      "Loss after iteration 3458: 0.500199\n",
      "Loss after iteration 3459: 0.498613\n",
      "Loss after iteration 3460: 0.500174\n",
      "Loss after iteration 3461: 0.498589\n",
      "Loss after iteration 3462: 0.500149\n",
      "Loss after iteration 3463: 0.498564\n",
      "Loss after iteration 3464: 0.500124\n",
      "Loss after iteration 3465: 0.498539\n",
      "Loss after iteration 3466: 0.500099\n",
      "Loss after iteration 3467: 0.498514\n",
      "Loss after iteration 3468: 0.500074\n",
      "Loss after iteration 3469: 0.498490\n",
      "Loss after iteration 3470: 0.500050\n",
      "Loss after iteration 3471: 0.498465\n",
      "Loss after iteration 3472: 0.500025\n",
      "Loss after iteration 3473: 0.498440\n",
      "Loss after iteration 3474: 0.500000\n",
      "Loss after iteration 3475: 0.498416\n",
      "Loss after iteration 3476: 0.499976\n",
      "Loss after iteration 3477: 0.498391\n",
      "Loss after iteration 3478: 0.499951\n",
      "Loss after iteration 3479: 0.498367\n",
      "Loss after iteration 3480: 0.499926\n",
      "Loss after iteration 3481: 0.498342\n",
      "Loss after iteration 3482: 0.499902\n",
      "Loss after iteration 3483: 0.498318\n",
      "Loss after iteration 3484: 0.499877\n",
      "Loss after iteration 3485: 0.498293\n",
      "Loss after iteration 3486: 0.499852\n",
      "Loss after iteration 3487: 0.498269\n",
      "Loss after iteration 3488: 0.499828\n",
      "Loss after iteration 3489: 0.498244\n",
      "Loss after iteration 3490: 0.499803\n",
      "Loss after iteration 3491: 0.498220\n",
      "Loss after iteration 3492: 0.499779\n",
      "Loss after iteration 3493: 0.498195\n",
      "Loss after iteration 3494: 0.499754\n",
      "Loss after iteration 3495: 0.498171\n",
      "Loss after iteration 3496: 0.499730\n",
      "Loss after iteration 3497: 0.498146\n",
      "Loss after iteration 3498: 0.499705\n",
      "Loss after iteration 3499: 0.498122\n",
      "Loss after iteration 3500: 0.499681\n",
      "Loss after iteration 3501: 0.498098\n",
      "Loss after iteration 3502: 0.499656\n",
      "Loss after iteration 3503: 0.498073\n",
      "Loss after iteration 3504: 0.499632\n",
      "Loss after iteration 3505: 0.498049\n",
      "Loss after iteration 3506: 0.499607\n",
      "Loss after iteration 3507: 0.498025\n",
      "Loss after iteration 3508: 0.499583\n",
      "Loss after iteration 3509: 0.498001\n",
      "Loss after iteration 3510: 0.499559\n",
      "Loss after iteration 3511: 0.497976\n",
      "Loss after iteration 3512: 0.499534\n",
      "Loss after iteration 3513: 0.497952\n",
      "Loss after iteration 3514: 0.499510\n",
      "Loss after iteration 3515: 0.497928\n",
      "Loss after iteration 3516: 0.499486\n",
      "Loss after iteration 3517: 0.497904\n",
      "Loss after iteration 3518: 0.499461\n",
      "Loss after iteration 3519: 0.497880\n",
      "Loss after iteration 3520: 0.499437\n",
      "Loss after iteration 3521: 0.497855\n",
      "Loss after iteration 3522: 0.499413\n",
      "Loss after iteration 3523: 0.497831\n",
      "Loss after iteration 3524: 0.499389\n",
      "Loss after iteration 3525: 0.497807\n",
      "Loss after iteration 3526: 0.499365\n",
      "Loss after iteration 3527: 0.497783\n",
      "Loss after iteration 3528: 0.499340\n",
      "Loss after iteration 3529: 0.497759\n",
      "Loss after iteration 3530: 0.499316\n",
      "Loss after iteration 3531: 0.497735\n",
      "Loss after iteration 3532: 0.499292\n",
      "Loss after iteration 3533: 0.497711\n",
      "Loss after iteration 3534: 0.499268\n",
      "Loss after iteration 3535: 0.497687\n",
      "Loss after iteration 3536: 0.499244\n",
      "Loss after iteration 3537: 0.497663\n",
      "Loss after iteration 3538: 0.499220\n",
      "Loss after iteration 3539: 0.497639\n",
      "Loss after iteration 3540: 0.499196\n",
      "Loss after iteration 3541: 0.497615\n",
      "Loss after iteration 3542: 0.499172\n",
      "Loss after iteration 3543: 0.497591\n",
      "Loss after iteration 3544: 0.499148\n",
      "Loss after iteration 3545: 0.497567\n",
      "Loss after iteration 3546: 0.499124\n",
      "Loss after iteration 3547: 0.497543\n",
      "Loss after iteration 3548: 0.499100\n",
      "Loss after iteration 3549: 0.497520\n",
      "Loss after iteration 3550: 0.499076\n",
      "Loss after iteration 3551: 0.497496\n",
      "Loss after iteration 3552: 0.499052\n",
      "Loss after iteration 3553: 0.497472\n",
      "Loss after iteration 3554: 0.499028\n",
      "Loss after iteration 3555: 0.497448\n",
      "Loss after iteration 3556: 0.499004\n",
      "Loss after iteration 3557: 0.497424\n",
      "Loss after iteration 3558: 0.498980\n",
      "Loss after iteration 3559: 0.497400\n",
      "Loss after iteration 3560: 0.498956\n",
      "Loss after iteration 3561: 0.497377\n",
      "Loss after iteration 3562: 0.498932\n",
      "Loss after iteration 3563: 0.497353\n",
      "Loss after iteration 3564: 0.498909\n",
      "Loss after iteration 3565: 0.497329\n",
      "Loss after iteration 3566: 0.498885\n",
      "Loss after iteration 3567: 0.497306\n",
      "Loss after iteration 3568: 0.498861\n",
      "Loss after iteration 3569: 0.497282\n",
      "Loss after iteration 3570: 0.498837\n",
      "Loss after iteration 3571: 0.497258\n",
      "Loss after iteration 3572: 0.498813\n",
      "Loss after iteration 3573: 0.497235\n",
      "Loss after iteration 3574: 0.498790\n",
      "Loss after iteration 3575: 0.497211\n",
      "Loss after iteration 3576: 0.498766\n",
      "Loss after iteration 3577: 0.497187\n",
      "Loss after iteration 3578: 0.498742\n",
      "Loss after iteration 3579: 0.497164\n",
      "Loss after iteration 3580: 0.498719\n",
      "Loss after iteration 3581: 0.497140\n",
      "Loss after iteration 3582: 0.498695\n",
      "Loss after iteration 3583: 0.497117\n",
      "Loss after iteration 3584: 0.498671\n",
      "Loss after iteration 3585: 0.497093\n",
      "Loss after iteration 3586: 0.498648\n",
      "Loss after iteration 3587: 0.497070\n",
      "Loss after iteration 3588: 0.498624\n",
      "Loss after iteration 3589: 0.497046\n",
      "Loss after iteration 3590: 0.498601\n",
      "Loss after iteration 3591: 0.497023\n",
      "Loss after iteration 3592: 0.498577\n",
      "Loss after iteration 3593: 0.496999\n",
      "Loss after iteration 3594: 0.498553\n",
      "Loss after iteration 3595: 0.496976\n",
      "Loss after iteration 3596: 0.498530\n",
      "Loss after iteration 3597: 0.496952\n",
      "Loss after iteration 3598: 0.498506\n",
      "Loss after iteration 3599: 0.496929\n",
      "Loss after iteration 3600: 0.498483\n",
      "Loss after iteration 3601: 0.496906\n",
      "Loss after iteration 3602: 0.498459\n",
      "Loss after iteration 3603: 0.496882\n",
      "Loss after iteration 3604: 0.498436\n",
      "Loss after iteration 3605: 0.496859\n",
      "Loss after iteration 3606: 0.498412\n",
      "Loss after iteration 3607: 0.496836\n",
      "Loss after iteration 3608: 0.498389\n",
      "Loss after iteration 3609: 0.496812\n",
      "Loss after iteration 3610: 0.498366\n",
      "Loss after iteration 3611: 0.496789\n",
      "Loss after iteration 3612: 0.498342\n",
      "Loss after iteration 3613: 0.496766\n",
      "Loss after iteration 3614: 0.498319\n",
      "Loss after iteration 3615: 0.496743\n",
      "Loss after iteration 3616: 0.498296\n",
      "Loss after iteration 3617: 0.496719\n",
      "Loss after iteration 3618: 0.498272\n",
      "Loss after iteration 3619: 0.496696\n",
      "Loss after iteration 3620: 0.498249\n",
      "Loss after iteration 3621: 0.496673\n",
      "Loss after iteration 3622: 0.498226\n",
      "Loss after iteration 3623: 0.496650\n",
      "Loss after iteration 3624: 0.498202\n",
      "Loss after iteration 3625: 0.496627\n",
      "Loss after iteration 3626: 0.498179\n",
      "Loss after iteration 3627: 0.496603\n",
      "Loss after iteration 3628: 0.498156\n",
      "Loss after iteration 3629: 0.496580\n",
      "Loss after iteration 3630: 0.498133\n",
      "Loss after iteration 3631: 0.496557\n",
      "Loss after iteration 3632: 0.498109\n",
      "Loss after iteration 3633: 0.496534\n",
      "Loss after iteration 3634: 0.498086\n",
      "Loss after iteration 3635: 0.496511\n",
      "Loss after iteration 3636: 0.498063\n",
      "Loss after iteration 3637: 0.496488\n",
      "Loss after iteration 3638: 0.498040\n",
      "Loss after iteration 3639: 0.496465\n",
      "Loss after iteration 3640: 0.498017\n",
      "Loss after iteration 3641: 0.496442\n",
      "Loss after iteration 3642: 0.497994\n",
      "Loss after iteration 3643: 0.496419\n",
      "Loss after iteration 3644: 0.497970\n",
      "Loss after iteration 3645: 0.496396\n",
      "Loss after iteration 3646: 0.497947\n",
      "Loss after iteration 3647: 0.496373\n",
      "Loss after iteration 3648: 0.497924\n",
      "Loss after iteration 3649: 0.496350\n",
      "Loss after iteration 3650: 0.497901\n",
      "Loss after iteration 3651: 0.496327\n",
      "Loss after iteration 3652: 0.497878\n",
      "Loss after iteration 3653: 0.496304\n",
      "Loss after iteration 3654: 0.497855\n",
      "Loss after iteration 3655: 0.496281\n",
      "Loss after iteration 3656: 0.497832\n",
      "Loss after iteration 3657: 0.496258\n",
      "Loss after iteration 3658: 0.497809\n",
      "Loss after iteration 3659: 0.496236\n",
      "Loss after iteration 3660: 0.497786\n",
      "Loss after iteration 3661: 0.496213\n",
      "Loss after iteration 3662: 0.497763\n",
      "Loss after iteration 3663: 0.496190\n",
      "Loss after iteration 3664: 0.497740\n",
      "Loss after iteration 3665: 0.496167\n",
      "Loss after iteration 3666: 0.497718\n",
      "Loss after iteration 3667: 0.496144\n",
      "Loss after iteration 3668: 0.497695\n",
      "Loss after iteration 3669: 0.496122\n",
      "Loss after iteration 3670: 0.497672\n",
      "Loss after iteration 3671: 0.496099\n",
      "Loss after iteration 3672: 0.497649\n",
      "Loss after iteration 3673: 0.496076\n",
      "Loss after iteration 3674: 0.497626\n",
      "Loss after iteration 3675: 0.496053\n",
      "Loss after iteration 3676: 0.497603\n",
      "Loss after iteration 3677: 0.496031\n",
      "Loss after iteration 3678: 0.497580\n",
      "Loss after iteration 3679: 0.496008\n",
      "Loss after iteration 3680: 0.497558\n",
      "Loss after iteration 3681: 0.495985\n",
      "Loss after iteration 3682: 0.497535\n",
      "Loss after iteration 3683: 0.495963\n",
      "Loss after iteration 3684: 0.497512\n",
      "Loss after iteration 3685: 0.495940\n",
      "Loss after iteration 3686: 0.497489\n",
      "Loss after iteration 3687: 0.495917\n",
      "Loss after iteration 3688: 0.497467\n",
      "Loss after iteration 3689: 0.495895\n",
      "Loss after iteration 3690: 0.497444\n",
      "Loss after iteration 3691: 0.495872\n",
      "Loss after iteration 3692: 0.497421\n",
      "Loss after iteration 3693: 0.495850\n",
      "Loss after iteration 3694: 0.497399\n",
      "Loss after iteration 3695: 0.495827\n",
      "Loss after iteration 3696: 0.497376\n",
      "Loss after iteration 3697: 0.495805\n",
      "Loss after iteration 3698: 0.497353\n",
      "Loss after iteration 3699: 0.495782\n",
      "Loss after iteration 3700: 0.497331\n",
      "Loss after iteration 3701: 0.495760\n",
      "Loss after iteration 3702: 0.497308\n",
      "Loss after iteration 3703: 0.495737\n",
      "Loss after iteration 3704: 0.497286\n",
      "Loss after iteration 3705: 0.495715\n",
      "Loss after iteration 3706: 0.497263\n",
      "Loss after iteration 3707: 0.495692\n",
      "Loss after iteration 3708: 0.497240\n",
      "Loss after iteration 3709: 0.495670\n",
      "Loss after iteration 3710: 0.497218\n",
      "Loss after iteration 3711: 0.495647\n",
      "Loss after iteration 3712: 0.497195\n",
      "Loss after iteration 3713: 0.495625\n",
      "Loss after iteration 3714: 0.497173\n",
      "Loss after iteration 3715: 0.495603\n",
      "Loss after iteration 3716: 0.497150\n",
      "Loss after iteration 3717: 0.495580\n",
      "Loss after iteration 3718: 0.497128\n",
      "Loss after iteration 3719: 0.495558\n",
      "Loss after iteration 3720: 0.497105\n",
      "Loss after iteration 3721: 0.495536\n",
      "Loss after iteration 3722: 0.497083\n",
      "Loss after iteration 3723: 0.495513\n",
      "Loss after iteration 3724: 0.497061\n",
      "Loss after iteration 3725: 0.495491\n",
      "Loss after iteration 3726: 0.497038\n",
      "Loss after iteration 3727: 0.495469\n",
      "Loss after iteration 3728: 0.497016\n",
      "Loss after iteration 3729: 0.495447\n",
      "Loss after iteration 3730: 0.496993\n",
      "Loss after iteration 3731: 0.495424\n",
      "Loss after iteration 3732: 0.496971\n",
      "Loss after iteration 3733: 0.495402\n",
      "Loss after iteration 3734: 0.496949\n",
      "Loss after iteration 3735: 0.495380\n",
      "Loss after iteration 3736: 0.496926\n",
      "Loss after iteration 3737: 0.495358\n",
      "Loss after iteration 3738: 0.496904\n",
      "Loss after iteration 3739: 0.495336\n",
      "Loss after iteration 3740: 0.496882\n",
      "Loss after iteration 3741: 0.495313\n",
      "Loss after iteration 3742: 0.496860\n",
      "Loss after iteration 3743: 0.495291\n",
      "Loss after iteration 3744: 0.496837\n",
      "Loss after iteration 3745: 0.495269\n",
      "Loss after iteration 3746: 0.496815\n",
      "Loss after iteration 3747: 0.495247\n",
      "Loss after iteration 3748: 0.496793\n",
      "Loss after iteration 3749: 0.495225\n",
      "Loss after iteration 3750: 0.496771\n",
      "Loss after iteration 3751: 0.495203\n",
      "Loss after iteration 3752: 0.496749\n",
      "Loss after iteration 3753: 0.495181\n",
      "Loss after iteration 3754: 0.496726\n",
      "Loss after iteration 3755: 0.495159\n",
      "Loss after iteration 3756: 0.496704\n",
      "Loss after iteration 3757: 0.495137\n",
      "Loss after iteration 3758: 0.496682\n",
      "Loss after iteration 3759: 0.495115\n",
      "Loss after iteration 3760: 0.496660\n",
      "Loss after iteration 3761: 0.495093\n",
      "Loss after iteration 3762: 0.496638\n",
      "Loss after iteration 3763: 0.495071\n",
      "Loss after iteration 3764: 0.496616\n",
      "Loss after iteration 3765: 0.495049\n",
      "Loss after iteration 3766: 0.496594\n",
      "Loss after iteration 3767: 0.495027\n",
      "Loss after iteration 3768: 0.496572\n",
      "Loss after iteration 3769: 0.495005\n",
      "Loss after iteration 3770: 0.496550\n",
      "Loss after iteration 3771: 0.494983\n",
      "Loss after iteration 3772: 0.496528\n",
      "Loss after iteration 3773: 0.494961\n",
      "Loss after iteration 3774: 0.496506\n",
      "Loss after iteration 3775: 0.494940\n",
      "Loss after iteration 3776: 0.496484\n",
      "Loss after iteration 3777: 0.494918\n",
      "Loss after iteration 3778: 0.496462\n",
      "Loss after iteration 3779: 0.494896\n",
      "Loss after iteration 3780: 0.496440\n",
      "Loss after iteration 3781: 0.494874\n",
      "Loss after iteration 3782: 0.496418\n",
      "Loss after iteration 3783: 0.494852\n",
      "Loss after iteration 3784: 0.496396\n",
      "Loss after iteration 3785: 0.494831\n",
      "Loss after iteration 3786: 0.496374\n",
      "Loss after iteration 3787: 0.494809\n",
      "Loss after iteration 3788: 0.496352\n",
      "Loss after iteration 3789: 0.494787\n",
      "Loss after iteration 3790: 0.496330\n",
      "Loss after iteration 3791: 0.494765\n",
      "Loss after iteration 3792: 0.496308\n",
      "Loss after iteration 3793: 0.494744\n",
      "Loss after iteration 3794: 0.496287\n",
      "Loss after iteration 3795: 0.494722\n",
      "Loss after iteration 3796: 0.496265\n",
      "Loss after iteration 3797: 0.494700\n",
      "Loss after iteration 3798: 0.496243\n",
      "Loss after iteration 3799: 0.494679\n",
      "Loss after iteration 3800: 0.496221\n",
      "Loss after iteration 3801: 0.494657\n",
      "Loss after iteration 3802: 0.496199\n",
      "Loss after iteration 3803: 0.494635\n",
      "Loss after iteration 3804: 0.496178\n",
      "Loss after iteration 3805: 0.494614\n",
      "Loss after iteration 3806: 0.496156\n",
      "Loss after iteration 3807: 0.494592\n",
      "Loss after iteration 3808: 0.496134\n",
      "Loss after iteration 3809: 0.494570\n",
      "Loss after iteration 3810: 0.496113\n",
      "Loss after iteration 3811: 0.494549\n",
      "Loss after iteration 3812: 0.496091\n",
      "Loss after iteration 3813: 0.494527\n",
      "Loss after iteration 3814: 0.496069\n",
      "Loss after iteration 3815: 0.494506\n",
      "Loss after iteration 3816: 0.496048\n",
      "Loss after iteration 3817: 0.494484\n",
      "Loss after iteration 3818: 0.496026\n",
      "Loss after iteration 3819: 0.494463\n",
      "Loss after iteration 3820: 0.496004\n",
      "Loss after iteration 3821: 0.494441\n",
      "Loss after iteration 3822: 0.495983\n",
      "Loss after iteration 3823: 0.494420\n",
      "Loss after iteration 3824: 0.495961\n",
      "Loss after iteration 3825: 0.494398\n",
      "Loss after iteration 3826: 0.495939\n",
      "Loss after iteration 3827: 0.494377\n",
      "Loss after iteration 3828: 0.495918\n",
      "Loss after iteration 3829: 0.494356\n",
      "Loss after iteration 3830: 0.495896\n",
      "Loss after iteration 3831: 0.494334\n",
      "Loss after iteration 3832: 0.495875\n",
      "Loss after iteration 3833: 0.494313\n",
      "Loss after iteration 3834: 0.495853\n",
      "Loss after iteration 3835: 0.494291\n",
      "Loss after iteration 3836: 0.495832\n",
      "Loss after iteration 3837: 0.494270\n",
      "Loss after iteration 3838: 0.495810\n",
      "Loss after iteration 3839: 0.494249\n",
      "Loss after iteration 3840: 0.495789\n",
      "Loss after iteration 3841: 0.494227\n",
      "Loss after iteration 3842: 0.495767\n",
      "Loss after iteration 3843: 0.494206\n",
      "Loss after iteration 3844: 0.495746\n",
      "Loss after iteration 3845: 0.494185\n",
      "Loss after iteration 3846: 0.495725\n",
      "Loss after iteration 3847: 0.494164\n",
      "Loss after iteration 3848: 0.495703\n",
      "Loss after iteration 3849: 0.494142\n",
      "Loss after iteration 3850: 0.495682\n",
      "Loss after iteration 3851: 0.494121\n",
      "Loss after iteration 3852: 0.495661\n",
      "Loss after iteration 3853: 0.494100\n",
      "Loss after iteration 3854: 0.495639\n",
      "Loss after iteration 3855: 0.494079\n",
      "Loss after iteration 3856: 0.495618\n",
      "Loss after iteration 3857: 0.494058\n",
      "Loss after iteration 3858: 0.495597\n",
      "Loss after iteration 3859: 0.494036\n",
      "Loss after iteration 3860: 0.495575\n",
      "Loss after iteration 3861: 0.494015\n",
      "Loss after iteration 3862: 0.495554\n",
      "Loss after iteration 3863: 0.493994\n",
      "Loss after iteration 3864: 0.495533\n",
      "Loss after iteration 3865: 0.493973\n",
      "Loss after iteration 3866: 0.495511\n",
      "Loss after iteration 3867: 0.493952\n",
      "Loss after iteration 3868: 0.495490\n",
      "Loss after iteration 3869: 0.493931\n",
      "Loss after iteration 3870: 0.495469\n",
      "Loss after iteration 3871: 0.493910\n",
      "Loss after iteration 3872: 0.495448\n",
      "Loss after iteration 3873: 0.493889\n",
      "Loss after iteration 3874: 0.495427\n",
      "Loss after iteration 3875: 0.493868\n",
      "Loss after iteration 3876: 0.495405\n",
      "Loss after iteration 3877: 0.493847\n",
      "Loss after iteration 3878: 0.495384\n",
      "Loss after iteration 3879: 0.493826\n",
      "Loss after iteration 3880: 0.495363\n",
      "Loss after iteration 3881: 0.493805\n",
      "Loss after iteration 3882: 0.495342\n",
      "Loss after iteration 3883: 0.493784\n",
      "Loss after iteration 3884: 0.495321\n",
      "Loss after iteration 3885: 0.493763\n",
      "Loss after iteration 3886: 0.495300\n",
      "Loss after iteration 3887: 0.493742\n",
      "Loss after iteration 3888: 0.495279\n",
      "Loss after iteration 3889: 0.493721\n",
      "Loss after iteration 3890: 0.495258\n",
      "Loss after iteration 3891: 0.493700\n",
      "Loss after iteration 3892: 0.495237\n",
      "Loss after iteration 3893: 0.493679\n",
      "Loss after iteration 3894: 0.495216\n",
      "Loss after iteration 3895: 0.493658\n",
      "Loss after iteration 3896: 0.495195\n",
      "Loss after iteration 3897: 0.493637\n",
      "Loss after iteration 3898: 0.495174\n",
      "Loss after iteration 3899: 0.493616\n",
      "Loss after iteration 3900: 0.495153\n",
      "Loss after iteration 3901: 0.493596\n",
      "Loss after iteration 3902: 0.495132\n",
      "Loss after iteration 3903: 0.493575\n",
      "Loss after iteration 3904: 0.495111\n",
      "Loss after iteration 3905: 0.493554\n",
      "Loss after iteration 3906: 0.495090\n",
      "Loss after iteration 3907: 0.493533\n",
      "Loss after iteration 3908: 0.495069\n",
      "Loss after iteration 3909: 0.493512\n",
      "Loss after iteration 3910: 0.495048\n",
      "Loss after iteration 3911: 0.493492\n",
      "Loss after iteration 3912: 0.495027\n",
      "Loss after iteration 3913: 0.493471\n",
      "Loss after iteration 3914: 0.495006\n",
      "Loss after iteration 3915: 0.493450\n",
      "Loss after iteration 3916: 0.494985\n",
      "Loss after iteration 3917: 0.493430\n",
      "Loss after iteration 3918: 0.494965\n",
      "Loss after iteration 3919: 0.493409\n",
      "Loss after iteration 3920: 0.494944\n",
      "Loss after iteration 3921: 0.493388\n",
      "Loss after iteration 3922: 0.494923\n",
      "Loss after iteration 3923: 0.493368\n",
      "Loss after iteration 3924: 0.494902\n",
      "Loss after iteration 3925: 0.493347\n",
      "Loss after iteration 3926: 0.494881\n",
      "Loss after iteration 3927: 0.493326\n",
      "Loss after iteration 3928: 0.494861\n",
      "Loss after iteration 3929: 0.493306\n",
      "Loss after iteration 3930: 0.494840\n",
      "Loss after iteration 3931: 0.493285\n",
      "Loss after iteration 3932: 0.494819\n",
      "Loss after iteration 3933: 0.493265\n",
      "Loss after iteration 3934: 0.494798\n",
      "Loss after iteration 3935: 0.493244\n",
      "Loss after iteration 3936: 0.494778\n",
      "Loss after iteration 3937: 0.493223\n",
      "Loss after iteration 3938: 0.494757\n",
      "Loss after iteration 3939: 0.493203\n",
      "Loss after iteration 3940: 0.494736\n",
      "Loss after iteration 3941: 0.493182\n",
      "Loss after iteration 3942: 0.494716\n",
      "Loss after iteration 3943: 0.493162\n",
      "Loss after iteration 3944: 0.494695\n",
      "Loss after iteration 3945: 0.493141\n",
      "Loss after iteration 3946: 0.494675\n",
      "Loss after iteration 3947: 0.493121\n",
      "Loss after iteration 3948: 0.494654\n",
      "Loss after iteration 3949: 0.493100\n",
      "Loss after iteration 3950: 0.494633\n",
      "Loss after iteration 3951: 0.493080\n",
      "Loss after iteration 3952: 0.494613\n",
      "Loss after iteration 3953: 0.493060\n",
      "Loss after iteration 3954: 0.494592\n",
      "Loss after iteration 3955: 0.493039\n",
      "Loss after iteration 3956: 0.494572\n",
      "Loss after iteration 3957: 0.493019\n",
      "Loss after iteration 3958: 0.494551\n",
      "Loss after iteration 3959: 0.492998\n",
      "Loss after iteration 3960: 0.494531\n",
      "Loss after iteration 3961: 0.492978\n",
      "Loss after iteration 3962: 0.494510\n",
      "Loss after iteration 3963: 0.492958\n",
      "Loss after iteration 3964: 0.494490\n",
      "Loss after iteration 3965: 0.492937\n",
      "Loss after iteration 3966: 0.494469\n",
      "Loss after iteration 3967: 0.492917\n",
      "Loss after iteration 3968: 0.494449\n",
      "Loss after iteration 3969: 0.492897\n",
      "Loss after iteration 3970: 0.494428\n",
      "Loss after iteration 3971: 0.492877\n",
      "Loss after iteration 3972: 0.494408\n",
      "Loss after iteration 3973: 0.492856\n",
      "Loss after iteration 3974: 0.494388\n",
      "Loss after iteration 3975: 0.492836\n",
      "Loss after iteration 3976: 0.494367\n",
      "Loss after iteration 3977: 0.492816\n",
      "Loss after iteration 3978: 0.494347\n",
      "Loss after iteration 3979: 0.492796\n",
      "Loss after iteration 3980: 0.494326\n",
      "Loss after iteration 3981: 0.492775\n",
      "Loss after iteration 3982: 0.494306\n",
      "Loss after iteration 3983: 0.492755\n",
      "Loss after iteration 3984: 0.494286\n",
      "Loss after iteration 3985: 0.492735\n",
      "Loss after iteration 3986: 0.494265\n",
      "Loss after iteration 3987: 0.492715\n",
      "Loss after iteration 3988: 0.494245\n",
      "Loss after iteration 3989: 0.492695\n",
      "Loss after iteration 3990: 0.494225\n",
      "Loss after iteration 3991: 0.492675\n",
      "Loss after iteration 3992: 0.494205\n",
      "Loss after iteration 3993: 0.492655\n",
      "Loss after iteration 3994: 0.494184\n",
      "Loss after iteration 3995: 0.492634\n",
      "Loss after iteration 3996: 0.494164\n",
      "Loss after iteration 3997: 0.492614\n",
      "Loss after iteration 3998: 0.494144\n",
      "Loss after iteration 3999: 0.492594\n",
      "Loss after iteration 4000: 0.494124\n",
      "Loss after iteration 4001: 0.492574\n",
      "Loss after iteration 4002: 0.494104\n",
      "Loss after iteration 4003: 0.492554\n",
      "Loss after iteration 4004: 0.494083\n",
      "Loss after iteration 4005: 0.492534\n",
      "Loss after iteration 4006: 0.494063\n",
      "Loss after iteration 4007: 0.492514\n",
      "Loss after iteration 4008: 0.494043\n",
      "Loss after iteration 4009: 0.492494\n",
      "Loss after iteration 4010: 0.494023\n",
      "Loss after iteration 4011: 0.492474\n",
      "Loss after iteration 4012: 0.494003\n",
      "Loss after iteration 4013: 0.492454\n",
      "Loss after iteration 4014: 0.493983\n",
      "Loss after iteration 4015: 0.492434\n",
      "Loss after iteration 4016: 0.493963\n",
      "Loss after iteration 4017: 0.492415\n",
      "Loss after iteration 4018: 0.493943\n",
      "Loss after iteration 4019: 0.492395\n",
      "Loss after iteration 4020: 0.493923\n",
      "Loss after iteration 4021: 0.492375\n",
      "Loss after iteration 4022: 0.493903\n",
      "Loss after iteration 4023: 0.492355\n",
      "Loss after iteration 4024: 0.493883\n",
      "Loss after iteration 4025: 0.492335\n",
      "Loss after iteration 4026: 0.493863\n",
      "Loss after iteration 4027: 0.492315\n",
      "Loss after iteration 4028: 0.493843\n",
      "Loss after iteration 4029: 0.492295\n",
      "Loss after iteration 4030: 0.493823\n",
      "Loss after iteration 4031: 0.492275\n",
      "Loss after iteration 4032: 0.493803\n",
      "Loss after iteration 4033: 0.492256\n",
      "Loss after iteration 4034: 0.493783\n",
      "Loss after iteration 4035: 0.492236\n",
      "Loss after iteration 4036: 0.493763\n",
      "Loss after iteration 4037: 0.492216\n",
      "Loss after iteration 4038: 0.493743\n",
      "Loss after iteration 4039: 0.492196\n",
      "Loss after iteration 4040: 0.493723\n",
      "Loss after iteration 4041: 0.492177\n",
      "Loss after iteration 4042: 0.493703\n",
      "Loss after iteration 4043: 0.492157\n",
      "Loss after iteration 4044: 0.493683\n",
      "Loss after iteration 4045: 0.492137\n",
      "Loss after iteration 4046: 0.493663\n",
      "Loss after iteration 4047: 0.492118\n",
      "Loss after iteration 4048: 0.493644\n",
      "Loss after iteration 4049: 0.492098\n",
      "Loss after iteration 4050: 0.493624\n",
      "Loss after iteration 4051: 0.492078\n",
      "Loss after iteration 4052: 0.493604\n",
      "Loss after iteration 4053: 0.492059\n",
      "Loss after iteration 4054: 0.493584\n",
      "Loss after iteration 4055: 0.492039\n",
      "Loss after iteration 4056: 0.493564\n",
      "Loss after iteration 4057: 0.492019\n",
      "Loss after iteration 4058: 0.493545\n",
      "Loss after iteration 4059: 0.492000\n",
      "Loss after iteration 4060: 0.493525\n",
      "Loss after iteration 4061: 0.491980\n",
      "Loss after iteration 4062: 0.493505\n",
      "Loss after iteration 4063: 0.491961\n",
      "Loss after iteration 4064: 0.493485\n",
      "Loss after iteration 4065: 0.491941\n",
      "Loss after iteration 4066: 0.493466\n",
      "Loss after iteration 4067: 0.491921\n",
      "Loss after iteration 4068: 0.493446\n",
      "Loss after iteration 4069: 0.491902\n",
      "Loss after iteration 4070: 0.493426\n",
      "Loss after iteration 4071: 0.491882\n",
      "Loss after iteration 4072: 0.493407\n",
      "Loss after iteration 4073: 0.491863\n",
      "Loss after iteration 4074: 0.493387\n",
      "Loss after iteration 4075: 0.491843\n",
      "Loss after iteration 4076: 0.493367\n",
      "Loss after iteration 4077: 0.491824\n",
      "Loss after iteration 4078: 0.493348\n",
      "Loss after iteration 4079: 0.491805\n",
      "Loss after iteration 4080: 0.493328\n",
      "Loss after iteration 4081: 0.491785\n",
      "Loss after iteration 4082: 0.493309\n",
      "Loss after iteration 4083: 0.491766\n",
      "Loss after iteration 4084: 0.493289\n",
      "Loss after iteration 4085: 0.491746\n",
      "Loss after iteration 4086: 0.493269\n",
      "Loss after iteration 4087: 0.491727\n",
      "Loss after iteration 4088: 0.493250\n",
      "Loss after iteration 4089: 0.491707\n",
      "Loss after iteration 4090: 0.493230\n",
      "Loss after iteration 4091: 0.491688\n",
      "Loss after iteration 4092: 0.493211\n",
      "Loss after iteration 4093: 0.491669\n",
      "Loss after iteration 4094: 0.493191\n",
      "Loss after iteration 4095: 0.491649\n",
      "Loss after iteration 4096: 0.493172\n",
      "Loss after iteration 4097: 0.491630\n",
      "Loss after iteration 4098: 0.493152\n",
      "Loss after iteration 4099: 0.491611\n",
      "Loss after iteration 4100: 0.493133\n",
      "Loss after iteration 4101: 0.491592\n",
      "Loss after iteration 4102: 0.493114\n",
      "Loss after iteration 4103: 0.491572\n",
      "Loss after iteration 4104: 0.493094\n",
      "Loss after iteration 4105: 0.491553\n",
      "Loss after iteration 4106: 0.493075\n",
      "Loss after iteration 4107: 0.491534\n",
      "Loss after iteration 4108: 0.493055\n",
      "Loss after iteration 4109: 0.491515\n",
      "Loss after iteration 4110: 0.493036\n",
      "Loss after iteration 4111: 0.491495\n",
      "Loss after iteration 4112: 0.493017\n",
      "Loss after iteration 4113: 0.491476\n",
      "Loss after iteration 4114: 0.492997\n",
      "Loss after iteration 4115: 0.491457\n",
      "Loss after iteration 4116: 0.492978\n",
      "Loss after iteration 4117: 0.491438\n",
      "Loss after iteration 4118: 0.492959\n",
      "Loss after iteration 4119: 0.491419\n",
      "Loss after iteration 4120: 0.492939\n",
      "Loss after iteration 4121: 0.491399\n",
      "Loss after iteration 4122: 0.492920\n",
      "Loss after iteration 4123: 0.491380\n",
      "Loss after iteration 4124: 0.492901\n",
      "Loss after iteration 4125: 0.491361\n",
      "Loss after iteration 4126: 0.492882\n",
      "Loss after iteration 4127: 0.491342\n",
      "Loss after iteration 4128: 0.492862\n",
      "Loss after iteration 4129: 0.491323\n",
      "Loss after iteration 4130: 0.492843\n",
      "Loss after iteration 4131: 0.491304\n",
      "Loss after iteration 4132: 0.492824\n",
      "Loss after iteration 4133: 0.491285\n",
      "Loss after iteration 4134: 0.492805\n",
      "Loss after iteration 4135: 0.491266\n",
      "Loss after iteration 4136: 0.492785\n",
      "Loss after iteration 4137: 0.491247\n",
      "Loss after iteration 4138: 0.492766\n",
      "Loss after iteration 4139: 0.491228\n",
      "Loss after iteration 4140: 0.492747\n",
      "Loss after iteration 4141: 0.491209\n",
      "Loss after iteration 4142: 0.492728\n",
      "Loss after iteration 4143: 0.491190\n",
      "Loss after iteration 4144: 0.492709\n",
      "Loss after iteration 4145: 0.491171\n",
      "Loss after iteration 4146: 0.492690\n",
      "Loss after iteration 4147: 0.491152\n",
      "Loss after iteration 4148: 0.492671\n",
      "Loss after iteration 4149: 0.491133\n",
      "Loss after iteration 4150: 0.492652\n",
      "Loss after iteration 4151: 0.491114\n",
      "Loss after iteration 4152: 0.492632\n",
      "Loss after iteration 4153: 0.491095\n",
      "Loss after iteration 4154: 0.492613\n",
      "Loss after iteration 4155: 0.491076\n",
      "Loss after iteration 4156: 0.492594\n",
      "Loss after iteration 4157: 0.491057\n",
      "Loss after iteration 4158: 0.492575\n",
      "Loss after iteration 4159: 0.491039\n",
      "Loss after iteration 4160: 0.492556\n",
      "Loss after iteration 4161: 0.491020\n",
      "Loss after iteration 4162: 0.492537\n",
      "Loss after iteration 4163: 0.491001\n",
      "Loss after iteration 4164: 0.492518\n",
      "Loss after iteration 4165: 0.490982\n",
      "Loss after iteration 4166: 0.492499\n",
      "Loss after iteration 4167: 0.490963\n",
      "Loss after iteration 4168: 0.492480\n",
      "Loss after iteration 4169: 0.490944\n",
      "Loss after iteration 4170: 0.492461\n",
      "Loss after iteration 4171: 0.490926\n",
      "Loss after iteration 4172: 0.492443\n",
      "Loss after iteration 4173: 0.490907\n",
      "Loss after iteration 4174: 0.492424\n",
      "Loss after iteration 4175: 0.490888\n",
      "Loss after iteration 4176: 0.492405\n",
      "Loss after iteration 4177: 0.490869\n",
      "Loss after iteration 4178: 0.492386\n",
      "Loss after iteration 4179: 0.490851\n",
      "Loss after iteration 4180: 0.492367\n",
      "Loss after iteration 4181: 0.490832\n",
      "Loss after iteration 4182: 0.492348\n",
      "Loss after iteration 4183: 0.490813\n",
      "Loss after iteration 4184: 0.492329\n",
      "Loss after iteration 4185: 0.490795\n",
      "Loss after iteration 4186: 0.492310\n",
      "Loss after iteration 4187: 0.490776\n",
      "Loss after iteration 4188: 0.492292\n",
      "Loss after iteration 4189: 0.490757\n",
      "Loss after iteration 4190: 0.492273\n",
      "Loss after iteration 4191: 0.490739\n",
      "Loss after iteration 4192: 0.492254\n",
      "Loss after iteration 4193: 0.490720\n",
      "Loss after iteration 4194: 0.492235\n",
      "Loss after iteration 4195: 0.490701\n",
      "Loss after iteration 4196: 0.492216\n",
      "Loss after iteration 4197: 0.490683\n",
      "Loss after iteration 4198: 0.492198\n",
      "Loss after iteration 4199: 0.490664\n",
      "Loss after iteration 4200: 0.492179\n",
      "Loss after iteration 4201: 0.490646\n",
      "Loss after iteration 4202: 0.492160\n",
      "Loss after iteration 4203: 0.490627\n",
      "Loss after iteration 4204: 0.492142\n",
      "Loss after iteration 4205: 0.490609\n",
      "Loss after iteration 4206: 0.492123\n",
      "Loss after iteration 4207: 0.490590\n",
      "Loss after iteration 4208: 0.492104\n",
      "Loss after iteration 4209: 0.490572\n",
      "Loss after iteration 4210: 0.492086\n",
      "Loss after iteration 4211: 0.490553\n",
      "Loss after iteration 4212: 0.492067\n",
      "Loss after iteration 4213: 0.490535\n",
      "Loss after iteration 4214: 0.492048\n",
      "Loss after iteration 4215: 0.490516\n",
      "Loss after iteration 4216: 0.492030\n",
      "Loss after iteration 4217: 0.490498\n",
      "Loss after iteration 4218: 0.492011\n",
      "Loss after iteration 4219: 0.490479\n",
      "Loss after iteration 4220: 0.491992\n",
      "Loss after iteration 4221: 0.490461\n",
      "Loss after iteration 4222: 0.491974\n",
      "Loss after iteration 4223: 0.490442\n",
      "Loss after iteration 4224: 0.491955\n",
      "Loss after iteration 4225: 0.490424\n",
      "Loss after iteration 4226: 0.491937\n",
      "Loss after iteration 4227: 0.490406\n",
      "Loss after iteration 4228: 0.491918\n",
      "Loss after iteration 4229: 0.490387\n",
      "Loss after iteration 4230: 0.491900\n",
      "Loss after iteration 4231: 0.490369\n",
      "Loss after iteration 4232: 0.491881\n",
      "Loss after iteration 4233: 0.490351\n",
      "Loss after iteration 4234: 0.491863\n",
      "Loss after iteration 4235: 0.490332\n",
      "Loss after iteration 4236: 0.491844\n",
      "Loss after iteration 4237: 0.490314\n",
      "Loss after iteration 4238: 0.491826\n",
      "Loss after iteration 4239: 0.490296\n",
      "Loss after iteration 4240: 0.491807\n",
      "Loss after iteration 4241: 0.490277\n",
      "Loss after iteration 4242: 0.491789\n",
      "Loss after iteration 4243: 0.490259\n",
      "Loss after iteration 4244: 0.491770\n",
      "Loss after iteration 4245: 0.490241\n",
      "Loss after iteration 4246: 0.491752\n",
      "Loss after iteration 4247: 0.490222\n",
      "Loss after iteration 4248: 0.491734\n",
      "Loss after iteration 4249: 0.490204\n",
      "Loss after iteration 4250: 0.491715\n",
      "Loss after iteration 4251: 0.490186\n",
      "Loss after iteration 4252: 0.491697\n",
      "Loss after iteration 4253: 0.490168\n",
      "Loss after iteration 4254: 0.491678\n",
      "Loss after iteration 4255: 0.490150\n",
      "Loss after iteration 4256: 0.491660\n",
      "Loss after iteration 4257: 0.490131\n",
      "Loss after iteration 4258: 0.491642\n",
      "Loss after iteration 4259: 0.490113\n",
      "Loss after iteration 4260: 0.491624\n",
      "Loss after iteration 4261: 0.490095\n",
      "Loss after iteration 4262: 0.491605\n",
      "Loss after iteration 4263: 0.490077\n",
      "Loss after iteration 4264: 0.491587\n",
      "Loss after iteration 4265: 0.490059\n",
      "Loss after iteration 4266: 0.491569\n",
      "Loss after iteration 4267: 0.490041\n",
      "Loss after iteration 4268: 0.491550\n",
      "Loss after iteration 4269: 0.490023\n",
      "Loss after iteration 4270: 0.491532\n",
      "Loss after iteration 4271: 0.490005\n",
      "Loss after iteration 4272: 0.491514\n",
      "Loss after iteration 4273: 0.489987\n",
      "Loss after iteration 4274: 0.491496\n",
      "Loss after iteration 4275: 0.489968\n",
      "Loss after iteration 4276: 0.491477\n",
      "Loss after iteration 4277: 0.489950\n",
      "Loss after iteration 4278: 0.491459\n",
      "Loss after iteration 4279: 0.489932\n",
      "Loss after iteration 4280: 0.491441\n",
      "Loss after iteration 4281: 0.489914\n",
      "Loss after iteration 4282: 0.491423\n",
      "Loss after iteration 4283: 0.489896\n",
      "Loss after iteration 4284: 0.491405\n",
      "Loss after iteration 4285: 0.489878\n",
      "Loss after iteration 4286: 0.491387\n",
      "Loss after iteration 4287: 0.489860\n",
      "Loss after iteration 4288: 0.491368\n",
      "Loss after iteration 4289: 0.489842\n",
      "Loss after iteration 4290: 0.491350\n",
      "Loss after iteration 4291: 0.489825\n",
      "Loss after iteration 4292: 0.491332\n",
      "Loss after iteration 4293: 0.489807\n",
      "Loss after iteration 4294: 0.491314\n",
      "Loss after iteration 4295: 0.489789\n",
      "Loss after iteration 4296: 0.491296\n",
      "Loss after iteration 4297: 0.489771\n",
      "Loss after iteration 4298: 0.491278\n",
      "Loss after iteration 4299: 0.489753\n",
      "Loss after iteration 4300: 0.491260\n",
      "Loss after iteration 4301: 0.489735\n",
      "Loss after iteration 4302: 0.491242\n",
      "Loss after iteration 4303: 0.489717\n",
      "Loss after iteration 4304: 0.491224\n",
      "Loss after iteration 4305: 0.489699\n",
      "Loss after iteration 4306: 0.491206\n",
      "Loss after iteration 4307: 0.489681\n",
      "Loss after iteration 4308: 0.491188\n",
      "Loss after iteration 4309: 0.489664\n",
      "Loss after iteration 4310: 0.491170\n",
      "Loss after iteration 4311: 0.489646\n",
      "Loss after iteration 4312: 0.491152\n",
      "Loss after iteration 4313: 0.489628\n",
      "Loss after iteration 4314: 0.491134\n",
      "Loss after iteration 4315: 0.489610\n",
      "Loss after iteration 4316: 0.491116\n",
      "Loss after iteration 4317: 0.489592\n",
      "Loss after iteration 4318: 0.491098\n",
      "Loss after iteration 4319: 0.489575\n",
      "Loss after iteration 4320: 0.491080\n",
      "Loss after iteration 4321: 0.489557\n",
      "Loss after iteration 4322: 0.491062\n",
      "Loss after iteration 4323: 0.489539\n",
      "Loss after iteration 4324: 0.491044\n",
      "Loss after iteration 4325: 0.489521\n",
      "Loss after iteration 4326: 0.491027\n",
      "Loss after iteration 4327: 0.489504\n",
      "Loss after iteration 4328: 0.491009\n",
      "Loss after iteration 4329: 0.489486\n",
      "Loss after iteration 4330: 0.490991\n",
      "Loss after iteration 4331: 0.489468\n",
      "Loss after iteration 4332: 0.490973\n",
      "Loss after iteration 4333: 0.489451\n",
      "Loss after iteration 4334: 0.490955\n",
      "Loss after iteration 4335: 0.489433\n",
      "Loss after iteration 4336: 0.490937\n",
      "Loss after iteration 4337: 0.489415\n",
      "Loss after iteration 4338: 0.490920\n",
      "Loss after iteration 4339: 0.489398\n",
      "Loss after iteration 4340: 0.490902\n",
      "Loss after iteration 4341: 0.489380\n",
      "Loss after iteration 4342: 0.490884\n",
      "Loss after iteration 4343: 0.489363\n",
      "Loss after iteration 4344: 0.490866\n",
      "Loss after iteration 4345: 0.489345\n",
      "Loss after iteration 4346: 0.490849\n",
      "Loss after iteration 4347: 0.489327\n",
      "Loss after iteration 4348: 0.490831\n",
      "Loss after iteration 4349: 0.489310\n",
      "Loss after iteration 4350: 0.490813\n",
      "Loss after iteration 4351: 0.489292\n",
      "Loss after iteration 4352: 0.490795\n",
      "Loss after iteration 4353: 0.489275\n",
      "Loss after iteration 4354: 0.490778\n",
      "Loss after iteration 4355: 0.489257\n",
      "Loss after iteration 4356: 0.490760\n",
      "Loss after iteration 4357: 0.489240\n",
      "Loss after iteration 4358: 0.490742\n",
      "Loss after iteration 4359: 0.489222\n",
      "Loss after iteration 4360: 0.490725\n",
      "Loss after iteration 4361: 0.489205\n",
      "Loss after iteration 4362: 0.490707\n",
      "Loss after iteration 4363: 0.489187\n",
      "Loss after iteration 4364: 0.490689\n",
      "Loss after iteration 4365: 0.489170\n",
      "Loss after iteration 4366: 0.490672\n",
      "Loss after iteration 4367: 0.489152\n",
      "Loss after iteration 4368: 0.490654\n",
      "Loss after iteration 4369: 0.489135\n",
      "Loss after iteration 4370: 0.490637\n",
      "Loss after iteration 4371: 0.489117\n",
      "Loss after iteration 4372: 0.490619\n",
      "Loss after iteration 4373: 0.489100\n",
      "Loss after iteration 4374: 0.490601\n",
      "Loss after iteration 4375: 0.489083\n",
      "Loss after iteration 4376: 0.490584\n",
      "Loss after iteration 4377: 0.489065\n",
      "Loss after iteration 4378: 0.490566\n",
      "Loss after iteration 4379: 0.489048\n",
      "Loss after iteration 4380: 0.490549\n",
      "Loss after iteration 4381: 0.489031\n",
      "Loss after iteration 4382: 0.490531\n",
      "Loss after iteration 4383: 0.489013\n",
      "Loss after iteration 4384: 0.490514\n",
      "Loss after iteration 4385: 0.488996\n",
      "Loss after iteration 4386: 0.490496\n",
      "Loss after iteration 4387: 0.488979\n",
      "Loss after iteration 4388: 0.490479\n",
      "Loss after iteration 4389: 0.488961\n",
      "Loss after iteration 4390: 0.490461\n",
      "Loss after iteration 4391: 0.488944\n",
      "Loss after iteration 4392: 0.490444\n",
      "Loss after iteration 4393: 0.488927\n",
      "Loss after iteration 4394: 0.490426\n",
      "Loss after iteration 4395: 0.488909\n",
      "Loss after iteration 4396: 0.490409\n",
      "Loss after iteration 4397: 0.488892\n",
      "Loss after iteration 4398: 0.490392\n",
      "Loss after iteration 4399: 0.488875\n",
      "Loss after iteration 4400: 0.490374\n",
      "Loss after iteration 4401: 0.488858\n",
      "Loss after iteration 4402: 0.490357\n",
      "Loss after iteration 4403: 0.488840\n",
      "Loss after iteration 4404: 0.490339\n",
      "Loss after iteration 4405: 0.488823\n",
      "Loss after iteration 4406: 0.490322\n",
      "Loss after iteration 4407: 0.488806\n",
      "Loss after iteration 4408: 0.490305\n",
      "Loss after iteration 4409: 0.488789\n",
      "Loss after iteration 4410: 0.490287\n",
      "Loss after iteration 4411: 0.488772\n",
      "Loss after iteration 4412: 0.490270\n",
      "Loss after iteration 4413: 0.488755\n",
      "Loss after iteration 4414: 0.490253\n",
      "Loss after iteration 4415: 0.488737\n",
      "Loss after iteration 4416: 0.490235\n",
      "Loss after iteration 4417: 0.488720\n",
      "Loss after iteration 4418: 0.490218\n",
      "Loss after iteration 4419: 0.488703\n",
      "Loss after iteration 4420: 0.490201\n",
      "Loss after iteration 4421: 0.488686\n",
      "Loss after iteration 4422: 0.490184\n",
      "Loss after iteration 4423: 0.488669\n",
      "Loss after iteration 4424: 0.490166\n",
      "Loss after iteration 4425: 0.488652\n",
      "Loss after iteration 4426: 0.490149\n",
      "Loss after iteration 4427: 0.488635\n",
      "Loss after iteration 4428: 0.490132\n",
      "Loss after iteration 4429: 0.488618\n",
      "Loss after iteration 4430: 0.490115\n",
      "Loss after iteration 4431: 0.488601\n",
      "Loss after iteration 4432: 0.490098\n",
      "Loss after iteration 4433: 0.488584\n",
      "Loss after iteration 4434: 0.490080\n",
      "Loss after iteration 4435: 0.488567\n",
      "Loss after iteration 4436: 0.490063\n",
      "Loss after iteration 4437: 0.488550\n",
      "Loss after iteration 4438: 0.490046\n",
      "Loss after iteration 4439: 0.488533\n",
      "Loss after iteration 4440: 0.490029\n",
      "Loss after iteration 4441: 0.488516\n",
      "Loss after iteration 4442: 0.490012\n",
      "Loss after iteration 4443: 0.488499\n",
      "Loss after iteration 4444: 0.489995\n",
      "Loss after iteration 4445: 0.488482\n",
      "Loss after iteration 4446: 0.489978\n",
      "Loss after iteration 4447: 0.488465\n",
      "Loss after iteration 4448: 0.489960\n",
      "Loss after iteration 4449: 0.488448\n",
      "Loss after iteration 4450: 0.489943\n",
      "Loss after iteration 4451: 0.488431\n",
      "Loss after iteration 4452: 0.489926\n",
      "Loss after iteration 4453: 0.488414\n",
      "Loss after iteration 4454: 0.489909\n",
      "Loss after iteration 4455: 0.488397\n",
      "Loss after iteration 4456: 0.489892\n",
      "Loss after iteration 4457: 0.488380\n",
      "Loss after iteration 4458: 0.489875\n",
      "Loss after iteration 4459: 0.488363\n",
      "Loss after iteration 4460: 0.489858\n",
      "Loss after iteration 4461: 0.488347\n",
      "Loss after iteration 4462: 0.489841\n",
      "Loss after iteration 4463: 0.488330\n",
      "Loss after iteration 4464: 0.489824\n",
      "Loss after iteration 4465: 0.488313\n",
      "Loss after iteration 4466: 0.489807\n",
      "Loss after iteration 4467: 0.488296\n",
      "Loss after iteration 4468: 0.489790\n",
      "Loss after iteration 4469: 0.488279\n",
      "Loss after iteration 4470: 0.489773\n",
      "Loss after iteration 4471: 0.488263\n",
      "Loss after iteration 4472: 0.489756\n",
      "Loss after iteration 4473: 0.488246\n",
      "Loss after iteration 4474: 0.489739\n",
      "Loss after iteration 4475: 0.488229\n",
      "Loss after iteration 4476: 0.489722\n",
      "Loss after iteration 4477: 0.488212\n",
      "Loss after iteration 4478: 0.489705\n",
      "Loss after iteration 4479: 0.488195\n",
      "Loss after iteration 4480: 0.489688\n",
      "Loss after iteration 4481: 0.488179\n",
      "Loss after iteration 4482: 0.489672\n",
      "Loss after iteration 4483: 0.488162\n",
      "Loss after iteration 4484: 0.489655\n",
      "Loss after iteration 4485: 0.488145\n",
      "Loss after iteration 4486: 0.489638\n",
      "Loss after iteration 4487: 0.488129\n",
      "Loss after iteration 4488: 0.489621\n",
      "Loss after iteration 4489: 0.488112\n",
      "Loss after iteration 4490: 0.489604\n",
      "Loss after iteration 4491: 0.488095\n",
      "Loss after iteration 4492: 0.489587\n",
      "Loss after iteration 4493: 0.488079\n",
      "Loss after iteration 4494: 0.489570\n",
      "Loss after iteration 4495: 0.488062\n",
      "Loss after iteration 4496: 0.489554\n",
      "Loss after iteration 4497: 0.488045\n",
      "Loss after iteration 4498: 0.489537\n",
      "Loss after iteration 4499: 0.488029\n",
      "Loss after iteration 4500: 0.489520\n",
      "Loss after iteration 4501: 0.488012\n",
      "Loss after iteration 4502: 0.489503\n",
      "Loss after iteration 4503: 0.487995\n",
      "Loss after iteration 4504: 0.489487\n",
      "Loss after iteration 4505: 0.487979\n",
      "Loss after iteration 4506: 0.489470\n",
      "Loss after iteration 4507: 0.487962\n",
      "Loss after iteration 4508: 0.489453\n",
      "Loss after iteration 4509: 0.487946\n",
      "Loss after iteration 4510: 0.489436\n",
      "Loss after iteration 4511: 0.487929\n",
      "Loss after iteration 4512: 0.489420\n",
      "Loss after iteration 4513: 0.487913\n",
      "Loss after iteration 4514: 0.489403\n",
      "Loss after iteration 4515: 0.487896\n",
      "Loss after iteration 4516: 0.489386\n",
      "Loss after iteration 4517: 0.487880\n",
      "Loss after iteration 4518: 0.489370\n",
      "Loss after iteration 4519: 0.487863\n",
      "Loss after iteration 4520: 0.489353\n",
      "Loss after iteration 4521: 0.487847\n",
      "Loss after iteration 4522: 0.489336\n",
      "Loss after iteration 4523: 0.487830\n",
      "Loss after iteration 4524: 0.489320\n",
      "Loss after iteration 4525: 0.487814\n",
      "Loss after iteration 4526: 0.489303\n",
      "Loss after iteration 4527: 0.487797\n",
      "Loss after iteration 4528: 0.489286\n",
      "Loss after iteration 4529: 0.487781\n",
      "Loss after iteration 4530: 0.489270\n",
      "Loss after iteration 4531: 0.487764\n",
      "Loss after iteration 4532: 0.489253\n",
      "Loss after iteration 4533: 0.487748\n",
      "Loss after iteration 4534: 0.489237\n",
      "Loss after iteration 4535: 0.487731\n",
      "Loss after iteration 4536: 0.489220\n",
      "Loss after iteration 4537: 0.487715\n",
      "Loss after iteration 4538: 0.489204\n",
      "Loss after iteration 4539: 0.487699\n",
      "Loss after iteration 4540: 0.489187\n",
      "Loss after iteration 4541: 0.487682\n",
      "Loss after iteration 4542: 0.489170\n",
      "Loss after iteration 4543: 0.487666\n",
      "Loss after iteration 4544: 0.489154\n",
      "Loss after iteration 4545: 0.487650\n",
      "Loss after iteration 4546: 0.489137\n",
      "Loss after iteration 4547: 0.487633\n",
      "Loss after iteration 4548: 0.489121\n",
      "Loss after iteration 4549: 0.487617\n",
      "Loss after iteration 4550: 0.489104\n",
      "Loss after iteration 4551: 0.487601\n",
      "Loss after iteration 4552: 0.489088\n",
      "Loss after iteration 4553: 0.487584\n",
      "Loss after iteration 4554: 0.489071\n",
      "Loss after iteration 4555: 0.487568\n",
      "Loss after iteration 4556: 0.489055\n",
      "Loss after iteration 4557: 0.487552\n",
      "Loss after iteration 4558: 0.489039\n",
      "Loss after iteration 4559: 0.487535\n",
      "Loss after iteration 4560: 0.489022\n",
      "Loss after iteration 4561: 0.487519\n",
      "Loss after iteration 4562: 0.489006\n",
      "Loss after iteration 4563: 0.487503\n",
      "Loss after iteration 4564: 0.488989\n",
      "Loss after iteration 4565: 0.487487\n",
      "Loss after iteration 4566: 0.488973\n",
      "Loss after iteration 4567: 0.487470\n",
      "Loss after iteration 4568: 0.488957\n",
      "Loss after iteration 4569: 0.487454\n",
      "Loss after iteration 4570: 0.488940\n",
      "Loss after iteration 4571: 0.487438\n",
      "Loss after iteration 4572: 0.488924\n",
      "Loss after iteration 4573: 0.487422\n",
      "Loss after iteration 4574: 0.488908\n",
      "Loss after iteration 4575: 0.487406\n",
      "Loss after iteration 4576: 0.488891\n",
      "Loss after iteration 4577: 0.487390\n",
      "Loss after iteration 4578: 0.488875\n",
      "Loss after iteration 4579: 0.487373\n",
      "Loss after iteration 4580: 0.488859\n",
      "Loss after iteration 4581: 0.487357\n",
      "Loss after iteration 4582: 0.488842\n",
      "Loss after iteration 4583: 0.487341\n",
      "Loss after iteration 4584: 0.488826\n",
      "Loss after iteration 4585: 0.487325\n",
      "Loss after iteration 4586: 0.488810\n",
      "Loss after iteration 4587: 0.487309\n",
      "Loss after iteration 4588: 0.488793\n",
      "Loss after iteration 4589: 0.487293\n",
      "Loss after iteration 4590: 0.488777\n",
      "Loss after iteration 4591: 0.487277\n",
      "Loss after iteration 4592: 0.488761\n",
      "Loss after iteration 4593: 0.487261\n",
      "Loss after iteration 4594: 0.488745\n",
      "Loss after iteration 4595: 0.487245\n",
      "Loss after iteration 4596: 0.488729\n",
      "Loss after iteration 4597: 0.487229\n",
      "Loss after iteration 4598: 0.488712\n",
      "Loss after iteration 4599: 0.487212\n",
      "Loss after iteration 4600: 0.488696\n",
      "Loss after iteration 4601: 0.487196\n",
      "Loss after iteration 4602: 0.488680\n",
      "Loss after iteration 4603: 0.487180\n",
      "Loss after iteration 4604: 0.488664\n",
      "Loss after iteration 4605: 0.487164\n",
      "Loss after iteration 4606: 0.488648\n",
      "Loss after iteration 4607: 0.487148\n",
      "Loss after iteration 4608: 0.488631\n",
      "Loss after iteration 4609: 0.487132\n",
      "Loss after iteration 4610: 0.488615\n",
      "Loss after iteration 4611: 0.487116\n",
      "Loss after iteration 4612: 0.488599\n",
      "Loss after iteration 4613: 0.487101\n",
      "Loss after iteration 4614: 0.488583\n",
      "Loss after iteration 4615: 0.487085\n",
      "Loss after iteration 4616: 0.488567\n",
      "Loss after iteration 4617: 0.487069\n",
      "Loss after iteration 4618: 0.488551\n",
      "Loss after iteration 4619: 0.487053\n",
      "Loss after iteration 4620: 0.488535\n",
      "Loss after iteration 4621: 0.487037\n",
      "Loss after iteration 4622: 0.488519\n",
      "Loss after iteration 4623: 0.487021\n",
      "Loss after iteration 4624: 0.488503\n",
      "Loss after iteration 4625: 0.487005\n",
      "Loss after iteration 4626: 0.488487\n",
      "Loss after iteration 4627: 0.486989\n",
      "Loss after iteration 4628: 0.488471\n",
      "Loss after iteration 4629: 0.486973\n",
      "Loss after iteration 4630: 0.488455\n",
      "Loss after iteration 4631: 0.486957\n",
      "Loss after iteration 4632: 0.488439\n",
      "Loss after iteration 4633: 0.486942\n",
      "Loss after iteration 4634: 0.488423\n",
      "Loss after iteration 4635: 0.486926\n",
      "Loss after iteration 4636: 0.488407\n",
      "Loss after iteration 4637: 0.486910\n",
      "Loss after iteration 4638: 0.488391\n",
      "Loss after iteration 4639: 0.486894\n",
      "Loss after iteration 4640: 0.488375\n",
      "Loss after iteration 4641: 0.486878\n",
      "Loss after iteration 4642: 0.488359\n",
      "Loss after iteration 4643: 0.486862\n",
      "Loss after iteration 4644: 0.488343\n",
      "Loss after iteration 4645: 0.486847\n",
      "Loss after iteration 4646: 0.488327\n",
      "Loss after iteration 4647: 0.486831\n",
      "Loss after iteration 4648: 0.488311\n",
      "Loss after iteration 4649: 0.486815\n",
      "Loss after iteration 4650: 0.488295\n",
      "Loss after iteration 4651: 0.486799\n",
      "Loss after iteration 4652: 0.488279\n",
      "Loss after iteration 4653: 0.486784\n",
      "Loss after iteration 4654: 0.488263\n",
      "Loss after iteration 4655: 0.486768\n",
      "Loss after iteration 4656: 0.488247\n",
      "Loss after iteration 4657: 0.486752\n",
      "Loss after iteration 4658: 0.488231\n",
      "Loss after iteration 4659: 0.486737\n",
      "Loss after iteration 4660: 0.488216\n",
      "Loss after iteration 4661: 0.486721\n",
      "Loss after iteration 4662: 0.488200\n",
      "Loss after iteration 4663: 0.486705\n",
      "Loss after iteration 4664: 0.488184\n",
      "Loss after iteration 4665: 0.486690\n",
      "Loss after iteration 4666: 0.488168\n",
      "Loss after iteration 4667: 0.486674\n",
      "Loss after iteration 4668: 0.488152\n",
      "Loss after iteration 4669: 0.486658\n",
      "Loss after iteration 4670: 0.488136\n",
      "Loss after iteration 4671: 0.486643\n",
      "Loss after iteration 4672: 0.488121\n",
      "Loss after iteration 4673: 0.486627\n",
      "Loss after iteration 4674: 0.488105\n",
      "Loss after iteration 4675: 0.486611\n",
      "Loss after iteration 4676: 0.488089\n",
      "Loss after iteration 4677: 0.486596\n",
      "Loss after iteration 4678: 0.488073\n",
      "Loss after iteration 4679: 0.486580\n",
      "Loss after iteration 4680: 0.488058\n",
      "Loss after iteration 4681: 0.486565\n",
      "Loss after iteration 4682: 0.488042\n",
      "Loss after iteration 4683: 0.486549\n",
      "Loss after iteration 4684: 0.488026\n",
      "Loss after iteration 4685: 0.486533\n",
      "Loss after iteration 4686: 0.488010\n",
      "Loss after iteration 4687: 0.486518\n",
      "Loss after iteration 4688: 0.487995\n",
      "Loss after iteration 4689: 0.486502\n",
      "Loss after iteration 4690: 0.487979\n",
      "Loss after iteration 4691: 0.486487\n",
      "Loss after iteration 4692: 0.487963\n",
      "Loss after iteration 4693: 0.486471\n",
      "Loss after iteration 4694: 0.487948\n",
      "Loss after iteration 4695: 0.486456\n",
      "Loss after iteration 4696: 0.487932\n",
      "Loss after iteration 4697: 0.486440\n",
      "Loss after iteration 4698: 0.487916\n",
      "Loss after iteration 4699: 0.486425\n",
      "Loss after iteration 4700: 0.487901\n",
      "Loss after iteration 4701: 0.486409\n",
      "Loss after iteration 4702: 0.487885\n",
      "Loss after iteration 4703: 0.486394\n",
      "Loss after iteration 4704: 0.487870\n",
      "Loss after iteration 4705: 0.486378\n",
      "Loss after iteration 4706: 0.487854\n",
      "Loss after iteration 4707: 0.486363\n",
      "Loss after iteration 4708: 0.487838\n",
      "Loss after iteration 4709: 0.486348\n",
      "Loss after iteration 4710: 0.487823\n",
      "Loss after iteration 4711: 0.486332\n",
      "Loss after iteration 4712: 0.487807\n",
      "Loss after iteration 4713: 0.486317\n",
      "Loss after iteration 4714: 0.487792\n",
      "Loss after iteration 4715: 0.486301\n",
      "Loss after iteration 4716: 0.487776\n",
      "Loss after iteration 4717: 0.486286\n",
      "Loss after iteration 4718: 0.487761\n",
      "Loss after iteration 4719: 0.486271\n",
      "Loss after iteration 4720: 0.487745\n",
      "Loss after iteration 4721: 0.486255\n",
      "Loss after iteration 4722: 0.487730\n",
      "Loss after iteration 4723: 0.486240\n",
      "Loss after iteration 4724: 0.487714\n",
      "Loss after iteration 4725: 0.486225\n",
      "Loss after iteration 4726: 0.487699\n",
      "Loss after iteration 4727: 0.486209\n",
      "Loss after iteration 4728: 0.487683\n",
      "Loss after iteration 4729: 0.486194\n",
      "Loss after iteration 4730: 0.487668\n",
      "Loss after iteration 4731: 0.486179\n",
      "Loss after iteration 4732: 0.487652\n",
      "Loss after iteration 4733: 0.486163\n",
      "Loss after iteration 4734: 0.487637\n",
      "Loss after iteration 4735: 0.486148\n",
      "Loss after iteration 4736: 0.487621\n",
      "Loss after iteration 4737: 0.486133\n",
      "Loss after iteration 4738: 0.487606\n",
      "Loss after iteration 4739: 0.486117\n",
      "Loss after iteration 4740: 0.487590\n",
      "Loss after iteration 4741: 0.486102\n",
      "Loss after iteration 4742: 0.487575\n",
      "Loss after iteration 4743: 0.486087\n",
      "Loss after iteration 4744: 0.487560\n",
      "Loss after iteration 4745: 0.486072\n",
      "Loss after iteration 4746: 0.487544\n",
      "Loss after iteration 4747: 0.486057\n",
      "Loss after iteration 4748: 0.487529\n",
      "Loss after iteration 4749: 0.486041\n",
      "Loss after iteration 4750: 0.487514\n",
      "Loss after iteration 4751: 0.486026\n",
      "Loss after iteration 4752: 0.487498\n",
      "Loss after iteration 4753: 0.486011\n",
      "Loss after iteration 4754: 0.487483\n",
      "Loss after iteration 4755: 0.485996\n",
      "Loss after iteration 4756: 0.487468\n",
      "Loss after iteration 4757: 0.485981\n",
      "Loss after iteration 4758: 0.487452\n",
      "Loss after iteration 4759: 0.485965\n",
      "Loss after iteration 4760: 0.487437\n",
      "Loss after iteration 4761: 0.485950\n",
      "Loss after iteration 4762: 0.487422\n",
      "Loss after iteration 4763: 0.485935\n",
      "Loss after iteration 4764: 0.487406\n",
      "Loss after iteration 4765: 0.485920\n",
      "Loss after iteration 4766: 0.487391\n",
      "Loss after iteration 4767: 0.485905\n",
      "Loss after iteration 4768: 0.487376\n",
      "Loss after iteration 4769: 0.485890\n",
      "Loss after iteration 4770: 0.487361\n",
      "Loss after iteration 4771: 0.485875\n",
      "Loss after iteration 4772: 0.487345\n",
      "Loss after iteration 4773: 0.485860\n",
      "Loss after iteration 4774: 0.487330\n",
      "Loss after iteration 4775: 0.485845\n",
      "Loss after iteration 4776: 0.487315\n",
      "Loss after iteration 4777: 0.485829\n",
      "Loss after iteration 4778: 0.487300\n",
      "Loss after iteration 4779: 0.485814\n",
      "Loss after iteration 4780: 0.487284\n",
      "Loss after iteration 4781: 0.485799\n",
      "Loss after iteration 4782: 0.487269\n",
      "Loss after iteration 4783: 0.485784\n",
      "Loss after iteration 4784: 0.487254\n",
      "Loss after iteration 4785: 0.485769\n",
      "Loss after iteration 4786: 0.487239\n",
      "Loss after iteration 4787: 0.485754\n",
      "Loss after iteration 4788: 0.487224\n",
      "Loss after iteration 4789: 0.485739\n",
      "Loss after iteration 4790: 0.487209\n",
      "Loss after iteration 4791: 0.485724\n",
      "Loss after iteration 4792: 0.487193\n",
      "Loss after iteration 4793: 0.485709\n",
      "Loss after iteration 4794: 0.487178\n",
      "Loss after iteration 4795: 0.485694\n",
      "Loss after iteration 4796: 0.487163\n",
      "Loss after iteration 4797: 0.485679\n",
      "Loss after iteration 4798: 0.487148\n",
      "Loss after iteration 4799: 0.485664\n",
      "Loss after iteration 4800: 0.487133\n",
      "Loss after iteration 4801: 0.485649\n",
      "Loss after iteration 4802: 0.487118\n",
      "Loss after iteration 4803: 0.485635\n",
      "Loss after iteration 4804: 0.487103\n",
      "Loss after iteration 4805: 0.485620\n",
      "Loss after iteration 4806: 0.487088\n",
      "Loss after iteration 4807: 0.485605\n",
      "Loss after iteration 4808: 0.487073\n",
      "Loss after iteration 4809: 0.485590\n",
      "Loss after iteration 4810: 0.487058\n",
      "Loss after iteration 4811: 0.485575\n",
      "Loss after iteration 4812: 0.487043\n",
      "Loss after iteration 4813: 0.485560\n",
      "Loss after iteration 4814: 0.487028\n",
      "Loss after iteration 4815: 0.485545\n",
      "Loss after iteration 4816: 0.487013\n",
      "Loss after iteration 4817: 0.485530\n",
      "Loss after iteration 4818: 0.486998\n",
      "Loss after iteration 4819: 0.485515\n",
      "Loss after iteration 4820: 0.486983\n",
      "Loss after iteration 4821: 0.485501\n",
      "Loss after iteration 4822: 0.486968\n",
      "Loss after iteration 4823: 0.485486\n",
      "Loss after iteration 4824: 0.486953\n",
      "Loss after iteration 4825: 0.485471\n",
      "Loss after iteration 4826: 0.486938\n",
      "Loss after iteration 4827: 0.485456\n",
      "Loss after iteration 4828: 0.486923\n",
      "Loss after iteration 4829: 0.485441\n",
      "Loss after iteration 4830: 0.486908\n",
      "Loss after iteration 4831: 0.485427\n",
      "Loss after iteration 4832: 0.486893\n",
      "Loss after iteration 4833: 0.485412\n",
      "Loss after iteration 4834: 0.486878\n",
      "Loss after iteration 4835: 0.485397\n",
      "Loss after iteration 4836: 0.486863\n",
      "Loss after iteration 4837: 0.485382\n",
      "Loss after iteration 4838: 0.486848\n",
      "Loss after iteration 4839: 0.485368\n",
      "Loss after iteration 4840: 0.486833\n",
      "Loss after iteration 4841: 0.485353\n",
      "Loss after iteration 4842: 0.486818\n",
      "Loss after iteration 4843: 0.485338\n",
      "Loss after iteration 4844: 0.486804\n",
      "Loss after iteration 4845: 0.485323\n",
      "Loss after iteration 4846: 0.486789\n",
      "Loss after iteration 4847: 0.485309\n",
      "Loss after iteration 4848: 0.486774\n",
      "Loss after iteration 4849: 0.485294\n",
      "Loss after iteration 4850: 0.486759\n",
      "Loss after iteration 4851: 0.485279\n",
      "Loss after iteration 4852: 0.486744\n",
      "Loss after iteration 4853: 0.485265\n",
      "Loss after iteration 4854: 0.486729\n",
      "Loss after iteration 4855: 0.485250\n",
      "Loss after iteration 4856: 0.486715\n",
      "Loss after iteration 4857: 0.485235\n",
      "Loss after iteration 4858: 0.486700\n",
      "Loss after iteration 4859: 0.485221\n",
      "Loss after iteration 4860: 0.486685\n",
      "Loss after iteration 4861: 0.485206\n",
      "Loss after iteration 4862: 0.486670\n",
      "Loss after iteration 4863: 0.485191\n",
      "Loss after iteration 4864: 0.486655\n",
      "Loss after iteration 4865: 0.485177\n",
      "Loss after iteration 4866: 0.486641\n",
      "Loss after iteration 4867: 0.485162\n",
      "Loss after iteration 4868: 0.486626\n",
      "Loss after iteration 4869: 0.485148\n",
      "Loss after iteration 4870: 0.486611\n",
      "Loss after iteration 4871: 0.485133\n",
      "Loss after iteration 4872: 0.486597\n",
      "Loss after iteration 4873: 0.485118\n",
      "Loss after iteration 4874: 0.486582\n",
      "Loss after iteration 4875: 0.485104\n",
      "Loss after iteration 4876: 0.486567\n",
      "Loss after iteration 4877: 0.485089\n",
      "Loss after iteration 4878: 0.486552\n",
      "Loss after iteration 4879: 0.485075\n",
      "Loss after iteration 4880: 0.486538\n",
      "Loss after iteration 4881: 0.485060\n",
      "Loss after iteration 4882: 0.486523\n",
      "Loss after iteration 4883: 0.485046\n",
      "Loss after iteration 4884: 0.486508\n",
      "Loss after iteration 4885: 0.485031\n",
      "Loss after iteration 4886: 0.486494\n",
      "Loss after iteration 4887: 0.485017\n",
      "Loss after iteration 4888: 0.486479\n",
      "Loss after iteration 4889: 0.485002\n",
      "Loss after iteration 4890: 0.486465\n",
      "Loss after iteration 4891: 0.484988\n",
      "Loss after iteration 4892: 0.486450\n",
      "Loss after iteration 4893: 0.484973\n",
      "Loss after iteration 4894: 0.486435\n",
      "Loss after iteration 4895: 0.484959\n",
      "Loss after iteration 4896: 0.486421\n",
      "Loss after iteration 4897: 0.484944\n",
      "Loss after iteration 4898: 0.486406\n",
      "Loss after iteration 4899: 0.484930\n",
      "Loss after iteration 4900: 0.486392\n",
      "Loss after iteration 4901: 0.484915\n",
      "Loss after iteration 4902: 0.486377\n",
      "Loss after iteration 4903: 0.484901\n",
      "Loss after iteration 4904: 0.486362\n",
      "Loss after iteration 4905: 0.484887\n",
      "Loss after iteration 4906: 0.486348\n",
      "Loss after iteration 4907: 0.484872\n",
      "Loss after iteration 4908: 0.486333\n",
      "Loss after iteration 4909: 0.484858\n",
      "Loss after iteration 4910: 0.486319\n",
      "Loss after iteration 4911: 0.484843\n",
      "Loss after iteration 4912: 0.486304\n",
      "Loss after iteration 4913: 0.484829\n",
      "Loss after iteration 4914: 0.486290\n",
      "Loss after iteration 4915: 0.484815\n",
      "Loss after iteration 4916: 0.486275\n",
      "Loss after iteration 4917: 0.484800\n",
      "Loss after iteration 4918: 0.486261\n",
      "Loss after iteration 4919: 0.484786\n",
      "Loss after iteration 4920: 0.486246\n",
      "Loss after iteration 4921: 0.484772\n",
      "Loss after iteration 4922: 0.486232\n",
      "Loss after iteration 4923: 0.484757\n",
      "Loss after iteration 4924: 0.486217\n",
      "Loss after iteration 4925: 0.484743\n",
      "Loss after iteration 4926: 0.486203\n",
      "Loss after iteration 4927: 0.484729\n",
      "Loss after iteration 4928: 0.486188\n",
      "Loss after iteration 4929: 0.484714\n",
      "Loss after iteration 4930: 0.486174\n",
      "Loss after iteration 4931: 0.484700\n",
      "Loss after iteration 4932: 0.486160\n",
      "Loss after iteration 4933: 0.484686\n",
      "Loss after iteration 4934: 0.486145\n",
      "Loss after iteration 4935: 0.484671\n",
      "Loss after iteration 4936: 0.486131\n",
      "Loss after iteration 4937: 0.484657\n",
      "Loss after iteration 4938: 0.486116\n",
      "Loss after iteration 4939: 0.484643\n",
      "Loss after iteration 4940: 0.486102\n",
      "Loss after iteration 4941: 0.484629\n",
      "Loss after iteration 4942: 0.486088\n",
      "Loss after iteration 4943: 0.484614\n",
      "Loss after iteration 4944: 0.486073\n",
      "Loss after iteration 4945: 0.484600\n",
      "Loss after iteration 4946: 0.486059\n",
      "Loss after iteration 4947: 0.484586\n",
      "Loss after iteration 4948: 0.486045\n",
      "Loss after iteration 4949: 0.484572\n",
      "Loss after iteration 4950: 0.486030\n",
      "Loss after iteration 4951: 0.484558\n",
      "Loss after iteration 4952: 0.486016\n",
      "Loss after iteration 4953: 0.484543\n",
      "Loss after iteration 4954: 0.486002\n",
      "Loss after iteration 4955: 0.484529\n",
      "Loss after iteration 4956: 0.485987\n",
      "Loss after iteration 4957: 0.484515\n",
      "Loss after iteration 4958: 0.485973\n",
      "Loss after iteration 4959: 0.484501\n",
      "Loss after iteration 4960: 0.485959\n",
      "Loss after iteration 4961: 0.484487\n",
      "Loss after iteration 4962: 0.485945\n",
      "Loss after iteration 4963: 0.484473\n",
      "Loss after iteration 4964: 0.485930\n",
      "Loss after iteration 4965: 0.484459\n",
      "Loss after iteration 4966: 0.485916\n",
      "Loss after iteration 4967: 0.484444\n",
      "Loss after iteration 4968: 0.485902\n",
      "Loss after iteration 4969: 0.484430\n",
      "Loss after iteration 4970: 0.485888\n",
      "Loss after iteration 4971: 0.484416\n",
      "Loss after iteration 4972: 0.485873\n",
      "Loss after iteration 4973: 0.484402\n",
      "Loss after iteration 4974: 0.485859\n",
      "Loss after iteration 4975: 0.484388\n",
      "Loss after iteration 4976: 0.485845\n",
      "Loss after iteration 4977: 0.484374\n",
      "Loss after iteration 4978: 0.485831\n",
      "Loss after iteration 4979: 0.484360\n",
      "Loss after iteration 4980: 0.485817\n",
      "Loss after iteration 4981: 0.484346\n",
      "Loss after iteration 4982: 0.485802\n",
      "Loss after iteration 4983: 0.484332\n",
      "Loss after iteration 4984: 0.485788\n",
      "Loss after iteration 4985: 0.484318\n",
      "Loss after iteration 4986: 0.485774\n",
      "Loss after iteration 4987: 0.484304\n",
      "Loss after iteration 4988: 0.485760\n",
      "Loss after iteration 4989: 0.484290\n",
      "Loss after iteration 4990: 0.485746\n",
      "Loss after iteration 4991: 0.484276\n",
      "Loss after iteration 4992: 0.485732\n",
      "Loss after iteration 4993: 0.484262\n",
      "Loss after iteration 4994: 0.485718\n",
      "Loss after iteration 4995: 0.484248\n",
      "Loss after iteration 4996: 0.485703\n",
      "Loss after iteration 4997: 0.484234\n",
      "Loss after iteration 4998: 0.485689\n",
      "Loss after iteration 4999: 0.484220\n"
     ]
    }
   ],
   "source": [
    "build_model(print_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.08244176,  0.08244176, -0.01505103, ...,  1.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.08244176,  0.08244176, -0.01505103, ...,  1.        ,\n",
       "         0.        ,  0.14970347],\n",
       "       [ 0.08187206,  0.08187206,  0.04587453, ...,  0.18624691,\n",
       "        -0.60151098,  0.        ],\n",
       "       ..., \n",
       "       [-0.10657906, -0.10657906,  0.37886877, ..., -0.91218657,\n",
       "        -1.10760129,  0.3649651 ],\n",
       "       [-0.14468427, -0.14468427, -0.32197362, ..., -0.91218657,\n",
       "        -1.91218657,  0.        ],\n",
       "       [ 0.03928163,  0.03928163, -0.14240259, ..., -0.82519865,\n",
       "        -0.91342413,  0.07467359]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#load test\n",
    "\n",
    "#load test context\n",
    "test_context_f = open('coordinate_data_150/test/context.txt')\n",
    "test_context_input = test_context_f.readlines()\n",
    "n = 0\n",
    "test_words = [([\"unknown\"]*6) for i in range(int(len(test_context_input)/3))]\n",
    "test_types = [([\"unknown\"]*6) for i in range(int(len(test_context_input)/3))]\n",
    "words_index = 0\n",
    "types_index = 0\n",
    "while n < len(test_context_input):\n",
    "    t = test_context_input[n].split()\n",
    "    if n%3==0:\n",
    "        for i in range(0,6):\n",
    "            test_words[words_index][i] = t[i]\n",
    "        words_index = words_index + 1\n",
    "    elif n%3==1:\n",
    "        for i in range(0,6):                                \n",
    "            test_types[types_index][i] = t[i]\n",
    "        types_index = types_index + 1\n",
    "    n = n + 1\n",
    "    \n",
    "#load test labels\n",
    "test_label_f = open('coordinate_data_150/test/labels.txt')\n",
    "test_label_input = test_label_f.readlines()\n",
    "n = 0\n",
    "test_labels = [([-1]*3) for i in range(len(test_label_input))]\n",
    "while n < len(test_label_input):\n",
    "    t = test_label_input[n].split()\n",
    "    for i in range(0,3):\n",
    "        test_labels[n][i] = int(t[i])\n",
    "    n = n + 1\n",
    "\n",
    "test_num = 10000  #测试量\n",
    "test_type_dis = [([float]*15) for i in range(test_num)]\n",
    "test_words_dis = [([float]*15) for i in range(test_num)]\n",
    "\n",
    "#type和words的150维向量\n",
    "test_type_vector = pd.DataFrame(test_types)\n",
    "test_words_vector = pd.DataFrame(test_words)\n",
    "\n",
    "for i in range(0,(test_num)):\n",
    "    for j in range(0,6):\n",
    "        if test_types[i][j] in embedding.index:\n",
    "            test_type_vector.ix[i][j]=embedding.ix[test_types[i][j]]\n",
    "    if i%1000==0:\n",
    "        print(i)\n",
    "        \n",
    "\n",
    "for i in range(0,(test_num)):\n",
    "    for j in range(0,6):\n",
    "        if test_words[i][j] in embedding.index:\n",
    "            test_words_vector.ix[i][j]=embedding.ix[test_words[i][j]]\n",
    "        else:\n",
    "            test_words_vector.ix[i][j] = np.zeros(150)\n",
    "    if i%1000==0:\n",
    "        print(i)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "for i in range(0,test_num):\n",
    "    \n",
    "    test_words_dis[i][0] = cosine_dis(test_words_vector.ix[i][0],test_words_vector.ix[i][1])\n",
    "    test_words_dis[i][1] = cosine_dis(test_words_vector.ix[i][0],test_words_vector.ix[i][2])\n",
    "    test_words_dis[i][2] = cosine_dis(test_words_vector.ix[i][0],test_words_vector.ix[i][3])\n",
    "    test_words_dis[i][3] = cosine_dis(test_words_vector.ix[i][0],test_words_vector.ix[i][4])\n",
    "    test_words_dis[i][4] = cosine_dis(test_words_vector.ix[i][0],test_words_vector.ix[i][5])\n",
    "    test_words_dis[i][5] = cosine_dis(test_words_vector.ix[i][1],test_words_vector.ix[i][2])\n",
    "    test_words_dis[i][6] = cosine_dis(test_words_vector.ix[i][1],test_words_vector.ix[i][3])\n",
    "    test_words_dis[i][7] = cosine_dis(test_words_vector.ix[i][1],test_words_vector.ix[i][4])\n",
    "    test_words_dis[i][8] = cosine_dis(test_words_vector.ix[i][1],test_words_vector.ix[i][5])\n",
    "    test_words_dis[i][9] = cosine_dis(test_words_vector.ix[i][2],test_words_vector.ix[i][3])\n",
    "    test_words_dis[i][10] = cosine_dis(test_words_vector.ix[i][2],test_words_vector.ix[i][4])\n",
    "    test_words_dis[i][11] = cosine_dis(test_words_vector.ix[i][2],test_words_vector.ix[i][5])\n",
    "    test_words_dis[i][12] = cosine_dis(test_words_vector.ix[i][3],test_words_vector.ix[i][4])\n",
    "    test_words_dis[i][13] = cosine_dis(test_words_vector.ix[i][3],test_words_vector.ix[i][5])\n",
    "    test_words_dis[i][14] = cosine_dis(test_words_vector.ix[i][4],test_words_vector.ix[i][5])\n",
    "    \n",
    "    \n",
    "    if(i%10==0):\n",
    "        print(i)\n",
    "        \n",
    "test_words_dis_temp = pd.DataFrame(test_words_dis)\n",
    "test_words_dis_temp.to_csv('test_words_dis1.csv')\n",
    "\n",
    "\n",
    "for i in range(0,test_num):\n",
    "    \n",
    "    test_type_dis[i][0] = cosine_dis(test_type_vector.ix[i][0],test_type_vector.ix[i][1])\n",
    "    test_type_dis[i][1] = cosine_dis(test_type_vector.ix[i][0],test_type_vector.ix[i][2])\n",
    "    test_type_dis[i][2] = cosine_dis(test_type_vector.ix[i][0],test_type_vector.ix[i][3])\n",
    "    test_type_dis[i][3] = cosine_dis(test_type_vector.ix[i][0],test_type_vector.ix[i][4])\n",
    "    test_type_dis[i][4] = cosine_dis(test_type_vector.ix[i][0],test_type_vector.ix[i][5])\n",
    "    test_type_dis[i][5] = cosine_dis(test_type_vector.ix[i][1],test_type_vector.ix[i][2])\n",
    "    test_type_dis[i][6] = cosine_dis(test_type_vector.ix[i][1],test_type_vector.ix[i][3])\n",
    "    test_type_dis[i][7] = cosine_dis(test_type_vector.ix[i][1],test_type_vector.ix[i][4])\n",
    "    test_type_dis[i][8] = cosine_dis(test_type_vector.ix[i][1],test_type_vector.ix[i][5])\n",
    "    test_type_dis[i][9] = cosine_dis(test_type_vector.ix[i][2],test_type_vector.ix[i][3])\n",
    "    test_type_dis[i][10] = cosine_dis(test_type_vector.ix[i][2],test_type_vector.ix[i][4])\n",
    "    test_type_dis[i][11] = cosine_dis(test_type_vector.ix[i][2],test_type_vector.ix[i][5])\n",
    "    test_type_dis[i][12] = cosine_dis(test_type_vector.ix[i][3],test_type_vector.ix[i][4])\n",
    "    test_type_dis[i][13] = cosine_dis(test_type_vector.ix[i][3],test_type_vector.ix[i][5])\n",
    "    test_type_dis[i][14] = cosine_dis(test_type_vector.ix[i][4],test_type_vector.ix[i][5])\n",
    "    \n",
    "    \n",
    "    if(i%10==0):\n",
    "        print(i)\n",
    "\n",
    "\n",
    "test_type_dis_temp = pd.DataFrame(test_type_dis)\n",
    "test_type_dis_temp.to_csv('test_type_dis1.csv')\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_word_df = pd.read_csv('test_words_dis1.csv')\n",
    "del test_word_df['Unnamed: 0']\n",
    "test_type_df = pd.read_csv('test_type_dis1.csv')\n",
    "del test_type_df['Unnamed: 0']\n",
    "test_num = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n"
     ]
    }
   ],
   "source": [
    "test_X = np.ndarray(shape=(test_num,8))\n",
    "for i in range(test_num):\n",
    "    test_X[i][0] = test_word_df.ix[i][6]\n",
    "    test_X[i][1] = test_word_df.ix[i][6]\n",
    "    test_X[i][2] = test_word_df.ix[i][0] - test_word_df.ix[i][2]\n",
    "    test_X[i][3] = test_word_df.ix[i][11] - test_word_df.ix[i][14]\n",
    "    test_X[i][4] = test_type_df.ix[i][6]\n",
    "    test_X[i][5] = test_type_df.ix[i][6]\n",
    "    test_X[i][6] = test_type_df.ix[i][0] - test_type_df.ix[i][2]\n",
    "    test_X[i][7] = test_type_df.ix[i][11] - test_type_df.ix[i][14]\n",
    "    if i %1000 ==0:\n",
    "        print(i)\n",
    "        \n",
    "#replace nan with 0\n",
    "for i in range(test_num):\n",
    "    for j in range(8):\n",
    "        if np.isnan(test_X[i][j]):\n",
    "            test_X[i][j] = 0\n",
    "            \n",
    "test_Y = [([int]) for i in range(test_num)]\n",
    "for i in range(test_num):\n",
    "    test_Y[i] = test_labels[i][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " ...]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "error_num = 0\n",
    "\n",
    "temp_y = predict(test_X)\n",
    "\n",
    "for i in range(0,test_num):\n",
    "    if(temp_y[i] != test_Y[i]):\n",
    "        error_num = error_num+1\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6342"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
